{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0x28_网络中的网络(NiN).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPWDEtid/Qp09LIW+t2ou+4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Matrixmax/Dive_into_DeepLearning/blob/main/0x28_%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E7%BD%91%E7%BB%9C(NiN).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4dH4aPkK-dE"
      },
      "source": [
        "前几节介绍的LeNet、AlexNet和VGG在设计上的共同之处是：先以由卷积层构成的模块充分抽取空间特征，再以由全连接层构成的模块来输出分类结果。其中，AlexNet和VGG对LeNet的改进主要在于如何对这两个模块加宽（增加通道数）和加深。本节我们介绍网络中的网络（NiN）[1]。它提出了另外一个思路，即串联多个由卷积层和“全连接”层构成的小网络来构建一个深层网络。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_d_mqs0Mwfq"
      },
      "source": [
        "## 1. NiN块\r\n",
        "我们知道，卷积层的输入和输出通常是四维数组（样本，通道，高，宽），而全连接层的输入和输出则通常是二维数组（样本，特征）。如果想在全连接层后再接上卷积层，则需要将全连接层的输出变换为四维。回忆在5.3节（多输入通道和多输出通道）里介绍的1×1卷积层。它可以看成全连接层，其中空间维度（高和宽）上的每个元素相当于样本，通道相当于特征。因此，NiN使用1×1卷积层来替代全连接层，从而使空间信息能够自然传递到后面的层中去。图5.7对比了NiN同AlexNet和VGG等网络在结构上的主要区别。\r\n",
        "\r\n",
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAioAAAEHCAYAAACA6m3GAAAgAElEQVR4Ae19X8wdR322r/gakmC3IU5DRe2WNAIRZKuJ0hYq2SIoAX2AkwaBrCp2VEE/qYS+5CpNQJ+DQwu5+LAl0uSCSubzexHlAuzAXVBrrj4hIWFky0a2AEtcVVXBEonE5X565uyz57dzZmf3nDN73t2d55XWMzt/fjPzzDO/ec7snuNdhf6EgBAQAkJACAgBITBQBHYNtF/qlhAQAkJACAgBISAECgkVkUAICAEhIASEgBAYLAISKoOdGnVMCAgBISAEhIAQkFARB4SAEBACQkAICIHBIiChMtipUceEgBAQAkJACAgBCRVxQAgIASEgBISAEBgsAhIqg50adUwICAEhIASEgBCQUBEHhIAQEAJCQAgIgcEiIKEy2KlRx4SAEBACQkAICAEJFXFACAgBISAEhIAQGCwCEiqDnRp1TAgIASEgBISAEJBQEQeiCPzmN78p7rxzb7Fr1y5diTC45ZZbih/96EdR3JUZR+C3v/1tsXfvXeJkIk72ub7hP37961/HJ1S5QiCCgIRKBBxlFcXVq1fdZrD3XfuK+x48pGtNDP74z97v8Nze3ha91kDg+vXrDsd33v1ucXJNTva5ruE3IIIuX768xmyrau4ISKjkzoCW8V+5csU5ms/8w5eL710rdK2JwbMvfVdCpYVzXbKvXbvmcHz8c8+Ik2tyss91ffSpE26eLl261GVaVUYIBBGQUAnCokQiIKGSVpxJqJBZ64USKml52ZdYkVBZj+eqPUNAQkVMiCIgoZJ2Q5BQidKtc6aESlpeSqh0pp4K7gACEio7APqYmpRQSbshSKikYb+ESlpeSqik4aWs9IOAhEo/uE7GqoRK2g1BQiXN0pBQSctLCZU0vJSVfhCQUOkH18lYlVBJuyFIqKRZGhIqaXkpoZKGl7LSDwISKv3gOhmrEippNwQJlTRLQ0IlLS8lVNLwUlb6QUBCpR9cJ2M1tVB57l/PFbjaHOOrP74ZLPPP2xc61d/62pkCZf12kIY8P31T9xIqaZbG1IQK+P5v/3Gj4iXilqe4b1oT5K6tz7SdDvWtnzR8z92KhEruDGgZf0qhAsf7J+89ULz99t01J+w7UzjkpjJ/8dCRaF3awo9YwUniHvb+ZfuH7vri179d3PqOPdU90n0Hv8yvdLK9rqGESgvhOmZPTah89rlTxQcePFwJFXD3w48dr+7BZytcwLfT5y8Wn/vS6eKTx79Y/On7DrrfK4EQx3X0C88HL8tTlOvKdbRv63aNS6h0JLSKRRGQUInCo8xUQuUTx7acSIFogIOFEIGDbXJ4LAPn3MWhUpTQnhUqsIX7psvfAOC8aScWdi1nbUiopFlTKYQKhMCqG7Cd01AcPId4WMY+yoLrsIc4eQkRA4Hut4N8pGMc+ADAfKwF1EfIC2V8vqKtLv3rWo7t2xDto1394Fsa3udqRUIl15nvOO51hQpOK/BJEQ4RzptODHE4V+T5JxosA4EBgcN7OFs4Pt6HQjhVfJrc+0f7nW06e7RnnTLu4UBtn2gP6TyBiYW+42f9WCih0pF4LcXWESrgBHgH/llOxOZtmTzYB/+62gfH0R97gVsQOkxjnHy2/fGFBMWJX8bnK+rBbozjyMOpzao4Sai0EFnZnRCQUOkEU76FVhUqEAAQDHjMEhMXyMPpCsqGRAOdLU9YUMZ3zCyD0AmVp04U/L+JrGOHs+X7MWgXwsfWZRwOHWW7XKzTNZRQSbOW1hEq4AZ4EeNR1/kMlQNvyEPEQ2VsGriN8rxwSoKL9za0oh5xCgkKDqSB2/6ag42QUMHa68Jz+4HB9r0tjn6gXZ2opOF9rlYkVHKd+Y7jXlWoWKHAT4VNITYNOEseb9MBwwnTMePT6UN/8+SCY+anQd9hwh6dNT+x4lMuT1ogoPgp1XfCvkP3ba9zL6HSkXgtxdYRKpw/bN5tQgKc/MuPPFqJDQhlcAePY2inKQT/2uxbroPLX3r5vLOPd6nIbT+koOe6wVqj4EAa2iX32bcmodLWP9ZfNUQ/JFRayKzsVgQkVFohyrvAqkKFzpQnIPxUiFMMXLxHaMvCIVoHTIeLDQNOFRdECx0z03xHinTUxUZgP7Fig8Gpim0f+bY+hQr60SSukO4LHGujKS6hkmY9bUqogJsQAhQm4CEFddMcMx38Aw95HwrJdfIa3OMjI6b5XPf5inIQ3bSPdrlumAa+k9c2jf1r4zryWW+ZEP2QUEnD+ZytSKjkPPsdxr6qUGlyZiEnumxZOF062Ka6cPb45Gs3FWw6cPoURk116dAhcqyg8eP+htFkz6ZLqHQgXYcimxIqmDvMMziBl78hWtr4w/kG19t4yrKwCbERehwJLuM9EZa1IfrGExXWRbsQ0vabPziNJK9Z366jNq4jn/WWCSVUOpBZRVoRkFBphSjvAkMQKnCovtPFIxybxk98cIzIg1Om40a87aLzheOHmIFj9o/cQ/fLihUJlTTraZNCBdzghgsukittIep0ESqwCZECjoZO8JAHMYI8XzDAPk9lwHec/Lj1Yr7xg37wsn3GySLqg8MhbvtpftvWVlOcuOkdlTS8z9WKhEquM99x3OsKlTaBwPyQo6Nz9R0vHDIcN/MRUqjA+cLxwgEjnXbhZP3TFNjwhQbass4fcXyiRT8R969lH/9IqHQkXkuxTQoVnnaALziZIKfaQvAPfImVw4kfeAmBgbLgNjhoL/CPa4BlYRN1WB510U8+omJ9lAP3uT5sX7h+wGHyGmNEG7y3YciGtReKow2sHQmVFkIrO4qAhEoUHmWuK1R850Xn6KeH7pvKwgnDgYbqMA35qM97bAh4KZKfCiFosAEwnyHq+OIDdbkBoBzivsBh/bZQQiXNmtqkUIEYAJ8gBLCJd92wwaU2noKHsAveWM6CpzzRwAkhOI8ySGd5iArEQ+sBeewn6qDf5D45Cl6zDNNQj20hDWNgeyyzTIj6EippOJ+zFQmVnGe/w9h3SqjAOeKo237jgg4y5JiZx9A6faTBScNpwmHjUzHeXwmJDYgXbB60A0fubzZoP+T4WScWSqh0IF2HIpsSKph/zDU3a859iDv+vHcRKraO5Szs4x4XBAXaR1+siGAf/PUA/kJwWNvoC9aTTbPjQjoEmS/SIcrx6Injt/W7xCVUOpBZRVoRkFBphSjvAqsKFTg2fiK0IUQCLpvGuHWGcM5wnLggVuiU4RwhOvxPgr7TtE6febCBtuGg4cjhuK3j5ydPlkcbPHZHH/HVUbwXg5cq+fVm22fWi4USKmnWUwqhAj74G7OdO8wteGiFK/Kx+cbq0Qb406Ucy4c4yzyE4GKI91aooM84gQmVA+c5FuSjPdqHIMFYYcvnOk5EsG5YdplQQiUN33O3IqGSOwNaxr+qUIHDg4Nb5qJogDO1j2XgVHEfs8XyqAsBgrIUNxA7EBZw1Ng4eAQOu0yD84WzhmOlI4ZNOHNeyMOFNtBXOHZbnvVioYRKC+E6ZqcQKrF52ok8cBO8BX/9C/z1T0DYR3CRogN8ZJz5DLmOcI81gXvEIW5gmzwnr2ELtnFhLVDk0F6XEDYwJr2j0pHYKhZEQEIlCIsSicCqQqWLE4uVoZiIlQnlwenCsVKkoIx/b+uhvG0L9zY/FkfZZcrDloQKmbVeOEWhAn6AtxQHNrR89jkJDjIfcctnvyzzluUt6/n22u4lVNbjuWrPEJBQEROiCOyUUGlzgGPNl1CJ0q1z5lSFylh53dRvCZXOlFbBCAISKhFwlFUUEipF5xOWJmdt0yVU0qwqCZW0vLQcTRmXUEnD99ytSKjkzoCW8UuopN0QJFRaCNcxW0IlLS9TihNrS0KlI6FVLIqAhEoUHmVKqKTdECRU0qwpCZW0vLTiImVcQiUN33O3IqGSOwNaxi+hknZDkFBpIVzHbAmVtLxMKU6sLQmVjoRWsSgCEipReJQpoZJ2Q5BQSbOmJFTS8tKKi5RxCZU0fM/dioRK7gxoGf/Vq1fd7yDg90b4OwsK57+tsiwWf/xn73d4bm9vtyCv7BgC169fdzi+8+53i5fmt36W5WPf5eE38Dsqly9fjk2n8oRAFAEJlSg8ynzzzTeLO/fudc4GDkfX+hjccsstxU9/+lORaw0Efve73xV777pLfBzBmoT/eOutt9aYbVXNHQEJldwZMKHxnzp1akKj0VByREAcznHWNeY2BCRU2hBS/igQuHnzZrFnz57i4sWLo+ivOikEfARu3LjhTogQ6k8ICIE5AhIqcywUGzECZ86ccU5+a2trxKNQ13NGAKcpeLR64sSJnGHQ2IXAAgISKguQKGGMCBw6dMg5eZyq6E8IjBGBfftmL57u379/jN1Xn4VAbwhIqPQGrQxvCgEemfNF33Pnzm2qabUjBJIggEeW5C/CCxcuJLErI0JgCghIqExhFjMfA47KrZM/cuRI5oho+GND4Pjx4zUO415/QkAIzBCQUBETRo8Aj8ytWMHLtfoTAmNBYPfu3TWhgkeY4vBYZk/97BsBCZW+EZb9XhHAETkEyoEDB1xI0aKvefYKu4wnRIAvguMkEFxmiHT9CQEhUBQSKmLBqBHgY5+TJ086J//pT3+6wKdTPf4Z9bRm1Xl8Uw0C5dlnn3XhV77yFRfq8U9WNNBgIwhIqETAUdY4EMDLtG+88YZz7jhJ0e9QjGPe1Ms5AuDsa6+95jh89uxZcXgOjWJCQCcq4sA0ELBCZRoj0ihyQ8AKldzGrvEKgRgCOlGJoaO80SAgoTKaqVJHGxCQUGkARsnZIyChkj0FpgGAhMo05jHnUUio5Dz7GnsMAQmVGDrKGw0CEiqjmSp1tAEBCZUGYJScPQISKtlTYBoASKhMYx5zHoWESs6zr7HHEJBQiaGjvNEgIKEymqlSRxsQkFBpAEbJ2SMgoZI9BaYBgITKNOYx51FIqOQ8+xp7DAEJlRg6yhsNAhIqo5kqdbQBAQmVBmCUnD0CEirZU2AaAEioTGMecx6FhErOs6+xxxCQUImho7zRICChMpqpUkcbEJBQaQBGydkjIKGSPQWmAYCEyjTmMedRSKjkPPsaewwBCZUYOsobDQISKqOZKnW0AQEJlQZglJw9AhIq2VNgGgBIqExjHnMehYRKzrOvsccQkFCJoaO80SAgoTKaqVJHGxCQUGkARsnZIyChkj0FpgGAhMo05jHnUUio5Dz7GnsMAQmVGDrKGw0CEiqjmSp1tAEBCZUGYJScPQISKtlTYBoASKhMYx5zHoWESs6zr7HHEJBQiaGjvNEgIKEymqlSRxsQkFBpAEbJ2SMgoZI9BdoBePnll4sTJ04M+nriiSeKXbt2FY888sig+wkcX3zxxeLNN99sB14logi88sorg5/rZdbNpz71Kcfhxx57bFLjgv/QnxBYBwEJlXXQy6DutWvXnPOECNCVDoNXX301A/b0N8Rf/OIX4uOI1uTPfvaz/sggy5NHQEJl8lO83gCvXr3qNoQPP3qs+OftC4O+/tf/fqn46v/990H38W+3XnB4bm9vrzcxmde+fv26w/Geh44WHz91YTLXh/7xm5MZC+bl3keOu3m6fPly5ozV8NdBQEJlHfQyqHvlyhXnaD7zD18uvnet0LUmBs++9F0JlQTrhid9B48+U/z9hULXQDG4//gJx/dLly4lmHWZyBUBCZVcZ77juCVU0oozCZWOxGspJqEyDnEmodJCZGV3QkBCpRNM+RaSUJFQGSL7JVQkVIbIS/WpHwQkVPrBdTJWJVQkVIZIZgkVCZUh8lJ96gcBCZV+cJ2MVQkVCZUhkllCRUJliLxUn/pBQEKlH1wnY1VCRUJliGSWUJFQGSIv1ad+EJBQ6QfXyViVUJFQGSKZJVQkVIbIS/WpHwQkVPrBdTJWUwqVV398s/jT9x10F+JNX3dG3iePf7EIlfnEsa0CV1Ndpt/34KHi6FMnFsptfe2Ma5/lQiHqHf3C852uUP1Ymr71k2ZpDEmo7P/QkeLJ79+sviJ9x3sOuN9CwdemkX7vR5+s5duvUz/+rYvFB586Xdz3+FZV3+Y3xQ8/c6a4/8nnO11HX72xlO2mNldJ17d+0vA9dysSKrkzoGX8qYTKv/3HDScQIBQ+/NhxF0da06bOMhArKEfhsPeP9hcP/c2T1T3T8WN01pYVKqfPXyw+8ODh6sIv7Np79MnWRb4TKxAskQvlbL0ucQmVFsJ1zB6SUMFmjB82w0YOYQJeULggHULG3+QhZlAO190HDhUQHijDND9EG9YG6uBCeuy67a59lWiy9TcVR98wFv2OSkdiq1gQAQmVICxKJAIphApExK3v2FNYQQAB8Pbbd7tfkW3a4CFWIDicUHnqhIv/yXsPBMVDTKhA7CD/s8+dqv1qLWw/96/nnH3bBzhWe98U71rO1pdQIbPWC4cgVCBGPnH6h8XDXz1fvOvgYRfH6cjtf7jfxZG3/68fdfmIW3GAX23FSQfEBuLMA6cYZ0ghwnuEFCk2LRT37YfK9JkmobIez1V7hoCEipgQRWAdocJHOE2CBCIBeU2PebDB21OXve/a54QG0qzosUIAAgjOnhfECPLRF7RFexAusGfrMo669sSlKY5yrNM1lFCJ0q1z5hCECh7bQAjYC5zAaYlNYzwkCJC3qlCBIIJAil1vu21PzX6oD32mSah0prQKRhCQUImAo6yiWFWoQDDgFAVC4Usvny/+ZfuHwQt5KIOyOPFo2vBhj6IDIoPxpvLIRx3k49EP2v/LjzzqRBHiEB8QSIgj39rBZoM2uly2Xpe4hEqaVTUEoeJv8BAcb7t1d/XYx8+39xAoEBgQEnfcc9DFkQ/u2XKIN52o4LES2my7+BjKt7uJewmVNHzP3YqESu4MaBn/OkIFGz1EAEQDL5xi4OI9QpRBWZ6SIOQpBtPgwPEiLtIRQtiwDEJfJMAuhQpevrXt+XH/5Vy0BXs4fWkSWCGB4/chdC+h0kK4jtlDECoUGzzRwAkHhAfvQyHFAcTFwy+cc8Lkrz5/qjr1WEaoQATAHmzh0VLTJaHSkVQqNlgEJFQGOzXD6NiqQiW0SSMN4oECoqkMBAKEC95RYVmKGaTj5AXvqiDOy7cFMYITFNZn/l88dKR6/MM0P6RQgUjyRY299wWObyd0L6GShtdDECo8yYDgwKmIf8KBe6SzHEIKFYR4gRZcw3ssTF9GqKBd1OOjpaYQj6hof9OhTlTS8D13KxIquTOgZfw7IVS4wTeJGogTCAaWsyFEzhe//m134oKTG7wH4+x84Xn3qAcnMfymkA1pA++yUKgwLWUoodJCuI7ZQxAq2PRxWgExgpMUvFTrn2o4AdPw9WS+ywJBgwv2wL3QxdMTCg2IEl/4MG9IoYRKR0KrWBQBCZUoPMpcV6hQJFAU8HEN7xmGxACFCsKQ87ZpFC44McEFkYJ6sMtTEJRnHCFersXJDOuiLEQQ0uzjJ/Y5FC57qiKhkmZNDUGo4JGNe8ek/Kqxf6LBryDjN1LwNeHD//Tt6mQDGzguCg6Ey5x88GvHqBd6xOSnLWM7pdCRUEnD99ytSKjkzoCW8ScRKua3SCgUrAihoPDFCsv46bETFZZFO9YuhAcEDPP5LSCETEPIcnz8hLYgRuyjJpzSIB2X/yKutRWKS6i0EK5j9k4LFZxmQCzg8QviEMH+Bm/TEecPw7EuTmMoVPgeCUK868IfaUMd/saKtc/2YAsX33dhf5jGfNq3NjYRl1DpSGgViyIgoRKFR5nrChV/s24SH3453DeVXUWoQGzglAU/FgcRAtESOg3BezH220cQMqiHOuwj6sEO75cJJVTSrKmdFip2k4cYoHBAOk5QcKKBxz4QIkizQoHiAekUKrTHUxbeo6z/TSKk4bSGZRBC0KAu01jGtsu8TYYSKmn4nrsVCZXcGdAy/p0SKvyNFYgE/9RiFaECMQHRAZGBTQUX7Pgiw/7WCsrjG0Y4ZbHlIFravk5ty9u4hEoL4TpmD1mo4DELxQiEAi4rZKxQsEIF5fAoCXVtGfsOC9IhhKwowYlL6NdvUQ5iaSfFioRKR0KrWBQBCZUoPMpcVahAXITe6cBP4OMK5VGQUAjgHhfEivsGT+T/3+EpCOriq8NogwIDNj73pdPOBjYMiBXkwS76QcGCNPu+Ch73QKigbZRDHPX5XgtEDetaMRKLS6ikWVNDEip4TANe4B0U/2VavGCLb/XgMZEVH4xboYI4xAXzGNI+QooZPhriIx+0wfdSIHZwCgN7aBf/1xBtbTqUUEnD99ytSKjkzoCW8a8qVCAY+Oima4g62OQhLCg8uOlX394x77tYuxQlPG3Box2+fwLxgXuUYRrtoh22hXat8EB53PNiHYboE9tlWlsoodJCuI7ZQxIq2PzxYi2EQeiC+Gh6mZV5ECB8hyUkJmAfNlAO4oRlcPpi30vxT09wHxI/rN93KKHSkdAqFkVAQiUKjzJXFSptG3au+RIqadbU0IRK3xv+WO1LqKThe+5WJFRyZ0DL+CVUitr7KesKLAmVFsJ1zJZQKapTlSGLGAmVjoRWsSgCEipReJQpoSKhMsRVIKEioTJEXqpP/SAgodIPrpOxKqEioTJEMkuoSKgMkZfqUz8ISKj0g+tkrEqoSKgMkcwSKhIqQ+Sl+tQPAhIq/eA6GasSKhIqQySzhIqEyhB5qT71g4CESj+4Tsbq1atX3W9EfPjRY9XXdPl1XYXzry53xeJvt15weG5vb0+GIzsxkOvXrzsc73noaPXjavyRNYWzn9UfAg74sTr8xszly5d3giZqcyIISKhMZCL7GsYvf/lL52jgbHSlw+A73/lOX1OWhd1f/epX4uOI1uTPf/7zLHipQfaDgIRKP7hOyurLL79cnDhxYvDX1tbW4PsIHF988cXirbfemhRHdmIwr7zyyijme5m1MxYOLzMm+A/9CYF1EJBQWQc91R0UAocOHSpu3rw5qD6pM0JgGQQOHDiwTHGVFQJZICChksU0T3+QFy9edI8Czpw5M/3BaoSTRODcuXOOwwj1JwSEwBwBCZU5FoqNGAEcmbv/GO7w4RGPQl3PGYHjx2cvnh45ciRnGDR2IbCAgITKAiRKGCMCu3fvrl6uvHHjxhiHoD5njAAeWdqX1fUIM2MyaOgLCEioLECihLEhwCNzOnq86Kc/ITAmBPDIkvxFeOrUqTF1X30VAr0iIKHSK7wyvgkEcFRunfz+/fs30azaEALJEMBLtJbDBw8eTGZbhoTA2BGQUBn7DGbefx6Z20c/cPgXLlzIHBkNfywI4FElOLtv375aiBfE9ScEhEBRSKiIBaNGAEfkcPJ8EfEzn/lMdT/qganz2SDAF8Gfeuopx93nnnvOhUjXnxAQAhIq4sDIEcDJCUQKfpIeguXzn/98AQevr3iOfGIz6j45/NJLLzkOf+Mb33CcFoczIoGGGkVAJypReJQ5FgTeeOMN5+T1EuJYZkz99BF47bXXHIfPnj3rZ+leCGSNgIRK1tM/ncFLqExnLnMdiYRKrjOvcbchIKHShpDyR4GAhMoopkmdjCAgoRIBR1lZIyChkvX0T2fwEirTmctcRyKhkuvMa9xtCEiotCGk/FEgIKEyimlSJyMISKhEwFFW1ghIqGQ9/dMZvITKdOYy15FIqOQ68xp3GwISKm0IKX8UCEiojGKa1MkIAhIqEXCUlTUCEipZT/90Bi+hMp25zHUkEiq5zrzG3YaAhEobQsofBQISKqOYJnUygoCESgQcZWWNgIRK1tM/ncFLqExnLnMdiYRKrjOvcbchIKHShpDyR4GAhMoopkmdjCAgoRIBR1lZIyChkvX0T2fwEirTmctcRyKhkuvMa9xtCEiotCGk/FEgIKEyimlSJyMISKhEwFFW1ghIqGQ9/dMZvITKdOYy15FIqOQ68xp3GwISKm0IKX8UCEiojGKa1MkIAhIqEXCUlTUCEipZT/90Bi+hMp25zHUkEiq5zrzG3YaAhEobQsofBQISKqOYJnUygoCESgQcZWWNgIRK1tM/ncFLqExnLnMdiYRKrjOvcbchIKHShpDyR4GAhMoopkmdjCAgoRIBR1lZIyChkvX0tw/+N7/5TfHOO/cWu3bt0pUIg9+75ZbiRz/6UTv4KtGIwG9/+9ti7967xMlEnOxzfd95597i17/+deNcKkMItCEgodKGUOb5V69edZvBbXftK+4+cGiw1973PVj8j3fcUfzBew4Mto/A7/f3v9/hub29nTmz1hv+9evXHY7vvPvdxX0PHprEdc999xe37/6D4j3v//NJjAfzsvdd+9w8Xb58eb0JV+2sEZBQyXr62wd/5coV52j+/IkvF39/odC1JgYPn/yuhEo77VpLXLt2zeH4+OeeKb53rdA1UAyOPnXCzdOlS5da51QFhEATAhIqTcgo3SEgoZJWnEmopFlYEirjEGcSKmn4nrsVCZXcGdAyfgkVCZUWiuxItoSKhMqOEE+N7ggCEio7Avt4GpVQkVAZIlslVCRUhshL9akfBCRU+sF1MlYlVCRUhkhmCRUJlSHyUn3qBwEJlX5wnYxVCRUJlSGSWUJFQmWIvFSf+kFAQqUfXCdjVUJFQmWIZJZQkVAZIi/Vp34QkFDpB9fJWO1bqBx99UbtK8+HnzlTfPzUhSrNzw99Rfrxb12syofyh5Smb/2kWRpTEyqv/vhm8W//caP6mjXiW187U7tHmdhXsW39WLlN5ulbP2n4nrsVCZXcGdAy/r6FCn4E7a8+f6oSGne850AlVCBA3nbr7uLJ79+s8iE6IGTuf/L54t6PPlm87bY9Beog/f7jJ1w68uxlhQ/Ldf0lTthMKXQkVFoI1zF7akLls8+dKj7w4OFKiGCD//Bjx6t7/HiaFS4QG6fPXyw+96XTxSePf7H40/cddL9X8s/bFwpcR7/wfPCyIgXluq4DtG/rdo1LqHQktIpFEZBQicKjzL6FCk5M8Ku3EAOIw3FSGEDE4ISF9wz3f+hIcQrEz4kAACAASURBVO8jx109m4+6SHeCBaLl+An3K7W+2GA+7TWFXcs11Q+lS6ikWVMphAqEwKobcNtGjdMPiIdl7KPsc/96zgkCxClMIGL+4qEjC0IB+UjHOP7kvQeqfIgD1EfIC2WwPmy/IVS69K9rOWubcQmVNHzP3YqESu4MaBl/n0IFQuRdBw9X1x33HHQnJDaN8dDjHdS3pyVwxPYeQiEkNkJpIVHRtVyoblOahEoL4TpmryNUsMHj9AKbe5eNmptu1xD29/7R/s72P3Fsy/UHfeIFLkPo8J5xihfbF19IUJz4ZSRUOpJLxQaHgITK4KZkWB3qU6hAVNgLj3n4jopNR9w+/sH9J07/sICw+eBTp10c+csIla7/d5E9sWkSH8ukS6ik4fc6QgX//ww2fH+Dtxv7OnEIIJyMQDB0EUJ4hIO+8MIpCS7e29C+h4L4v2z/0D3+gZBBHGnLCJW3377b9RH9jF0QU6tgohOVNHzP3YqESu4MaBl/X0KFYgOCAxfeKbn9D/e7ONP8kIIAJym4IGzwfgriOHFZRqjgtIT2NhlKqLQQrmP2OkKFGy4EQJuQgGD4y488Wm3SeKRz6zv2FHgcQztNYRehQrEBkYHrSy+fd/a/+PVvu3um25Av1UJsof8QXhQcSFtGqLSNv2lsXdMlVDoSWsWiCEioROFRZl9ChWIDIU43IDr8NP/UwwoKvs9iX8RdRaigTT5eCoWhR062H8vGJVTSrKlNCRWIAggBChOednTZqLsIFYoNCAacxIDDfCTFNIoQ3OPCCYxtH2k4UWHaKkIF/eBjplCIfNpfJpRQScP33K1IqOTOgJbx9yVUuMG7b/bctmfhpVmkQ6g0CYX7Ht9y4gZlcDoDe3Dy+CaQ/cYPxId/esIXblEHdWOXfeTEPq8TSqi0EK5j9qaECjZlCAOIBXy7BqKFJxptG3YXoUIbsAmxYb/pwzwIF3y7h/c2RN94osK6aBdiw37z56G/edKtD1vXnijhZMc+YvLjyLd1u8YlVDoSWsWiCEioROFRZp9CBachEBc4TQmdZuBREPIgPKw4wGkKvvmD05CHXzhXCRq+/OqHFDK0gXp8F8Z/vBS6TylWJFTSrKlNChVsytxwsYEvs0njtKOtPGxCpGAthE4zkAcxgjxfMMA+T2UgVHDyA3vob+iyfcF7NDyhsY+WmuJ+29ZWU5y4Xbp0Kc3Ey0qWCEioZDnt3Qfdh1DBKQlECN4vQRwOOnSqgXSIBIgSlIdAgdigQEGIehArPHlBnMLCplOkIORvtaA+L/u+C9MY0ra1sWpcQqU792IlNylUeNoBsYCTiaZN2U/HJt0mVPAoCac1EBgoC7Hhn2bgRIXig2XRFuqwPOqin3xERRsoB4ERenRDIYMXZVEfF09neG/DkA1/zP69hEqMxcrrioCESlekMi3Xh1DBJm+/TQNBwo0fogCnGg9/9bwTMDYdcYgPCBfEISTsaQmEDAQHhQpObHBSQxsIkWfbQxps4BES68GOf4pjbawTl1BJs5A2KVQgBrBhQwhAKHTdsLsIFZxq8KQCbaAOxQVPNvBVZwgPpqMfiENUII481LUiAXnsJ+yj32yH5SCSWIZpFEW8R3/YHtOWCSVU0vA9dysSKrkzoGX8fQkVu9lb4YDHNjzJ4IuyeKeEpymoR0HhCxX77gnt4/SEdpBmhQ5t4bTGCh7Y9+vR3rqhhEoL4Tpmb0qoYCPHJs/NGqIA9/4LraHNu4tQsfWsUIF93OOCoED76AsFC+qxD75QgfiBULG20Rc8OrJpdlxIhyDzv4aMExo8euL4bf0ucQmVjoRWsSgCEipReJS5KaFC8eELAQgUK2RsvhUqECD2NIXlIEBsOk5jKFycILnnoLvnuyn4XRacpuAkBu3CLm2lCCVU0qypFEIFG72/MdvNF5szRAI2fpuOzTdWj2UhLLqUY3krVJhmQ5x2+CcgyLdCBX3GCUyoHMQLx4J8tEf7ECQYK2zhJAdfk8bLuHiBGOtgmUdetIlQQiUN33O3IqGSOwNaxr8JoQIhgVON0Au1cJIQJCGRYIUKbNjHSbY8ylGc2Ec8SMPJCfJx8UQG6RA4TeLH2l42LqHSQriO2SmEit1QhxCHqAHfQy/U4rdb/BMQ9tkKFQgDK0BYBiHECcQO4jg9oZiBuIFt1MMFwQI7uGAbF+pR5FibbXEJlY6EVrEoAhIqUXiUuQmhgs0ewiB0NZ20oA7eZ2G+fTTkiwebx/J+maZ7W7epzDLpEipp1tQUhQo2fZzyUBzYkI95QsIAQoP5iPvvotg6zEM5m94WZ722cn6+hEoavuduRUIldwa0jH9TQmWZzX7MZSVUWgjXMXuqQsXf6Md+L6HSkdAqFkVAQiUKjzIlVIrgY6dVxZKESpo1JaFSLHUislOCR0IlDd9ztyKhkjsDWsYvoSKh0kKRHcmWUJFQ2RHiqdEdQUBCZUdgH0+jEioSKkNkq4SKhMoQeak+9YOAhEo/uE7GqoSKhMoQySyhIqEyRF6qT/0gIKHSD66TsSqhIqEyRDJLqEioDJGX6lM/CEio9IPrZKxevXrV/bYDfn+EvzeicP5/BC2Lxe/vf7/Dc3t7ezIc2YmBXL9+3eH4zrvfXf3+B38HROHs91CGgAN+ZA6/DXP58uWdoInanAgCEioTmci+hvHmm28Wd9651zkbOBxd62Pwe7fcUvz0pz/ta8qysPu73/2u2HvXXeLjCNbknXv3Fm+99VYWvNQg+0FAQqUfXGV1BxA4derUDrSqJoVAOgTE4XRYytJ0EJBQmc5cZj2SmzdvFnv27CkuXryYNQ4a/HgRuHFj9v9aIdSfEBACcwQkVOZYKDZiBM6cOeMeA2xtbY14FOp6zgjgNAWPVk+cOJEzDBq7EFhAQEJlARIljBGBQ4cOOSePUxX9CYExIrBv3+zF0/3794+x++qzEOgNAQmV3qCV4U0hwCNzvuh77ty5TTWtdoRAEgTwyJL8RXjhwoUkdmVECEwBAQmVKcxi5mPAUbl18keOHMkcEQ1/bAgcP368xmHc608ICIEZAhIqYsLoEeCRuRUreLlWf0JgLAjs3r27JlTwCFMcHsvsqZ99IyCh0jfCst8rAjgih0A5cOCACyla9DXPXmGX8YQI8EVwnASCywyRrj8hIASKQkJFLBg1Anzsc/LkSefkP/3pTxf4dKrHP6Oe1qw6j2+qQaA8++yzLvzKV77iQj3+yYoGGmwEAQmVCDjKGgcCeJn2jTfecM4dJyn6HYpxzJt6OUcAnH3ttdcch8+ePSsOz6FRTAjoREUcmAYCVqhMY0QaRW4IWKGS29g1XiEQQ0AnKjF0lDcaBCRURjNV6mgDAhIqDcAoOXsEJFSyp8A0AJBQmcY85jwKCZWcZ19jjyEgoRJDR3mjQUBCZTRTpY42ICCh0gCMkrNHQEIlewpMAwAJlWnMY86jkFDJefY19hgCEioxdJQ3GgQkVEYzVepoAwISKg3AKDl7BCRUsqfANACQUJnGPOY8CgmVnGdfY48hIKESQ0d5o0FAQmU0U6WONiAgodIAjJKzR0BCJXsKTAMACZVpzGPOo5BQyXn2NfYYAhIqMXSUNxoEJFRGM1XqaAMCEioNwCg5ewQkVLKnwDQAkFCZxjzmPAoJlZxnX2OPISChEkNHeaNBQEJlNFOljjYgIKHSAIySs0dAQiV7CkwDAAmVacxjzqOQUMl59jX2GAISKjF0lDcaBCRURjNV6mgDAhIqDcAoOXsEJFSyp8A0AJBQmcY85jwKCZWcZ19jjyEgoRJDR3mjQUBCZTRTpY42ICCh0gCMkrNHQEIlewpMAwAJlWnMY86jkFDJefY19hgCEioxdJQ3GgQkVEYzVepoAwISKg3AKDl7BCRUsqdAOwAvv/xyceLEiUFfTzzxRLFr167ikUceGXQ/geOLL75YvPnmm+3Aq0QUgVdeeWXwc73MuvnUpz7lOPzYY49NalzwH/oTAusgIKGyDnoZ1L127ZpznhAButJh8Oqrr2bAnv6G+Itf/EJ8HNGa/NnPftYfGWR58ghIqEx+itcb4NWrV92GcO/Dx4qPn7ow6OtD//jN4n/+n38fdB8f+LsXHJ7b29vrTUzmta9fv+5wvOeho4Oe72XXzAf/8ZuTGs+9jxx383T58uXMGavhr4OAhMo66GVQ98qVK87R/PkTXy7+/kKha00MHj75XQmVBOuGJ30Hjz4jTq7JyT7X9f3HTzi+X7p0KcGsy0SuCEio5DrzHcctoZJWnEmodCReSzEJlbS87EusSKi0EFnZnRCQUOkEU76FJFTSbggSKmnWkoRKWl5KqKThpaz0g4CESj+4TsaqhEraDUFCJc3SkFBJy0sJlTS8lJV+EJBQ6QfXyViVUEm7IUiopFkaEippeSmhkoaXstIPAhIq/eA6GasSKmk3BAmVNEtDQiUtLyVU0vBSVvpBQEKlH1wnY1VCJe2GIKGSZmlIqKTlpYRKGl7KSj8ISKj0g+tkrPYtVO57fKs4+uqN6iumdx84VBx+5kx1j99hePL7N6t761BR74NPnS7u/eiTwXxb1sbx2xb3P/l8pwtlbd114xIqaZbGkITK/g8dqXH0jvcccL+FAq6Au+BnG4exDpbhFtZIVw7b9bVMGynK6ls/afieuxUJldwZ0DL+voXKX33+VPGug4crJ/22W3dXwgVODsLFd5hI46/kWmHDND+EHWuDdhHGLtj261o7q8QlVFoI1zF7SEIFHIGgBh8gSMA/ChOkQ8j4XIGYIU9X4TDqkJ8xDt92175KNPl92MQ9+oZx6ndUOhJbxYIISKgEYVEiEehTqOC04hOnf1js/+tHXYjTkbfdtsfFmf7wV8+7ezp+ONfHv3XRiRk4anviAYdo71GWTtw65VCazWe8azmW7xJKqJBZ64VDECrgJHgKjkJsIw4O3/6H+4MctvwAT3HSEeKwLYd4iIcUKX5Z/9637+f3fY++S6isx3XVLgoJFbEgikCfQgVO1F44TcEnTZvGOMSJ71SRZ4XJMkIFG0vb0bkr453G+H1Y9l5CJUq3zplDECrgJPnJEBxs4nCIKyEO++WahEoXDkM02TXi2+77XkKlM6VVMIKAhEoEHGUVRZ9CxTpJfLqEULEnJzbfxuHc4aRx+nLHPQddHJvGMkIFR/Jw4G1X6uf7EippVtUQhIrlJOLg0jochg1w2LfbJFS6crjLmvLbTHUvoZKG77lbkVDJnQEt4+9LqFBsQHDgwic/CA/eh0I6T4oLbAp4xwX3cMbLCBU4UNhDXRzZN12pnbyESgvhOmYPQaisy+GHXzjnOEsOg4/LCJWd4jDXYZdQQqUjoVUsioCEShQeZfYlVCg2EPKo3KbhWxA4LbFp1jHiWw9w6ihDMbGKUMFmE7tCj5xsP5aNS6ikWVNDECrkJgQHeOifcOC+C4ftt9aWESpoF/yL8Rd5qTm8DOclVNLwPXcrEiq5M6Bl/H0JFTg7CAy8SIuTFL40a0824OSRTyFiHSS+SQGBA0GDDQF5cPKhi588WR/O209jXt+hhEoL4TpmD0GogCvgJnga43DT15Mp0MFlipUQf5Hm8xUchlDqm6/r2pdQ6UhoFYsiIKEShUeZfQkVnIjgcQ+cNBxx6FMhHTQe8Rz+p29XThlH5RAodNawYX97pc25sh7C0CMmPy3lJ1IJlTRraghCBTx070mVXzX2OQwhAg6Dq/iasOUwNnBclovL8IxfO94JDretL5svoZKG77lbkVDJnQEt4+9DqMAhw9Hy6BrO3Do3xpmO8hAjeLGVdfFJlk7enrhgc+ALsDhpgaOkPYYQPihjj+6x4UDs2DTGrX3aWDWUUGkhXMfsnRYq4AY5jDi5anlh0xEHh8El1g1xGGkQ8JbDIRHO9mALF993QWjTGE/JYTvGtriESkdCq1gUAQmVKDzK7EOo+M6NThfp+PSJEw0cp2MjQBqdNuMUOBQqtAeHjjTeox5FCdModHiPEG3iYhrK+PWYt24ooZJmTe20ULE8gBho4jD5aIUCxQNs+BzmKQvtoyy46NeHIGcZhBBBVpSjHsrYerb8puISKmn4nrsVCZXcGdAy/k0LFXvSQSeLTcCKFTpZ6+RRNvSbEXCUED6sg3srSiBuQg4dZSCW2AfWXzeUUGkhXMfsIQsVCF2KEfAHlxUylkM+h3Gyh7q2DE4G+R4W0sFNK0rAYQgVW4fl+uCw307sXkKlI6FVLIqAhEoUHmVuSqjgFz3ti7SM4yVDfKIMOUPr5OHIQ84amwTq0/nz2T7s8bgcbfC9FPc16Vt3u0+6EDB4mTfU9qppEipp1tSQhApENIQI3kEhb20IDvF00OeN5TD4a0U0y9I+Qsfn2/ZUwp0cRhvkMMQOOA/baJcv6tLeJkMJlTR8z92KhEruDGgZ/yaECl5KhFMNXXDcfNTjO1jk4dMr0iFUQqcuyLO/U4E47aAuPo3y02+oPuxic2CddUMJlRbCdcweklABJ7Ahh/iLNMtTnz/MA8cgVJq4Bt6Cr8i36wHcxX0Th1EebfjtbupeQqUjoVUsioCEShQeZW5CqGzKaQ6hHQmVNGtqaEJlCNwaYh8kVNLwPXcrEiq5M6Bl/BIqRdJPoxIqLYTrmC2hkpaXfYkcCZWOhFaxKAISKlF4lCmhknZDkFBJs6YkVNLyUkIlDS9lpR8EJFT6wXUyViVU0m4IEipploaESlpeSqik4aWs9IOAhEo/uE7GqoRK2g1BQiXN0pBQSctLCZU0vJSVfhCQUOkH18lYlVBJuyFIqKRZGhIqaXkpoZKGl7LSDwISKv3gOhmrV69edb8Rce/Dx6qvQPKrkApnP1++DA4P/N0LDs/t7e3JcGQnBnL9+nWH4z0PHRUvy5/RX4aHmyqLr/fjN2YuX768EzRRmxNBQEJlIhPZ1zB++ctfOkcDZ6MrHQbf+c53+pqyLOz+6le/Eh9HtCZ//vOfZ8FLDbIfBCRU+sFVVoWAEBACQkAICIEECEioJABRJoSAEBACQkAICIF+EJBQ6QdXWRUCQkAICAEhIAQSICChkgBEmRACQkAICAEhIAT6QUBCpR9cZVUICAEhIASEgBBIgICESgIQZUIICAEhIASEgBDoBwEJlX5wlVUhIASEgBAQAkIgAQISKglAlAkhsKMI/OcPiq2PnSzO/6fXi5+cLR742Nnikpccur300rHigad/UPx3KLO4UpwO2Q+WrSfG7dbL6k4IbBwBrZ2NQ75Kg9kLFTnSVWijOnMEsIkfK07/ZJ4yj8Xy5qXWjTkOv3TFM/NfxfmnjxUPLKR7xXDrnHXTGFCgXaj89+snA0JnM+Ovjah1LLXSG7mRj2mCOcaPWF6TveXTtXYMZgNcO+xdQqEyI9YDHztWLH11cabscdJwCWfu2k01xrLdjlhtvf5fSUc9bGMG48ZP+MMZQXiDLvuHE43YGErHsPR6sTadjabTlOa1aIVV+0aKOQm0YaYhiMMSJzqufsf1QLzsGExXipmtxZMkN86FNuLjsnZXii/h/FfBYIaFNwaHe/PcE79ZuIjTSuNcoVKQM7SjtdP5NHQV3oxi7ZALRVEkFireginahUBY0Zoe9hqdbYrdhUDIYa8yxlmdLu0Cny7l6jC196lefifvyr6aDWT58e5A/xtFBtfA4rjmG0S5OTgbS24UngOv8cNtUGifYo99meOzsIkHNtLwhh7a+OZ9X9x0YuOnrVj9eZ8XY7Px0dl2cdQoW8MKRh1e8z4strN+ygLeEZOurP+hrZrThopu/rx57jquVfhnuuF4YkWzyYtGS87N1wP5wHHEuFPO1yp9By6mvzU+VDhr7fjzslNrx3KoV6HSZZE6sleLkyQhcWdh08YVc1CLdWLkr7fnT5S7dwRH/7iYZjAuP0bUW64vi2OxbYeETGm/wtVO+QpxOpaAPTd+s/hbrUcdTH0DmtmKY7X1+v9zj17CjzjKurX+BTgWGFdwjqyd0DhcWskPOL6A3RpeIRttAFpna+Pu8Qwe9fxg9sjn6R8Ul/zHMa69+iOexfVX5/esO4u8j84NcHKOPyICvPzYWg6uR/9xG8a2gDfmfz6e2sZUzIRLaI7apiCUv0r/7biwGTgbtTHMuNq0/l0/LOfYMYdtB5/mPhx4c1RyJIRLjbuVD5vXJwYUkOwO11I1DtfGvN7COND/Gg4zS7X2QzbmDYZjsMs1bONaOzu6dsKTVU/tT6i0LZaSMDVHGVp09f6237l2584pVqFG/FjBKs9z2KuM0dmabaDVwq3sL0Z85zovUdp4OvRuQLlBBxb7vP4SMTovT6TBwtIYRh3MzDHXHV0TVvP0mYP0HB86V/a7slfOVx33uZ0KEY7Xw2+Rq16brh645/GkMuzhVZbfejomHsrK5PXrc2c7G7fdkNAftM1+GR6UY6qNnTbLl3Br4zN97vKOiqvLTaB0/BXuNVtmPFV5D5dQ+VpaM09qbWJ8po3aWqrmqmY46U0dk3bTbj4rzpVzZ04arbBB3I01NA43r+RApF1X1ytX8uSBFdZ6xUeD+ax1b401tqu1M1trdX+wMIMerx3uC5gv1CoTxrF2/N73I1TcQmk4DSnz6FBqzjG06PweR+9nk1Bzxo3ljRNvLONnwH4pglYdozPpLVy/GXNfc64mfbYJny0uBTFbZWzWuBcvHct5/xP6KkLF4Waco7NtN9t6/PRPmrAy6aUNcoq9nzlOttWVGx2xKzGpfaPGpZ0sTr90MviJEP2qORXaKOu5b+24+LHi9Ev1T5XVOgF+vlNincgLvdjY/HVRbSzVZkisiCBDw3sm1cISM/YLfaw23PLkgnms542jhgvLLBvWbM76ZDlh15LD0+/Tsu3FyqMvFBOxcibPYeBwK/EMnXjwJIN9t9yhLdd201yyEIW8V87ZO1usstZn/Z+d6NW5NhtPlVa2obWDudDaMYxsjPYgVGYbwsxB2Eng4qsvjMoBo4shAjd2fTFjKWfn2mpRrgtN0GGvMUZnk1jUN2X/ExPvqwVu+jMf68xWvUxp32wWqOrqVJvSfOxM95169anKYcVPO/N6lU06TfavdNJ+/91cm/Z9Rx7qBxdyfXxoyI7bxtmJ+tH+zHade/OSJub6XopRk7wQDXG1wgml0afF9lw/iJexwXT7uGa+sRq+oX+sX3aqtoZqHS158LGTBU5tgPcijqhg7OPW9audmxVfbHmvb7P3QAJ4euPg3JMz3ULfLsYxw5x42q9bB/Gs4ZXqZoZn16+Gs1XX55eu1MVsuZYc1mW8Noc1zpWWynLdMPQ4WtnzOEH/YebX8c7cV5i79q1db326Nmw+Ocf51NpxM+lwJCZkSflulY+751fb5963O5S1Y8Zpoj0IFWMdUevEAp8wak42sMBqi9IzXb+dLayu5WeO0VssdYOBO7ThT/CSYwxYXT6pvvB9h8FNPPqptnJIs9YrJ4NbL8/e+7jV6tGZVY8eYMxzeAtOjKPnploXQhzL4rzWMfD74bdb4xmbDISLdgKFkNTB2V7CyYpxKKhWs1+zERi/yz9WbNnHe8DP2vTvy+668cJ5WbFa2oMTq0RG+a7GIr4N4w4koy328Tx+j6Vqcz5HKGPbrOHg4xJoYyGpwm6OW9Q5P/2DAn1z46Sfqfq5YH2tBGK/LKYOk0CfaM+Nz849eulwCPikVUdg7Ln+mLXsz5nrl+nPPL+ck2oscx64blVzZzpp2sWa19qZY5bT2jGMqEXTCxU8Q7fqzhAZLfuLbsGxvW5+D8KRt+lTYG0cS77BP9s8a/20ffbj1RhKobLqGMvxdG63qR+1Rc1vL1hn5TkKv3wJHeZi7kxnmOC+nu47w/kCgpm5c8KdJ0rKdlwZOq0mocLNA5/6nz5p+lVvrzTpnaj4fSQmFKIeHs6Ix4Gyf46f1XyjYFmXc8G8xrmMzAPxIhbOBvvIEy9bn2lmk68Jkxkf3aMm9o8h25gDNo+VWGOu5/PHcdbbn1dqil0pTtde3J31CY+x5rZRF+neWImlh0vNR3A8DDkuD7um3tn0Oq/L+ac9W3CNuBsz+9olXMCgfBna1LUCbyZM5qddp1+f/9hfFDdjr8n/uHZqvqK+9urzufhIr5YfseOPYd4fy73FNevsc768+Z/hbutr7cxxLfnSgF0Xum9i7cT6kV6o2F/HdGTyQPJ64xYXAfTycFsjfyCfSV3LVTYDJyOuL8Zx0PY8nDvhKi3BGGmrthCZGAgXxzpzunPR4S1yIwIayYt2qnLzDcU1X3M6LDdzCrW+WCx8x0hcXRue/epT/Q9m3+BhXceLurOcw+Gn1+99Xvn3czuBR0Tsqy3kc9GN1RuHjxPql5hws6n1w9ooy7n5MevB4Wsf2QA/9o/zZcrPulzOP3EMhK4/rO/yrZNvr8/HGhhPXfCUoAXm2fLbxlHD4mLjpTUX1NID9m3ZUJx9rfJC81VlrhBpxLPBFspzLskvby7dmBfmx9jrPIaA7zJmqqhvz40psNY5Z37/zb3ru7uvr83ZmtDaIR9rPhQTEeC2XS82juIO55I3Nl7NqVcmZN+WDcXZ1yrP50mV0U+kX6Fi+uzAdQuuTtAmYFl1Vq9eh3nzsHSs3iKf59uYv6nP81xfzEKb5zAWX+xLjTEw0T4B2Wo9nPV/QXAA26rvHh4B4tdtzu6a+j9zLHYjmy8OV4ftBsa00E6oL1XabGzzT3Y4SfCcXGVwMX3eF2OnLD8bW5hHNQ66vtTHyibn9ilAPHsN43f1Qo7ElZ/ZqLjn0niCUvLNneCVfUL/iDc7thDOsKE4qmcvYlPPx12svh07ys36X8PGP4Vy656nAGbTq9ZrfS5r82E6V0uvODMr4PJq7bC9+aMolJmL+fk462mmwWWirj9os+kF94Axby4tTxZLz+YN677W3wbO1bByxkou2Q+Si40EHyXRVn2OSx9guOjnz09Y6/PbWah44o39cN3W2ilnr45tDSMzv7X0oa0dNmC6rAAABf9JREFU08+maE9CZQZebTOtnFK9KzUA61nuzuWbxRAoUj1yqC3gcMHZCY05grbF2tuyi329MS4u6vlRZQ03Ol/i50gW2Ehr6WXfWKe2+dkRm3jl8Ly6KFLlBcqbr8vSMUXnwVsk9U0xtInO+rNoM5BO58XQdHc2Bs/Jl/l1DpYbArEzNmpz1thGYG6MjVpbpQ33DQtzwufaeenK4iclrAPg568HZ4cbM4TDDJtlhYpr172/EqtPPtRFWg0bM96FKPrPF0YrjO26KjdA8t4PWcfDoYaradSmI+7zKJRmqneO2naC6yVkyRsD590VRV5t7A28cnPv54XWUcnrmk1yxtQP2WNaba13ESrzE/Hq/SAMztmr86cLZosYl99O0tqp/p8vh1FwjueC3Z2oGB9Sw9Xw1KYj3tfaMU02RnsSKovt1QGcL4wKDEdefpIs65eLtXK45b0PGDfIqtxi87OUUBumrOuLmUCTVUbrDtXPbx1jVYFOY44DsmqOqipbjzT3cWZzhs1so5m/1Ejn77X3kyvlf0Lnlfdxcvf1ulV/sSgMZvPNzvT7J1fm/zFeyDY3n+A7LrO+Lc55KL3EAC+fVjbn/WDf6ra8saM4NwnPRg17N47VnG3VfonFwqdk9GHBfsk9s1lwPBb/2WhnYwqvhxlGVV6wD7H6ob7NNyT7LZs58osxhyXxdXjPsazlLVZ1KW7shndNdWw64hX2zoqHBdJKPCp8GtqPJjsbi+tloQ7GbcbgxkRM/MJmnpyAYblAWxUvWMbZKvmzwokKqlc2TX8dtuben5PZEMr1ZU+CXJ/n8+3KBcYxqz//tzZ/Bo/6nHIOrX2tnTmKi2vVrhFbzqbXsHeFAmvHVk4c35hQaeq3BYOCY/5JwlvsDRtIN+cS2JC8Trm+mIXnZZcnN16fFgstpNTHOP8k4r6Kajb6qKOC1XJxLizMskXXTnlaRMdiyzJtju/80QrfOWDnWdY57EYnMiOrdbauPufJKHvbj1k/Z5/mbDrnv75JzB2d7Tfj9foUZJ7g5aBcWPa5oW/zooF2reN3mFhnyPmJ8WNms+pzyEbZAeBfx6HM8Da3eX9tLNB3M15gB9tujoN8b68f5EvQlu0X49bJeZiwSDRcrOOvMVa36YhvvV5/D6qai7LCjPfevNJY17BxvXgGvLl0bVuOecWDt35b7h7zO+f5jEe4j3GztO7bqxot7Zk5dtia+0Y+lf6gwtq14WHc2C474M15yEZZFP3Q2iFufujh6L+/Yoovu3ZM1eTRxEKFR4lLhssuzlVgME7BTYDnuLnxNYZuQc4Xf2O5JrvlGEOOsHt/vMW9Cg6DrjPDt+5kFhfWbAhN6RsaoHOUIZ5HNoOyTjW+iLNtGkXjZlCrMMOmaqeWF8K4VmCJd1Tm9aL9Kjeq2prhmu+EQWDdmQ0SvYiuobItlKk2y3nXa7EuZWoVQjeN3AjwxYxj5hsCZZp8SpVOzs1wqo1xib7U6oXGlSqtsU8cR6Chsk7FaXe/nD+McrRqUmsntE6TrIsK4+UjiYVKhGgNfXMOhk6rocxwkuEI1hijc9jLLa7hjH0TPQltok2CpCl9E/3k6Yk3l855Gn7ENmh0s6OzdWuk2pTaN9sNITCoZpr8iE1HPL4Zr7a+F4DwebBQoEwwH56Q4jbSZX1h1dYOr4emMYbSQ7yvxmGwMZx3m6fFJmQj0JbWTgAUL8muEZtl09vXjq2ZPp5QqKTvnCwKASEgBISAEBACeSMgoZL3/Gv0QkAICAEhIAQGjYCEyqCnR50TAkJACAgBIZA3AhIqec+/Ri8EhIAQEAJCYNAISKgMenrUOSEgBISAEBACeSMgoZL3/Gv0QkAICAEhIAQGjYCEyqCnR50TAkJACAgBIZA3AhIqec+/Ri8EhIAQEAJCYNAISKgMenrUOSEgBISAEBACeSMgoZL3/Gv0QkAICAEhIAQGjYCEyqCnR50TAkJACAgBIZA3AhIqec+/Ri8EhIAQEAJCYNAISKgMenrUOSEgBISAEBACeSMgoZL3/Gv0QkAICAEhIAQGjcD/B5zSjNzPBowXAAAAAElFTkSuQmCC)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kUPwbx6NFLE"
      },
      "source": [
        "NiN块是NiN中的基础块。它由一个卷积层加两个充当全连接层的1×11×1卷积层串联而成。其中第一个卷积层的超参数可以自行设置，而第二和第三个卷积层的超参数一般是固定的。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phN3VGxdK43m"
      },
      "source": [
        "import time\r\n",
        "import torch\r\n",
        "from torch import nn, optim\r\n",
        "\r\n",
        "import collections\r\n",
        "import math\r\n",
        "import os\r\n",
        "import random\r\n",
        "import sys\r\n",
        "import tarfile\r\n",
        "import time\r\n",
        "import json\r\n",
        "import zipfile\r\n",
        "from tqdm import tqdm\r\n",
        "from PIL import Image\r\n",
        "from collections import namedtuple\r\n",
        "\r\n",
        "from IPython import display\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "import torch\r\n",
        "from torch import nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torchvision\r\n",
        "import torchvision.transforms as transforms\r\n",
        "import torchtext\r\n",
        "import torchtext.vocab as Vocab\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "\r\n",
        "VOC_CLASSES = ['background', 'aeroplane', 'bicycle', 'bird', 'boat',\r\n",
        "               'bottle', 'bus', 'car', 'cat', 'chair', 'cow',\r\n",
        "               'diningtable', 'dog', 'horse', 'motorbike', 'person',\r\n",
        "               'potted plant', 'sheep', 'sofa', 'train', 'tv/monitor']\r\n",
        "\r\n",
        "\r\n",
        "VOC_COLORMAP = [[0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0],\r\n",
        "                [0, 0, 128], [128, 0, 128], [0, 128, 128], [128, 128, 128],\r\n",
        "                [64, 0, 0], [192, 0, 0], [64, 128, 0], [192, 128, 0],\r\n",
        "                [64, 0, 128], [192, 0, 128], [64, 128, 128], [192, 128, 128],\r\n",
        "                [0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0],\r\n",
        "                [0, 64, 128]]\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ###################### 3.2 ############################\r\n",
        "def set_figsize(figsize=(3.5, 2.5)):\r\n",
        "    use_svg_display()\r\n",
        "    # 设置图的尺寸\r\n",
        "    plt.rcParams['figure.figsize'] = figsize\r\n",
        "\r\n",
        "def use_svg_display():\r\n",
        "    \"\"\"Use svg format to display plot in jupyter\"\"\"\r\n",
        "    display.set_matplotlib_formats('svg')\r\n",
        "\r\n",
        "def data_iter(batch_size, features, labels):\r\n",
        "    num_examples = len(features)\r\n",
        "    indices = list(range(num_examples))\r\n",
        "    random.shuffle(indices)  # 样本的读取顺序是随机的\r\n",
        "    for i in range(0, num_examples, batch_size):\r\n",
        "        j = torch.LongTensor(indices[i: min(i + batch_size, num_examples)]) # 最后一次可能不足一个batch\r\n",
        "        yield  features.index_select(0, j), labels.index_select(0, j) \r\n",
        "\r\n",
        "def linreg(X, w, b):\r\n",
        "    return torch.mm(X, w) + b\r\n",
        "\r\n",
        "def squared_loss(y_hat, y): \r\n",
        "    # 注意这里返回的是向量, 另外, pytorch里的MSELoss并没有除以 2\r\n",
        "    return ((y_hat - y.view(y_hat.size())) ** 2) / 2\r\n",
        "\r\n",
        "def sgd(params, lr, batch_size):\r\n",
        "    # 为了和原书保持一致，这里除以了batch_size，但是应该是不用除的，因为一般用PyTorch计算loss时就默认已经\r\n",
        "    # 沿batch维求了平均了。\r\n",
        "    for param in params:\r\n",
        "        param.data -= lr * param.grad / batch_size # 注意这里更改param时用的param.data\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ######################3##### 3.5 #############################\r\n",
        "def get_fashion_mnist_labels(labels):\r\n",
        "    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\r\n",
        "                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\r\n",
        "    return [text_labels[int(i)] for i in labels]\r\n",
        "\r\n",
        "def show_fashion_mnist(images, labels):\r\n",
        "    use_svg_display()\r\n",
        "    # 这里的_表示我们忽略（不使用）的变量\r\n",
        "    _, figs = plt.subplots(1, len(images), figsize=(12, 12))\r\n",
        "    for f, img, lbl in zip(figs, images, labels):\r\n",
        "        f.imshow(img.view((28, 28)).numpy())\r\n",
        "        f.set_title(lbl)\r\n",
        "        f.axes.get_xaxis().set_visible(False)\r\n",
        "        f.axes.get_yaxis().set_visible(False)\r\n",
        "    # plt.show()\r\n",
        "\r\n",
        "# 5.6 修改\r\n",
        "# def load_data_fashion_mnist(batch_size, root='~/Datasets/FashionMNIST'):\r\n",
        "#     \"\"\"Download the fashion mnist dataset and then load into memory.\"\"\"\r\n",
        "#     transform = transforms.ToTensor()\r\n",
        "#     mnist_train = torchvision.datasets.FashionMNIST(root=root, train=True, download=True, transform=transform)\r\n",
        "#     mnist_test = torchvision.datasets.FashionMNIST(root=root, train=False, download=True, transform=transform)\r\n",
        "#     if sys.platform.startswith('win'):\r\n",
        "#         num_workers = 0  # 0表示不用额外的进程来加速读取数据\r\n",
        "#     else:\r\n",
        "#         num_workers = 4\r\n",
        "#     train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\r\n",
        "#     test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\r\n",
        "\r\n",
        "#     return train_iter, test_iter\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ########################### 3.6  ###############################\r\n",
        "# (3.13节修改)\r\n",
        "# def evaluate_accuracy(data_iter, net):\r\n",
        "#     acc_sum, n = 0.0, 0\r\n",
        "#     for X, y in data_iter:\r\n",
        "#         acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()\r\n",
        "#         n += y.shape[0]\r\n",
        "#     return acc_sum / n\r\n",
        "\r\n",
        "\r\n",
        "def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,\r\n",
        "              params=None, lr=None, optimizer=None):\r\n",
        "    for epoch in range(num_epochs):\r\n",
        "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\r\n",
        "        for X, y in train_iter:\r\n",
        "            y_hat = net(X)\r\n",
        "            l = loss(y_hat, y).sum()\r\n",
        "            \r\n",
        "            # 梯度清零\r\n",
        "            if optimizer is not None:\r\n",
        "                optimizer.zero_grad()\r\n",
        "            elif params is not None and params[0].grad is not None:\r\n",
        "                for param in params:\r\n",
        "                    param.grad.data.zero_()\r\n",
        "            \r\n",
        "            l.backward()\r\n",
        "            if optimizer is None:\r\n",
        "                sgd(params, lr, batch_size)\r\n",
        "            else:\r\n",
        "                optimizer.step()  # “softmax回归的简洁实现”一节将用到\r\n",
        "            \r\n",
        "            \r\n",
        "            train_l_sum += l.item()\r\n",
        "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\r\n",
        "            n += y.shape[0]\r\n",
        "        test_acc = evaluate_accuracy(test_iter, net)\r\n",
        "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\r\n",
        "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ########################### 3.7 #####################################3\r\n",
        "class FlattenLayer(torch.nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(FlattenLayer, self).__init__()\r\n",
        "    def forward(self, x): # x shape: (batch, *, *, ...)\r\n",
        "        return x.view(x.shape[0], -1)\r\n",
        "\r\n",
        "\r\n",
        "# ########################### 3.11 ###############################\r\n",
        "def semilogy(x_vals, y_vals, x_label, y_label, x2_vals=None, y2_vals=None,\r\n",
        "             legend=None, figsize=(3.5, 2.5)):\r\n",
        "    set_figsize(figsize)\r\n",
        "    plt.xlabel(x_label)\r\n",
        "    plt.ylabel(y_label)\r\n",
        "    plt.semilogy(x_vals, y_vals)\r\n",
        "    if x2_vals and y2_vals:\r\n",
        "        plt.semilogy(x2_vals, y2_vals, linestyle=':')\r\n",
        "        plt.legend(legend)\r\n",
        "    # plt.show()\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ############################# 3.13 ##############################\r\n",
        "# 5.5 修改\r\n",
        "# def evaluate_accuracy(data_iter, net):\r\n",
        "#     acc_sum, n = 0.0, 0\r\n",
        "#     for X, y in data_iter:\r\n",
        "#         if isinstance(net, torch.nn.Module):\r\n",
        "#             net.eval() # 评估模式, 这会关闭dropout\r\n",
        "#             acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()\r\n",
        "#             net.train() # 改回训练模式\r\n",
        "#         else: # 自定义的模型\r\n",
        "#             if('is_training' in net.__code__.co_varnames): # 如果有is_training这个参数\r\n",
        "#                 # 将is_training设置成False\r\n",
        "#                 acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() \r\n",
        "#             else:\r\n",
        "#                 acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() \r\n",
        "#         n += y.shape[0]\r\n",
        "#     return acc_sum / n\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ########################### 5.1 #########################\r\n",
        "def corr2d(X, K):  \r\n",
        "    h, w = K.shape\r\n",
        "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\r\n",
        "    for i in range(Y.shape[0]):\r\n",
        "        for j in range(Y.shape[1]):\r\n",
        "            Y[i, j] = (X[i: i + h, j: j + w] * K).sum()\r\n",
        "    return Y\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ############################ 5.5 #########################\r\n",
        "def evaluate_accuracy(data_iter, net, device=None):\r\n",
        "    if device is None and isinstance(net, torch.nn.Module):\r\n",
        "        # 如果没指定device就使用net的device\r\n",
        "        device = list(net.parameters())[0].device \r\n",
        "    acc_sum, n = 0.0, 0\r\n",
        "    with torch.no_grad():\r\n",
        "        for X, y in data_iter:\r\n",
        "            if isinstance(net, torch.nn.Module):\r\n",
        "                net.eval() # 评估模式, 这会关闭dropout\r\n",
        "                acc_sum += (net(X.to(device)).argmax(dim=1) == y.to(device)).float().sum().cpu().item()\r\n",
        "                net.train() # 改回训练模式\r\n",
        "            else: # 自定义的模型, 3.13节之后不会用到, 不考虑GPU\r\n",
        "                if('is_training' in net.__code__.co_varnames): # 如果有is_training这个参数\r\n",
        "                    # 将is_training设置成False\r\n",
        "                    acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() \r\n",
        "                else:\r\n",
        "                    acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() \r\n",
        "            n += y.shape[0]\r\n",
        "    return acc_sum / n\r\n",
        "\r\n",
        "def train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs):\r\n",
        "    net = net.to(device)\r\n",
        "    print(\"training on \", device)\r\n",
        "    loss = torch.nn.CrossEntropyLoss()\r\n",
        "    for epoch in range(num_epochs):\r\n",
        "        train_l_sum, train_acc_sum, n, batch_count, start = 0.0, 0.0, 0, 0, time.time()\r\n",
        "        for X, y in train_iter:\r\n",
        "            X = X.to(device)\r\n",
        "            y = y.to(device)\r\n",
        "            y_hat = net(X)\r\n",
        "            l = loss(y_hat, y)\r\n",
        "            optimizer.zero_grad()\r\n",
        "            l.backward()\r\n",
        "            optimizer.step()\r\n",
        "            train_l_sum += l.cpu().item()\r\n",
        "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\r\n",
        "            n += y.shape[0]\r\n",
        "            batch_count += 1\r\n",
        "        test_acc = evaluate_accuracy(test_iter, net)\r\n",
        "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\r\n",
        "              % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ########################## 5.6 #########################3\r\n",
        "def load_data_fashion_mnist(batch_size, resize=None, root='~/Datasets/FashionMNIST'):\r\n",
        "    \"\"\"Download the fashion mnist dataset and then load into memory.\"\"\"\r\n",
        "    trans = []\r\n",
        "    if resize:\r\n",
        "        trans.append(torchvision.transforms.Resize(size=resize))\r\n",
        "    trans.append(torchvision.transforms.ToTensor())\r\n",
        "    \r\n",
        "    transform = torchvision.transforms.Compose(trans)\r\n",
        "    mnist_train = torchvision.datasets.FashionMNIST(root=root, train=True, download=True, transform=transform)\r\n",
        "    mnist_test = torchvision.datasets.FashionMNIST(root=root, train=False, download=True, transform=transform)\r\n",
        "    if sys.platform.startswith('win'):\r\n",
        "        num_workers = 0  # 0表示不用额外的进程来加速读取数据\r\n",
        "    else:\r\n",
        "        num_workers = 4\r\n",
        "    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\r\n",
        "    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\r\n",
        "\r\n",
        "    return train_iter, test_iter\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "############################# 5.8 ##############################\r\n",
        "class GlobalAvgPool2d(nn.Module):\r\n",
        "    # 全局平均池化层可通过将池化窗口形状设置成输入的高和宽实现\r\n",
        "    def __init__(self):\r\n",
        "        super(GlobalAvgPool2d, self).__init__()\r\n",
        "    def forward(self, x):\r\n",
        "        return F.avg_pool2d(x, kernel_size=x.size()[2:])\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ########################### 5.11 ################################\r\n",
        "class Residual(nn.Module): \r\n",
        "    def __init__(self, in_channels, out_channels, use_1x1conv=False, stride=1):\r\n",
        "        super(Residual, self).__init__()\r\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride)\r\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\r\n",
        "        if use_1x1conv:\r\n",
        "            self.conv3 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride)\r\n",
        "        else:\r\n",
        "            self.conv3 = None\r\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\r\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\r\n",
        "\r\n",
        "    def forward(self, X):\r\n",
        "        Y = F.relu(self.bn1(self.conv1(X)))\r\n",
        "        Y = self.bn2(self.conv2(Y))\r\n",
        "        if self.conv3:\r\n",
        "            X = self.conv3(X)\r\n",
        "        return F.relu(Y + X)\r\n",
        "\r\n",
        "def resnet_block(in_channels, out_channels, num_residuals, first_block=False):\r\n",
        "    if first_block:\r\n",
        "        assert in_channels == out_channels # 第一个模块的通道数同输入通道数一致\r\n",
        "    blk = []\r\n",
        "    for i in range(num_residuals):\r\n",
        "        if i == 0 and not first_block:\r\n",
        "            blk.append(Residual(in_channels, out_channels, use_1x1conv=True, stride=2))\r\n",
        "        else:\r\n",
        "            blk.append(Residual(out_channels, out_channels))\r\n",
        "    return nn.Sequential(*blk)\r\n",
        "    \r\n",
        "def resnet18(output=10, in_channels=3):\r\n",
        "    net = nn.Sequential(\r\n",
        "        nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3),\r\n",
        "        nn.BatchNorm2d(64), \r\n",
        "        nn.ReLU(),\r\n",
        "        nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\r\n",
        "    net.add_module(\"resnet_block1\", resnet_block(64, 64, 2, first_block=True))\r\n",
        "    net.add_module(\"resnet_block2\", resnet_block(64, 128, 2))\r\n",
        "    net.add_module(\"resnet_block3\", resnet_block(128, 256, 2))\r\n",
        "    net.add_module(\"resnet_block4\", resnet_block(256, 512, 2))\r\n",
        "    net.add_module(\"global_avg_pool\", GlobalAvgPool2d()) # GlobalAvgPool2d的输出: (Batch, 512, 1, 1)\r\n",
        "    net.add_module(\"fc\", nn.Sequential(FlattenLayer(), nn.Linear(512, output))) \r\n",
        "    return net\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ############################## 6.3 ##################################3\r\n",
        "def load_data_jay_lyrics():\r\n",
        "    \"\"\"加载周杰伦歌词数据集\"\"\"\r\n",
        "    with zipfile.ZipFile('../../data/jaychou_lyrics.txt.zip') as zin:\r\n",
        "        with zin.open('jaychou_lyrics.txt') as f:\r\n",
        "            corpus_chars = f.read().decode('utf-8')\r\n",
        "    corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\r\n",
        "    corpus_chars = corpus_chars[0:10000]\r\n",
        "    idx_to_char = list(set(corpus_chars))\r\n",
        "    char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\r\n",
        "    vocab_size = len(char_to_idx)\r\n",
        "    corpus_indices = [char_to_idx[char] for char in corpus_chars]\r\n",
        "    return corpus_indices, char_to_idx, idx_to_char, vocab_size\r\n",
        "\r\n",
        "def data_iter_random(corpus_indices, batch_size, num_steps, device=None):\r\n",
        "    # 减1是因为输出的索引x是相应输入的索引y加1\r\n",
        "    num_examples = (len(corpus_indices) - 1) // num_steps\r\n",
        "    epoch_size = num_examples // batch_size\r\n",
        "    example_indices = list(range(num_examples))\r\n",
        "    random.shuffle(example_indices)\r\n",
        "\r\n",
        "    # 返回从pos开始的长为num_steps的序列\r\n",
        "    def _data(pos):\r\n",
        "        return corpus_indices[pos: pos + num_steps]\r\n",
        "    if device is None:\r\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "    \r\n",
        "    for i in range(epoch_size):\r\n",
        "        # 每次读取batch_size个随机样本\r\n",
        "        i = i * batch_size\r\n",
        "        batch_indices = example_indices[i: i + batch_size]\r\n",
        "        X = [_data(j * num_steps) for j in batch_indices]\r\n",
        "        Y = [_data(j * num_steps + 1) for j in batch_indices]\r\n",
        "        yield torch.tensor(X, dtype=torch.float32, device=device), torch.tensor(Y, dtype=torch.float32, device=device)\r\n",
        "\r\n",
        "def data_iter_consecutive(corpus_indices, batch_size, num_steps, device=None):\r\n",
        "    if device is None:\r\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "    corpus_indices = torch.tensor(corpus_indices, dtype=torch.float32, device=device)\r\n",
        "    data_len = len(corpus_indices)\r\n",
        "    batch_len = data_len // batch_size\r\n",
        "    indices = corpus_indices[0: batch_size*batch_len].view(batch_size, batch_len)\r\n",
        "    epoch_size = (batch_len - 1) // num_steps\r\n",
        "    for i in range(epoch_size):\r\n",
        "        i = i * num_steps\r\n",
        "        X = indices[:, i: i + num_steps]\r\n",
        "        Y = indices[:, i + 1: i + num_steps + 1]\r\n",
        "        yield X, Y\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ###################################### 6.4 ######################################\r\n",
        "def one_hot(x, n_class, dtype=torch.float32): \r\n",
        "    # X shape: (batch), output shape: (batch, n_class)\r\n",
        "    x = x.long()\r\n",
        "    res = torch.zeros(x.shape[0], n_class, dtype=dtype, device=x.device)\r\n",
        "    res.scatter_(1, x.view(-1, 1), 1)\r\n",
        "    return res\r\n",
        "\r\n",
        "def to_onehot(X, n_class):  \r\n",
        "    # X shape: (batch, seq_len), output: seq_len elements of (batch, n_class)\r\n",
        "    return [one_hot(X[:, i], n_class) for i in range(X.shape[1])]\r\n",
        "\r\n",
        "def predict_rnn(prefix, num_chars, rnn, params, init_rnn_state,\r\n",
        "                num_hiddens, vocab_size, device, idx_to_char, char_to_idx):\r\n",
        "    state = init_rnn_state(1, num_hiddens, device)\r\n",
        "    output = [char_to_idx[prefix[0]]]\r\n",
        "    for t in range(num_chars + len(prefix) - 1):\r\n",
        "        # 将上一时间步的输出作为当前时间步的输入\r\n",
        "        X = to_onehot(torch.tensor([[output[-1]]], device=device), vocab_size)\r\n",
        "        # 计算输出和更新隐藏状态\r\n",
        "        (Y, state) = rnn(X, state, params)\r\n",
        "        # 下一个时间步的输入是prefix里的字符或者当前的最佳预测字符\r\n",
        "        if t < len(prefix) - 1:\r\n",
        "            output.append(char_to_idx[prefix[t + 1]])\r\n",
        "        else:\r\n",
        "            output.append(int(Y[0].argmax(dim=1).item()))\r\n",
        "    return ''.join([idx_to_char[i] for i in output])\r\n",
        "\r\n",
        "def grad_clipping(params, theta, device):\r\n",
        "    norm = torch.tensor([0.0], device=device)\r\n",
        "    for param in params:\r\n",
        "        norm += (param.grad.data ** 2).sum()\r\n",
        "    norm = norm.sqrt().item()\r\n",
        "    if norm > theta:\r\n",
        "        for param in params:\r\n",
        "            param.grad.data *= (theta / norm)\r\n",
        "\r\n",
        "def train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\r\n",
        "                          vocab_size, device, corpus_indices, idx_to_char,\r\n",
        "                          char_to_idx, is_random_iter, num_epochs, num_steps,\r\n",
        "                          lr, clipping_theta, batch_size, pred_period,\r\n",
        "                          pred_len, prefixes):\r\n",
        "    if is_random_iter:\r\n",
        "        data_iter_fn = data_iter_random\r\n",
        "    else:\r\n",
        "        data_iter_fn = data_iter_consecutive\r\n",
        "    params = get_params()\r\n",
        "    loss = nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "    for epoch in range(num_epochs):\r\n",
        "        if not is_random_iter:  # 如使用相邻采样，在epoch开始时初始化隐藏状态\r\n",
        "            state = init_rnn_state(batch_size, num_hiddens, device)\r\n",
        "        l_sum, n, start = 0.0, 0, time.time()\r\n",
        "        data_iter = data_iter_fn(corpus_indices, batch_size, num_steps, device)\r\n",
        "        for X, Y in data_iter:\r\n",
        "            if is_random_iter:  # 如使用随机采样，在每个小批量更新前初始化隐藏状态\r\n",
        "                state = init_rnn_state(batch_size, num_hiddens, device)\r\n",
        "            else: \r\n",
        "            # 否则需要使用detach函数从计算图分离隐藏状态, 这是为了\r\n",
        "            # 使模型参数的梯度计算只依赖一次迭代读取的小批量序列(防止梯度计算开销太大)\r\n",
        "                for s in state:\r\n",
        "                    s.detach_()\r\n",
        "            \r\n",
        "            inputs = to_onehot(X, vocab_size)\r\n",
        "            # outputs有num_steps个形状为(batch_size, vocab_size)的矩阵\r\n",
        "            (outputs, state) = rnn(inputs, state, params)\r\n",
        "            # 拼接之后形状为(num_steps * batch_size, vocab_size)\r\n",
        "            outputs = torch.cat(outputs, dim=0)\r\n",
        "            # Y的形状是(batch_size, num_steps)，转置后再变成长度为\r\n",
        "            # batch * num_steps 的向量，这样跟输出的行一一对应\r\n",
        "            y = torch.transpose(Y, 0, 1).contiguous().view(-1)\r\n",
        "            # 使用交叉熵损失计算平均分类误差\r\n",
        "            l = loss(outputs, y.long())\r\n",
        "            \r\n",
        "            # 梯度清0\r\n",
        "            if params[0].grad is not None:\r\n",
        "                for param in params:\r\n",
        "                    param.grad.data.zero_()\r\n",
        "            l.backward()\r\n",
        "            grad_clipping(params, clipping_theta, device)  # 裁剪梯度\r\n",
        "            sgd(params, lr, 1)  # 因为误差已经取过均值，梯度不用再做平均\r\n",
        "            l_sum += l.item() * y.shape[0]\r\n",
        "            n += y.shape[0]\r\n",
        "\r\n",
        "        if (epoch + 1) % pred_period == 0:\r\n",
        "            print('epoch %d, perplexity %f, time %.2f sec' % (\r\n",
        "                epoch + 1, math.exp(l_sum / n), time.time() - start))\r\n",
        "            for prefix in prefixes:\r\n",
        "                print(' -', predict_rnn(prefix, pred_len, rnn, params, init_rnn_state,\r\n",
        "                    num_hiddens, vocab_size, device, idx_to_char, char_to_idx))\r\n",
        "\r\n",
        "                \r\n",
        "                \r\n",
        "                \r\n",
        "# ################################### 6.5 ################################################\r\n",
        "class RNNModel(nn.Module):\r\n",
        "    def __init__(self, rnn_layer, vocab_size):\r\n",
        "        super(RNNModel, self).__init__()\r\n",
        "        self.rnn = rnn_layer\r\n",
        "        self.hidden_size = rnn_layer.hidden_size * (2 if rnn_layer.bidirectional else 1) \r\n",
        "        self.vocab_size = vocab_size\r\n",
        "        self.dense = nn.Linear(self.hidden_size, vocab_size)\r\n",
        "        self.state = None\r\n",
        "\r\n",
        "    def forward(self, inputs, state): # inputs: (batch, seq_len)\r\n",
        "        # 获取one-hot向量表示\r\n",
        "        X = to_onehot(inputs, self.vocab_size) # X是个list\r\n",
        "        Y, self.state = self.rnn(torch.stack(X), state)\r\n",
        "        # 全连接层会首先将Y的形状变成(num_steps * batch_size, num_hiddens)，它的输出\r\n",
        "        # 形状为(num_steps * batch_size, vocab_size)\r\n",
        "        output = self.dense(Y.view(-1, Y.shape[-1]))\r\n",
        "        return output, self.state\r\n",
        "\r\n",
        "def predict_rnn_pytorch(prefix, num_chars, model, vocab_size, device, idx_to_char,\r\n",
        "                      char_to_idx):\r\n",
        "    state = None\r\n",
        "    output = [char_to_idx[prefix[0]]] # output会记录prefix加上输出\r\n",
        "    for t in range(num_chars + len(prefix) - 1):\r\n",
        "        X = torch.tensor([output[-1]], device=device).view(1, 1)\r\n",
        "        if state is not None:\r\n",
        "            if isinstance(state, tuple): # LSTM, state:(h, c)  \r\n",
        "                state = (state[0].to(device), state[1].to(device))\r\n",
        "            else:   \r\n",
        "                state = state.to(device)\r\n",
        "            \r\n",
        "        (Y, state) = model(X, state)  # 前向计算不需要传入模型参数\r\n",
        "        if t < len(prefix) - 1:\r\n",
        "            output.append(char_to_idx[prefix[t + 1]])\r\n",
        "        else:\r\n",
        "            output.append(int(Y.argmax(dim=1).item()))\r\n",
        "    return ''.join([idx_to_char[i] for i in output])\r\n",
        "\r\n",
        "def train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,\r\n",
        "                                corpus_indices, idx_to_char, char_to_idx,\r\n",
        "                                num_epochs, num_steps, lr, clipping_theta,\r\n",
        "                                batch_size, pred_period, pred_len, prefixes):\r\n",
        "    loss = nn.CrossEntropyLoss()\r\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\r\n",
        "    model.to(device)\r\n",
        "    state = None\r\n",
        "    for epoch in range(num_epochs):\r\n",
        "        l_sum, n, start = 0.0, 0, time.time()\r\n",
        "        data_iter = data_iter_consecutive(corpus_indices, batch_size, num_steps, device) # 相邻采样\r\n",
        "        for X, Y in data_iter:\r\n",
        "            if state is not None:\r\n",
        "                # 使用detach函数从计算图分离隐藏状态, 这是为了\r\n",
        "                # 使模型参数的梯度计算只依赖一次迭代读取的小批量序列(防止梯度计算开销太大)\r\n",
        "                if isinstance (state, tuple): # LSTM, state:(h, c)  \r\n",
        "                    state = (state[0].detach(), state[1].detach())\r\n",
        "                else:   \r\n",
        "                    state = state.detach()\r\n",
        "    \r\n",
        "            (output, state) = model(X, state) # output: 形状为(num_steps * batch_size, vocab_size)\r\n",
        "            \r\n",
        "            # Y的形状是(batch_size, num_steps)，转置后再变成长度为\r\n",
        "            # batch * num_steps 的向量，这样跟输出的行一一对应\r\n",
        "            y = torch.transpose(Y, 0, 1).contiguous().view(-1)\r\n",
        "            l = loss(output, y.long())\r\n",
        "            \r\n",
        "            optimizer.zero_grad()\r\n",
        "            l.backward()\r\n",
        "            # 梯度裁剪\r\n",
        "            grad_clipping(model.parameters(), clipping_theta, device)\r\n",
        "            optimizer.step()\r\n",
        "            l_sum += l.item() * y.shape[0]\r\n",
        "            n += y.shape[0]\r\n",
        "        \r\n",
        "        try:\r\n",
        "            perplexity = math.exp(l_sum / n)\r\n",
        "        except OverflowError:\r\n",
        "            perplexity = float('inf')\r\n",
        "        if (epoch + 1) % pred_period == 0:\r\n",
        "            print('epoch %d, perplexity %f, time %.2f sec' % (\r\n",
        "                epoch + 1, perplexity, time.time() - start))\r\n",
        "            for prefix in prefixes:\r\n",
        "                print(' -', predict_rnn_pytorch(\r\n",
        "                    prefix, pred_len, model, vocab_size, device, idx_to_char,\r\n",
        "                    char_to_idx))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ######################################## 7.2 ###############################################\r\n",
        "def train_2d(trainer):  \r\n",
        "    x1, x2, s1, s2 = -5, -2, 0, 0  # s1和s2是自变量状态，本章后续几节会使用\r\n",
        "    results = [(x1, x2)]\r\n",
        "    for i in range(20):\r\n",
        "        x1, x2, s1, s2 = trainer(x1, x2, s1, s2)\r\n",
        "        results.append((x1, x2))\r\n",
        "    print('epoch %d, x1 %f, x2 %f' % (i + 1, x1, x2))\r\n",
        "    return results\r\n",
        "\r\n",
        "def show_trace_2d(f, results):  \r\n",
        "    plt.plot(*zip(*results), '-o', color='#ff7f0e')\r\n",
        "    x1, x2 = np.meshgrid(np.arange(-5.5, 1.0, 0.1), np.arange(-3.0, 1.0, 0.1))\r\n",
        "    plt.contour(x1, x2, f(x1, x2), colors='#1f77b4')\r\n",
        "    plt.xlabel('x1')\r\n",
        "    plt.ylabel('x2')\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ######################################## 7.3 ###############################################\r\n",
        "def get_data_ch7():  \r\n",
        "    data = np.genfromtxt('../../data/airfoil_self_noise.dat', delimiter='\\t')\r\n",
        "    data = (data - data.mean(axis=0)) / data.std(axis=0)\r\n",
        "    return torch.tensor(data[:1500, :-1], dtype=torch.float32), \\\r\n",
        "        torch.tensor(data[:1500, -1], dtype=torch.float32) # 前1500个样本(每个样本5个特征)\r\n",
        "\r\n",
        "def train_ch7(optimizer_fn, states, hyperparams, features, labels,\r\n",
        "              batch_size=10, num_epochs=2):\r\n",
        "    # 初始化模型\r\n",
        "    net, loss = linreg, squared_loss\r\n",
        "    \r\n",
        "    w = torch.nn.Parameter(torch.tensor(np.random.normal(0, 0.01, size=(features.shape[1], 1)), dtype=torch.float32),\r\n",
        "                           requires_grad=True)\r\n",
        "    b = torch.nn.Parameter(torch.zeros(1, dtype=torch.float32), requires_grad=True)\r\n",
        "\r\n",
        "    def eval_loss():\r\n",
        "        return loss(net(features, w, b), labels).mean().item()\r\n",
        "\r\n",
        "    ls = [eval_loss()]\r\n",
        "    data_iter = torch.utils.data.DataLoader(\r\n",
        "        torch.utils.data.TensorDataset(features, labels), batch_size, shuffle=True)\r\n",
        "    \r\n",
        "    for _ in range(num_epochs):\r\n",
        "        start = time.time()\r\n",
        "        for batch_i, (X, y) in enumerate(data_iter):\r\n",
        "            l = loss(net(X, w, b), y).mean()  # 使用平均损失\r\n",
        "            \r\n",
        "            # 梯度清零\r\n",
        "            if w.grad is not None:\r\n",
        "                w.grad.data.zero_()\r\n",
        "                b.grad.data.zero_()\r\n",
        "                \r\n",
        "            l.backward()\r\n",
        "            optimizer_fn([w, b], states, hyperparams)  # 迭代模型参数\r\n",
        "            if (batch_i + 1) * batch_size % 100 == 0:\r\n",
        "                ls.append(eval_loss())  # 每100个样本记录下当前训练误差\r\n",
        "    # 打印结果和作图\r\n",
        "    print('loss: %f, %f sec per epoch' % (ls[-1], time.time() - start))\r\n",
        "    set_figsize()\r\n",
        "    plt.plot(np.linspace(0, num_epochs, len(ls)), ls)\r\n",
        "    plt.xlabel('epoch')\r\n",
        "    plt.ylabel('loss')\r\n",
        "\r\n",
        "# 本函数与原书不同的是这里第一个参数优化器函数而不是优化器的名字\r\n",
        "# 例如: optimizer_fn=torch.optim.SGD, optimizer_hyperparams={\"lr\": 0.05}\r\n",
        "def train_pytorch_ch7(optimizer_fn, optimizer_hyperparams, features, labels,\r\n",
        "                    batch_size=10, num_epochs=2):\r\n",
        "    # 初始化模型\r\n",
        "    net = nn.Sequential(\r\n",
        "        nn.Linear(features.shape[-1], 1)\r\n",
        "    )\r\n",
        "    loss = nn.MSELoss()\r\n",
        "    optimizer = optimizer_fn(net.parameters(), **optimizer_hyperparams)\r\n",
        "\r\n",
        "    def eval_loss():\r\n",
        "        return loss(net(features).view(-1), labels).item() / 2\r\n",
        "\r\n",
        "    ls = [eval_loss()]\r\n",
        "    data_iter = torch.utils.data.DataLoader(\r\n",
        "        torch.utils.data.TensorDataset(features, labels), batch_size, shuffle=True)\r\n",
        "\r\n",
        "    for _ in range(num_epochs):\r\n",
        "        start = time.time()\r\n",
        "        for batch_i, (X, y) in enumerate(data_iter):\r\n",
        "            # 除以2是为了和train_ch7保持一致, 因为squared_loss中除了2\r\n",
        "            l = loss(net(X).view(-1), y) / 2 \r\n",
        "            \r\n",
        "            optimizer.zero_grad()\r\n",
        "            l.backward()\r\n",
        "            optimizer.step()\r\n",
        "            if (batch_i + 1) * batch_size % 100 == 0:\r\n",
        "                ls.append(eval_loss())\r\n",
        "    # 打印结果和作图\r\n",
        "    print('loss: %f, %f sec per epoch' % (ls[-1], time.time() - start))\r\n",
        "    set_figsize()\r\n",
        "    plt.plot(np.linspace(0, num_epochs, len(ls)), ls)\r\n",
        "    plt.xlabel('epoch')\r\n",
        "    plt.ylabel('loss')\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "############################## 8.3 ##################################\r\n",
        "class Benchmark():\r\n",
        "    def __init__(self, prefix=None):\r\n",
        "        self.prefix = prefix + ' ' if prefix else ''\r\n",
        "\r\n",
        "    def __enter__(self):\r\n",
        "        self.start = time.time()\r\n",
        "\r\n",
        "    def __exit__(self, *args):\r\n",
        "        print('%stime: %.4f sec' % (self.prefix, time.time() - self.start))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ########################### 9.1 ########################################\r\n",
        "def show_images(imgs, num_rows, num_cols, scale=2):\r\n",
        "    figsize = (num_cols * scale, num_rows * scale)\r\n",
        "    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)\r\n",
        "    for i in range(num_rows):\r\n",
        "        for j in range(num_cols):\r\n",
        "            axes[i][j].imshow(imgs[i * num_cols + j])\r\n",
        "            axes[i][j].axes.get_xaxis().set_visible(False)\r\n",
        "            axes[i][j].axes.get_yaxis().set_visible(False)\r\n",
        "    return axes\r\n",
        "\r\n",
        "def train(train_iter, test_iter, net, loss, optimizer, device, num_epochs):\r\n",
        "    net = net.to(device)\r\n",
        "    print(\"training on \", device)\r\n",
        "    batch_count = 0\r\n",
        "    for epoch in range(num_epochs):\r\n",
        "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\r\n",
        "        for X, y in train_iter:\r\n",
        "            X = X.to(device)\r\n",
        "            y = y.to(device)\r\n",
        "            y_hat = net(X)\r\n",
        "            l = loss(y_hat, y) \r\n",
        "            optimizer.zero_grad()\r\n",
        "            l.backward()\r\n",
        "            optimizer.step()\r\n",
        "            train_l_sum += l.cpu().item()\r\n",
        "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\r\n",
        "            n += y.shape[0]\r\n",
        "            batch_count += 1\r\n",
        "        test_acc = evaluate_accuracy(test_iter, net)\r\n",
        "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\r\n",
        "              % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "############################## 9.3 #####################\r\n",
        "def bbox_to_rect(bbox, color):\r\n",
        "    # 将边界框(左上x, 左上y, 右下x, 右下y)格式转换成matplotlib格式：\r\n",
        "    # ((左上x, 左上y), 宽, 高)\r\n",
        "    return plt.Rectangle(\r\n",
        "        xy=(bbox[0], bbox[1]), width=bbox[2]-bbox[0], height=bbox[3]-bbox[1],\r\n",
        "        fill=False, edgecolor=color, linewidth=2)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "############################ 9.4 ###########################\r\n",
        "def MultiBoxPrior(feature_map, sizes=[0.75, 0.5, 0.25], ratios=[1, 2, 0.5]):\r\n",
        "    \"\"\"\r\n",
        "    # 按照「9.4.1. 生成多个锚框」所讲的实现, anchor表示成(xmin, ymin, xmax, ymax).\r\n",
        "    https://zh.d2l.ai/chapter_computer-vision/anchor.html\r\n",
        "    Args:\r\n",
        "        feature_map: torch tensor, Shape: [N, C, H, W].\r\n",
        "        sizes: List of sizes (0~1) of generated MultiBoxPriores. \r\n",
        "        ratios: List of aspect ratios (non-negative) of generated MultiBoxPriores. \r\n",
        "    Returns:\r\n",
        "        anchors of shape (1, num_anchors, 4). 由于batch里每个都一样, 所以第一维为1\r\n",
        "    \"\"\"\r\n",
        "    pairs = [] # pair of (size, sqrt(ration))\r\n",
        "    for r in ratios:\r\n",
        "        pairs.append([sizes[0], math.sqrt(r)])\r\n",
        "    for s in sizes[1:]:\r\n",
        "        pairs.append([s, math.sqrt(ratios[0])])\r\n",
        "    \r\n",
        "    pairs = np.array(pairs)\r\n",
        "    \r\n",
        "    ss1 = pairs[:, 0] * pairs[:, 1] # size * sqrt(ration)\r\n",
        "    ss2 = pairs[:, 0] / pairs[:, 1] # size / sqrt(ration)\r\n",
        "    \r\n",
        "    base_anchors = np.stack([-ss1, -ss2, ss1, ss2], axis=1) / 2\r\n",
        "    \r\n",
        "    h, w = feature_map.shape[-2:]\r\n",
        "    shifts_x = np.arange(0, w) / w\r\n",
        "    shifts_y = np.arange(0, h) / h\r\n",
        "    shift_x, shift_y = np.meshgrid(shifts_x, shifts_y)\r\n",
        "    shift_x = shift_x.reshape(-1)\r\n",
        "    shift_y = shift_y.reshape(-1)\r\n",
        "    shifts = np.stack((shift_x, shift_y, shift_x, shift_y), axis=1)\r\n",
        "    \r\n",
        "    anchors = shifts.reshape((-1, 1, 4)) + base_anchors.reshape((1, -1, 4))\r\n",
        "    \r\n",
        "    return torch.tensor(anchors, dtype=torch.float32).view(1, -1, 4)\r\n",
        "\r\n",
        "def show_bboxes(axes, bboxes, labels=None, colors=None):\r\n",
        "    def _make_list(obj, default_values=None):\r\n",
        "        if obj is None:\r\n",
        "            obj = default_values\r\n",
        "        elif not isinstance(obj, (list, tuple)):\r\n",
        "            obj = [obj]\r\n",
        "        return obj\r\n",
        "\r\n",
        "    labels = _make_list(labels)\r\n",
        "    colors = _make_list(colors, ['b', 'g', 'r', 'm', 'c'])\r\n",
        "    for i, bbox in enumerate(bboxes):\r\n",
        "        color = colors[i % len(colors)]\r\n",
        "        rect = bbox_to_rect(bbox.detach().cpu().numpy(), color)\r\n",
        "        axes.add_patch(rect)\r\n",
        "        if labels and len(labels) > i:\r\n",
        "            text_color = 'k' if color == 'w' else 'w'\r\n",
        "            axes.text(rect.xy[0], rect.xy[1], labels[i],\r\n",
        "                      va='center', ha='center', fontsize=6, color=text_color,\r\n",
        "                      bbox=dict(facecolor=color, lw=0))\r\n",
        "\r\n",
        "def compute_intersection(set_1, set_2):\r\n",
        "    \"\"\"\r\n",
        "    计算anchor之间的交集\r\n",
        "    Args:\r\n",
        "        set_1: a tensor of dimensions (n1, 4), anchor表示成(xmin, ymin, xmax, ymax)\r\n",
        "        set_2: a tensor of dimensions (n2, 4), anchor表示成(xmin, ymin, xmax, ymax)\r\n",
        "    Returns:\r\n",
        "        intersection of each of the boxes in set 1 with respect to each of the boxes in set 2, shape: (n1, n2)\r\n",
        "    \"\"\"\r\n",
        "    # PyTorch auto-broadcasts singleton dimensions\r\n",
        "    lower_bounds = torch.max(set_1[:, :2].unsqueeze(1), set_2[:, :2].unsqueeze(0))  # (n1, n2, 2)\r\n",
        "    upper_bounds = torch.min(set_1[:, 2:].unsqueeze(1), set_2[:, 2:].unsqueeze(0))  # (n1, n2, 2)\r\n",
        "    intersection_dims = torch.clamp(upper_bounds - lower_bounds, min=0)  # (n1, n2, 2)\r\n",
        "    return intersection_dims[:, :, 0] * intersection_dims[:, :, 1]  # (n1, n2)\r\n",
        "\r\n",
        "def compute_jaccard(set_1, set_2):\r\n",
        "    \"\"\"\r\n",
        "    计算anchor之间的Jaccard系数(IoU)\r\n",
        "    Args:\r\n",
        "        set_1: a tensor of dimensions (n1, 4), anchor表示成(xmin, ymin, xmax, ymax)\r\n",
        "        set_2: a tensor of dimensions (n2, 4), anchor表示成(xmin, ymin, xmax, ymax)\r\n",
        "    Returns:\r\n",
        "        Jaccard Overlap of each of the boxes in set 1 with respect to each of the boxes in set 2, shape: (n1, n2)\r\n",
        "    \"\"\"\r\n",
        "    # Find intersections\r\n",
        "    intersection = compute_intersection(set_1, set_2)  # (n1, n2)\r\n",
        "\r\n",
        "    # Find areas of each box in both sets\r\n",
        "    areas_set_1 = (set_1[:, 2] - set_1[:, 0]) * (set_1[:, 3] - set_1[:, 1])  # (n1)\r\n",
        "    areas_set_2 = (set_2[:, 2] - set_2[:, 0]) * (set_2[:, 3] - set_2[:, 1])  # (n2)\r\n",
        "\r\n",
        "    # Find the union\r\n",
        "    # PyTorch auto-broadcasts singleton dimensions\r\n",
        "    union = areas_set_1.unsqueeze(1) + areas_set_2.unsqueeze(0) - intersection  # (n1, n2)\r\n",
        "\r\n",
        "    return intersection / union  # (n1, n2)\r\n",
        "\r\n",
        "def assign_anchor(bb, anchor, jaccard_threshold=0.5):\r\n",
        "    \"\"\"\r\n",
        "    # 按照「9.4.1. 生成多个锚框」图9.3所讲为每个anchor分配真实的bb, anchor表示成归一化(xmin, ymin, xmax, ymax).\r\n",
        "    https://zh.d2l.ai/chapter_computer-vision/anchor.html\r\n",
        "    Args:\r\n",
        "        bb: 真实边界框(bounding box), shape:（nb, 4）\r\n",
        "        anchor: 待分配的anchor, shape:（na, 4）\r\n",
        "        jaccard_threshold: 预先设定的阈值\r\n",
        "    Returns:\r\n",
        "        assigned_idx: shape: (na, ), 每个anchor分配的真实bb对应的索引, 若未分配任何bb则为-1\r\n",
        "    \"\"\"\r\n",
        "    na = anchor.shape[0]\r\n",
        "    nb = bb.shape[0]\r\n",
        "    jaccard = compute_jaccard(anchor, bb).detach().cpu().numpy() # shape: (na, nb)\r\n",
        "    assigned_idx = np.ones(na) * -1  # 初始全为-1\r\n",
        "    \r\n",
        "    # 先为每个bb分配一个anchor(不要求满足jaccard_threshold)\r\n",
        "    jaccard_cp = jaccard.copy()\r\n",
        "    for j in range(nb):\r\n",
        "        i = np.argmax(jaccard_cp[:, j])\r\n",
        "        assigned_idx[i] = j\r\n",
        "        jaccard_cp[i, :] = float(\"-inf\") # 赋值为负无穷, 相当于去掉这一行\r\n",
        "     \r\n",
        "    # 处理还未被分配的anchor, 要求满足jaccard_threshold\r\n",
        "    for i in range(na):\r\n",
        "        if assigned_idx[i] == -1:\r\n",
        "            j = np.argmax(jaccard[i, :])\r\n",
        "            if jaccard[i, j] >= jaccard_threshold:\r\n",
        "                assigned_idx[i] = j\r\n",
        "    \r\n",
        "    return torch.tensor(assigned_idx, dtype=torch.long)\r\n",
        "\r\n",
        "def xy_to_cxcy(xy):\r\n",
        "    \"\"\"\r\n",
        "    将(x_min, y_min, x_max, y_max)形式的anchor转换成(center_x, center_y, w, h)形式的.\r\n",
        "    https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/utils.py\r\n",
        "    Args:\r\n",
        "        xy: bounding boxes in boundary coordinates, a tensor of size (n_boxes, 4)\r\n",
        "    Returns: \r\n",
        "        bounding boxes in center-size coordinates, a tensor of size (n_boxes, 4)\r\n",
        "    \"\"\"\r\n",
        "    return torch.cat([(xy[:, 2:] + xy[:, :2]) / 2,  # c_x, c_y\r\n",
        "                      xy[:, 2:] - xy[:, :2]], 1)  # w, h\r\n",
        "\r\n",
        "def MultiBoxTarget(anchor, label):\r\n",
        "    \"\"\"\r\n",
        "    # 按照「9.4.1. 生成多个锚框」所讲的实现, anchor表示成归一化(xmin, ymin, xmax, ymax).\r\n",
        "    https://zh.d2l.ai/chapter_computer-vision/anchor.html\r\n",
        "    Args:\r\n",
        "        anchor: torch tensor, 输入的锚框, 一般是通过MultiBoxPrior生成, shape:（1，锚框总数，4）\r\n",
        "        label: 真实标签, shape为(bn, 每张图片最多的真实锚框数, 5)\r\n",
        "               第二维中，如果给定图片没有这么多锚框, 可以先用-1填充空白, 最后一维中的元素为[类别标签, 四个坐标值]\r\n",
        "    Returns:\r\n",
        "        列表, [bbox_offset, bbox_mask, cls_labels]\r\n",
        "        bbox_offset: 每个锚框的标注偏移量，形状为(bn，锚框总数*4)\r\n",
        "        bbox_mask: 形状同bbox_offset, 每个锚框的掩码, 一一对应上面的偏移量, 负类锚框(背景)对应的掩码均为0, 正类锚框的掩码均为1\r\n",
        "        cls_labels: 每个锚框的标注类别, 其中0表示为背景, 形状为(bn，锚框总数)\r\n",
        "    \"\"\"\r\n",
        "    assert len(anchor.shape) == 3 and len(label.shape) == 3\r\n",
        "    bn = label.shape[0]\r\n",
        "    \r\n",
        "    def MultiBoxTarget_one(anc, lab, eps=1e-6):\r\n",
        "        \"\"\"\r\n",
        "        MultiBoxTarget函数的辅助函数, 处理batch中的一个\r\n",
        "        Args:\r\n",
        "            anc: shape of (锚框总数, 4)\r\n",
        "            lab: shape of (真实锚框数, 5), 5代表[类别标签, 四个坐标值]\r\n",
        "            eps: 一个极小值, 防止log0\r\n",
        "        Returns:\r\n",
        "            offset: (锚框总数*4, )\r\n",
        "            bbox_mask: (锚框总数*4, ), 0代表背景, 1代表非背景\r\n",
        "            cls_labels: (锚框总数, 4), 0代表背景\r\n",
        "        \"\"\"\r\n",
        "        an = anc.shape[0]\r\n",
        "        assigned_idx = assign_anchor(lab[:, 1:], anc) # (锚框总数, )\r\n",
        "        bbox_mask = ((assigned_idx >= 0).float().unsqueeze(-1)).repeat(1, 4) # (锚框总数, 4)\r\n",
        "\r\n",
        "        cls_labels = torch.zeros(an, dtype=torch.long) # 0表示背景\r\n",
        "        assigned_bb = torch.zeros((an, 4), dtype=torch.float32) # 所有anchor对应的bb坐标\r\n",
        "        for i in range(an):\r\n",
        "            bb_idx = assigned_idx[i]\r\n",
        "            if bb_idx >= 0: # 即非背景\r\n",
        "                cls_labels[i] = lab[bb_idx, 0].long().item() + 1 # 注意要加一\r\n",
        "                assigned_bb[i, :] = lab[bb_idx, 1:]\r\n",
        "\r\n",
        "        center_anc = xy_to_cxcy(anc) # (center_x, center_y, w, h)\r\n",
        "        center_assigned_bb = xy_to_cxcy(assigned_bb)\r\n",
        "\r\n",
        "        offset_xy = 10.0 * (center_assigned_bb[:, :2] - center_anc[:, :2]) / center_anc[:, 2:]\r\n",
        "        offset_wh = 5.0 * torch.log(eps + center_assigned_bb[:, 2:] / center_anc[:, 2:])\r\n",
        "        offset = torch.cat([offset_xy, offset_wh], dim = 1) * bbox_mask # (锚框总数, 4)\r\n",
        "\r\n",
        "        return offset.view(-1), bbox_mask.view(-1), cls_labels\r\n",
        "    \r\n",
        "    batch_offset = []\r\n",
        "    batch_mask = []\r\n",
        "    batch_cls_labels = []\r\n",
        "    for b in range(bn):\r\n",
        "        offset, bbox_mask, cls_labels = MultiBoxTarget_one(anchor[0, :, :], label[b, :, :])\r\n",
        "        \r\n",
        "        batch_offset.append(offset)\r\n",
        "        batch_mask.append(bbox_mask)\r\n",
        "        batch_cls_labels.append(cls_labels)\r\n",
        "    \r\n",
        "    bbox_offset = torch.stack(batch_offset)\r\n",
        "    bbox_mask = torch.stack(batch_mask)\r\n",
        "    cls_labels = torch.stack(batch_cls_labels)\r\n",
        "    \r\n",
        "    return [bbox_offset, bbox_mask, cls_labels]\r\n",
        "\r\n",
        "\r\n",
        "Pred_BB_Info = namedtuple(\"Pred_BB_Info\", [\"index\", \"class_id\", \"confidence\", \"xyxy\"])\r\n",
        "def non_max_suppression(bb_info_list, nms_threshold = 0.5):\r\n",
        "    \"\"\"\r\n",
        "    非极大抑制处理预测的边界框\r\n",
        "    Args:\r\n",
        "        bb_info_list: Pred_BB_Info的列表, 包含预测类别、置信度等信息\r\n",
        "        nms_threshold: 阈值\r\n",
        "    Returns:\r\n",
        "        output: Pred_BB_Info的列表, 只保留过滤后的边界框信息\r\n",
        "    \"\"\"\r\n",
        "    output = []\r\n",
        "    # 先根据置信度从高到低排序\r\n",
        "    sorted_bb_info_list = sorted(bb_info_list, key = lambda x: x.confidence, reverse=True)\r\n",
        "\r\n",
        "    while len(sorted_bb_info_list) != 0:\r\n",
        "        best = sorted_bb_info_list.pop(0)\r\n",
        "        output.append(best)\r\n",
        "        \r\n",
        "        if len(sorted_bb_info_list) == 0:\r\n",
        "            break\r\n",
        "\r\n",
        "        bb_xyxy = []\r\n",
        "        for bb in sorted_bb_info_list:\r\n",
        "            bb_xyxy.append(bb.xyxy)\r\n",
        "        \r\n",
        "        iou = compute_jaccard(torch.tensor([best.xyxy]), \r\n",
        "                              torch.tensor(bb_xyxy))[0] # shape: (len(sorted_bb_info_list), )\r\n",
        "        \r\n",
        "        n = len(sorted_bb_info_list)\r\n",
        "        sorted_bb_info_list = [sorted_bb_info_list[i] for i in range(n) if iou[i] <= nms_threshold]\r\n",
        "    return output\r\n",
        "\r\n",
        "def MultiBoxDetection(cls_prob, loc_pred, anchor, nms_threshold = 0.5):\r\n",
        "    \"\"\"\r\n",
        "    # 按照「9.4.1. 生成多个锚框」所讲的实现, anchor表示成归一化(xmin, ymin, xmax, ymax).\r\n",
        "    https://zh.d2l.ai/chapter_computer-vision/anchor.html\r\n",
        "    Args:\r\n",
        "        cls_prob: 经过softmax后得到的各个锚框的预测概率, shape:(bn, 预测总类别数+1, 锚框个数)\r\n",
        "        loc_pred: 预测的各个锚框的偏移量, shape:(bn, 锚框个数*4)\r\n",
        "        anchor: MultiBoxPrior输出的默认锚框, shape: (1, 锚框个数, 4)\r\n",
        "        nms_threshold: 非极大抑制中的阈值\r\n",
        "    Returns:\r\n",
        "        所有锚框的信息, shape: (bn, 锚框个数, 6)\r\n",
        "        每个锚框信息由[class_id, confidence, xmin, ymin, xmax, ymax]表示\r\n",
        "        class_id=-1 表示背景或在非极大值抑制中被移除了\r\n",
        "    \"\"\"\r\n",
        "    assert len(cls_prob.shape) == 3 and len(loc_pred.shape) == 2 and len(anchor.shape) == 3\r\n",
        "    bn = cls_prob.shape[0]\r\n",
        "    \r\n",
        "    def MultiBoxDetection_one(c_p, l_p, anc, nms_threshold = 0.5):\r\n",
        "        \"\"\"\r\n",
        "        MultiBoxDetection的辅助函数, 处理batch中的一个\r\n",
        "        Args:\r\n",
        "            c_p: (预测总类别数+1, 锚框个数)\r\n",
        "            l_p: (锚框个数*4, )\r\n",
        "            anc: (锚框个数, 4)\r\n",
        "            nms_threshold: 非极大抑制中的阈值\r\n",
        "        Return:\r\n",
        "            output: (锚框个数, 6)\r\n",
        "        \"\"\"\r\n",
        "        pred_bb_num = c_p.shape[1]\r\n",
        "        anc = (anc + l_p.view(pred_bb_num, 4)).detach().cpu().numpy() # 加上偏移量\r\n",
        "        \r\n",
        "        confidence, class_id = torch.max(c_p, 0)\r\n",
        "        confidence = confidence.detach().cpu().numpy()\r\n",
        "        class_id = class_id.detach().cpu().numpy()\r\n",
        "        \r\n",
        "        pred_bb_info = [Pred_BB_Info(\r\n",
        "                            index = i,\r\n",
        "                            class_id = class_id[i] - 1, # 正类label从0开始\r\n",
        "                            confidence = confidence[i],\r\n",
        "                            xyxy=[*anc[i]]) # xyxy是个列表\r\n",
        "                        for i in range(pred_bb_num)]\r\n",
        "        \r\n",
        "        # 正类的index\r\n",
        "        obj_bb_idx = [bb.index for bb in non_max_suppression(pred_bb_info, nms_threshold)]\r\n",
        "        \r\n",
        "        output = []\r\n",
        "        for bb in pred_bb_info:\r\n",
        "            output.append([\r\n",
        "                (bb.class_id if bb.index in obj_bb_idx else -1.0),\r\n",
        "                bb.confidence,\r\n",
        "                *bb.xyxy\r\n",
        "            ])\r\n",
        "            \r\n",
        "        return torch.tensor(output) # shape: (锚框个数, 6)\r\n",
        "    \r\n",
        "    batch_output = []\r\n",
        "    for b in range(bn):\r\n",
        "        batch_output.append(MultiBoxDetection_one(cls_prob[b], loc_pred[b], anchor[0], nms_threshold))\r\n",
        "    \r\n",
        "    return torch.stack(batch_output)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ################################# 9.6 ############################\r\n",
        "class PikachuDetDataset(torch.utils.data.Dataset):\r\n",
        "    \"\"\"皮卡丘检测数据集类\"\"\"\r\n",
        "    def __init__(self, data_dir, part, image_size=(256, 256)):\r\n",
        "        assert part in [\"train\", \"val\"]\r\n",
        "        self.image_size = image_size\r\n",
        "        self.image_dir = os.path.join(data_dir, part, \"images\")\r\n",
        "        \r\n",
        "        with open(os.path.join(data_dir, part, \"label.json\")) as f:\r\n",
        "            self.label = json.load(f)\r\n",
        "            \r\n",
        "        self.transform = torchvision.transforms.Compose([\r\n",
        "            # 将 PIL 图片转换成位于[0.0, 1.0]的floatTensor, shape (C x H x W)\r\n",
        "            torchvision.transforms.ToTensor()])\r\n",
        "            \r\n",
        "    def __len__(self):\r\n",
        "        return len(self.label)\r\n",
        "    \r\n",
        "    def __getitem__(self, index):\r\n",
        "        image_path = str(index + 1) + \".png\"\r\n",
        "        \r\n",
        "        cls = self.label[image_path][\"class\"]\r\n",
        "        label = np.array([cls] + self.label[image_path][\"loc\"], \r\n",
        "                         dtype=\"float32\")[None, :]\r\n",
        "        \r\n",
        "        PIL_img = Image.open(os.path.join(self.image_dir, image_path)\r\n",
        "                            ).convert('RGB').resize(self.image_size)\r\n",
        "        img = self.transform(PIL_img)\r\n",
        "        \r\n",
        "        sample = {\r\n",
        "            \"label\": label, # shape: (1, 5) [class, xmin, ymin, xmax, ymax]\r\n",
        "            \"image\": img    # shape: (3, *image_size)\r\n",
        "        }\r\n",
        "        \r\n",
        "        return sample\r\n",
        "\r\n",
        "def load_data_pikachu(batch_size, edge_size=256, data_dir = '../../data/pikachu'):  \r\n",
        "    \"\"\"edge_size：输出图像的宽和高\"\"\"\r\n",
        "    image_size = (edge_size, edge_size)\r\n",
        "    train_dataset = PikachuDetDataset(data_dir, 'train', image_size)\r\n",
        "    val_dataset = PikachuDetDataset(data_dir, 'val', image_size)\r\n",
        "    \r\n",
        "\r\n",
        "    train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \r\n",
        "                                             shuffle=True, num_workers=4)\r\n",
        "\r\n",
        "    val_iter = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size,\r\n",
        "                                           shuffle=False, num_workers=4)\r\n",
        "    return train_iter, val_iter\r\n",
        "\r\n",
        "\r\n",
        "# ################################# 9.9 #########################\r\n",
        "def read_voc_images(root=\"../../data/VOCdevkit/VOC2012\", \r\n",
        "                    is_train=True, max_num=None):\r\n",
        "    txt_fname = '%s/ImageSets/Segmentation/%s' % (\r\n",
        "        root, 'train.txt' if is_train else 'val.txt')\r\n",
        "    with open(txt_fname, 'r') as f:\r\n",
        "        images = f.read().split()\r\n",
        "    if max_num is not None:\r\n",
        "        images = images[:min(max_num, len(images))]\r\n",
        "    features, labels = [None] * len(images), [None] * len(images)\r\n",
        "    for i, fname in tqdm(enumerate(images)):\r\n",
        "        features[i] = Image.open('%s/JPEGImages/%s.jpg' % (root, fname)).convert(\"RGB\")\r\n",
        "        labels[i] = Image.open('%s/SegmentationClass/%s.png' % (root, fname)).convert(\"RGB\")\r\n",
        "    return features, labels # PIL image\r\n",
        "\r\n",
        "# colormap2label = torch.zeros(256 ** 3, dtype=torch.uint8)\r\n",
        "# for i, colormap in enumerate(VOC_COLORMAP):\r\n",
        "#     colormap2label[(colormap[0] * 256 + colormap[1]) * 256 + colormap[2]] = i\r\n",
        "def voc_label_indices(colormap, colormap2label):\r\n",
        "    \"\"\"\r\n",
        "    convert colormap (PIL image) to colormap2label (uint8 tensor).\r\n",
        "    \"\"\"\r\n",
        "    colormap = np.array(colormap.convert(\"RGB\")).astype('int32')\r\n",
        "    idx = ((colormap[:, :, 0] * 256 + colormap[:, :, 1]) * 256\r\n",
        "           + colormap[:, :, 2])\r\n",
        "    return colormap2label[idx]\r\n",
        "\r\n",
        "def voc_rand_crop(feature, label, height, width):\r\n",
        "    \"\"\"\r\n",
        "    Random crop feature (PIL image) and label (PIL image).\r\n",
        "    \"\"\"\r\n",
        "    i, j, h, w = torchvision.transforms.RandomCrop.get_params(\r\n",
        "            feature, output_size=(height, width))\r\n",
        "    \r\n",
        "    feature = torchvision.transforms.functional.crop(feature, i, j, h, w)\r\n",
        "    label = torchvision.transforms.functional.crop(label, i, j, h, w)    \r\n",
        "\r\n",
        "    return feature, label\r\n",
        "\r\n",
        "class VOCSegDataset(torch.utils.data.Dataset):\r\n",
        "    def __init__(self, is_train, crop_size, voc_dir, colormap2label, max_num=None):\r\n",
        "        \"\"\"\r\n",
        "        crop_size: (h, w)\r\n",
        "        \"\"\"\r\n",
        "        self.rgb_mean = np.array([0.485, 0.456, 0.406])\r\n",
        "        self.rgb_std = np.array([0.229, 0.224, 0.225])\r\n",
        "        self.tsf = torchvision.transforms.Compose([\r\n",
        "            torchvision.transforms.ToTensor(),\r\n",
        "            torchvision.transforms.Normalize(mean=self.rgb_mean, \r\n",
        "                                             std=self.rgb_std)\r\n",
        "        ])\r\n",
        "        \r\n",
        "        self.crop_size = crop_size # (h, w)\r\n",
        "        features, labels = read_voc_images(root=voc_dir, \r\n",
        "                                           is_train=is_train, \r\n",
        "                                           max_num=max_num)\r\n",
        "        self.features = self.filter(features) # PIL image\r\n",
        "        self.labels = self.filter(labels)     # PIL image\r\n",
        "        self.colormap2label = colormap2label\r\n",
        "        print('read ' + str(len(self.features)) + ' valid examples')\r\n",
        "\r\n",
        "    def filter(self, imgs):\r\n",
        "        return [img for img in imgs if (\r\n",
        "            img.size[1] >= self.crop_size[0] and\r\n",
        "            img.size[0] >= self.crop_size[1])]\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        feature, label = voc_rand_crop(self.features[idx], self.labels[idx],\r\n",
        "                                       *self.crop_size)\r\n",
        "        \r\n",
        "        return (self.tsf(feature),\r\n",
        "                voc_label_indices(label, self.colormap2label))\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.features)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ############################# 10.7 ##########################\r\n",
        "def read_imdb(folder='train', data_root=\"/S1/CSCL/tangss/Datasets/aclImdb\"): \r\n",
        "    data = []\r\n",
        "    for label in ['pos', 'neg']:\r\n",
        "        folder_name = os.path.join(data_root, folder, label)\r\n",
        "        for file in tqdm(os.listdir(folder_name)):\r\n",
        "            with open(os.path.join(folder_name, file), 'rb') as f:\r\n",
        "                review = f.read().decode('utf-8').replace('\\n', '').lower()\r\n",
        "                data.append([review, 1 if label == 'pos' else 0])\r\n",
        "    random.shuffle(data)\r\n",
        "    return data\r\n",
        "\r\n",
        "def get_tokenized_imdb(data):\r\n",
        "    \"\"\"\r\n",
        "    data: list of [string, label]\r\n",
        "    \"\"\"\r\n",
        "    def tokenizer(text):\r\n",
        "        return [tok.lower() for tok in text.split(' ')]\r\n",
        "    return [tokenizer(review) for review, _ in data]\r\n",
        "\r\n",
        "def get_vocab_imdb(data):\r\n",
        "    tokenized_data = get_tokenized_imdb(data)\r\n",
        "    counter = collections.Counter([tk for st in tokenized_data for tk in st])\r\n",
        "    return torchtext.vocab.Vocab(counter, min_freq=5)\r\n",
        "\r\n",
        "def preprocess_imdb(data, vocab):\r\n",
        "    max_l = 500  # 将每条评论通过截断或者补0，使得长度变成500\r\n",
        "\r\n",
        "    def pad(x):\r\n",
        "        return x[:max_l] if len(x) > max_l else x + [0] * (max_l - len(x))\r\n",
        "\r\n",
        "    tokenized_data = get_tokenized_imdb(data)\r\n",
        "    features = torch.tensor([pad([vocab.stoi[word] for word in words]) for words in tokenized_data])\r\n",
        "    labels = torch.tensor([score for _, score in data])\r\n",
        "    return features, labels\r\n",
        "\r\n",
        "def load_pretrained_embedding(words, pretrained_vocab):\r\n",
        "    \"\"\"从预训练好的vocab中提取出words对应的词向量\"\"\"\r\n",
        "    embed = torch.zeros(len(words), pretrained_vocab.vectors[0].shape[0]) # 初始化为0\r\n",
        "    oov_count = 0 # out of vocabulary\r\n",
        "    for i, word in enumerate(words):\r\n",
        "        try:\r\n",
        "            idx = pretrained_vocab.stoi[word]\r\n",
        "            embed[i, :] = pretrained_vocab.vectors[idx]\r\n",
        "        except KeyError:\r\n",
        "            oov_count += 1\r\n",
        "    if oov_count > 0:\r\n",
        "        print(\"There are %d oov words.\" % oov_count)\r\n",
        "    return embed\r\n",
        "\r\n",
        "def predict_sentiment(net, vocab, sentence):\r\n",
        "    \"\"\"sentence是词语的列表\"\"\"\r\n",
        "    device = list(net.parameters())[0].device\r\n",
        "    sentence = torch.tensor([vocab.stoi[word] for word in sentence], device=device)\r\n",
        "    label = torch.argmax(net(sentence.view((1, -1))), dim=1)\r\n",
        "    return 'positive' if label.item() == 1 else 'negative'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XU5bUiCOQxd",
        "outputId": "67cefae7-4abe-47a3-bb3e-5c0b48dd500a"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "!nvidia-smi"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Feb 16 14:24:37 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8    11W /  70W |      3MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjJA_9a5Oij_"
      },
      "source": [
        "def nin_block(in_channels,out_channels,kernel_size,stride,padding):\r\n",
        "    blk = nn.Sequential(\r\n",
        "        nn.Conv2d(in_channels,out_channels,kernel_size,stride,padding),\r\n",
        "        nn.ReLU(),\r\n",
        "        nn.Conv2d(out_channels,out_channels,kernel_size=1),\r\n",
        "        nn.ReLU(),\r\n",
        "        nn.Conv2d(out_channels,out_channels,kernel_size=1),\r\n",
        "        nn.ReLU()\r\n",
        "    )\r\n",
        "    return blk"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZsAU6xiPKXQ"
      },
      "source": [
        "## 2.NiN模型\r\n",
        "NiN是在AlexNet问世不久后提出的。它们的卷积层设定有类似之处。NiN使用卷积窗口形状分别为11×11、5×5和3×3的卷积层，相应的输出通道数也与AlexNet中的一致。每个NiN块后接一个步幅为2、窗口形状为3×3的最大池化层。\r\n",
        "\r\n",
        "除使用NiN块以外，NiN还有一个设计与AlexNet显著不同：**NiN去掉了AlexNet最后的3个全连接层，取而代之的，NiN使用了输出通道数等于标签类别数的NiN块**，然后使用全局平均池化层对每个通道中所有元素求平均并直接用于分类。这里的全局平均池化层即窗口形状等于输入空间维形状的平均池化层。NiN的这个设计的好处是可以显著减小模型参数尺寸，从而缓解过拟合。然而，该设计有时会造成获得有效模型的训练时间的增加。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-oY93JgPKBd"
      },
      "source": [
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "class GlobalAvgPool2d(nn.Module):\r\n",
        "    # 全局平均池化层可通过将池化窗口形状设置成输入的高和宽实现\r\n",
        "    def __init__(self):\r\n",
        "        super(GlobalAvgPool2d,self).__init__()\r\n",
        "    def forward(self,x):\r\n",
        "        return F.avg_pool2d(x,kernel_size=x.size()[2:])\r\n",
        "\r\n",
        "net = nn.Sequential(\r\n",
        "    nin_block(1,96,kernel_size=11,stride=4,padding=0),\r\n",
        "    nn.MaxPool2d(kernel_size=3,stride=2),\r\n",
        "    nin_block(96,256,kernel_size=5,stride=1,padding=2),\r\n",
        "    nn.MaxPool2d(kernel_size=3,stride=2),\r\n",
        "    nin_block(256,384,kernel_size=3,stride=1,padding=1),\r\n",
        "    nn.MaxPool2d(kernel_size=3,stride=2),\r\n",
        "    nn.Dropout(0.5),\r\n",
        "    # 标签类别数是10\r\n",
        "    nin_block(384,10,kernel_size=3,stride=1,padding=1),\r\n",
        "    GlobalAvgPool2d(),\r\n",
        "    # 将四维的输出转成二维的输出，其形状为(批量大小, 10)\r\n",
        "    FlattenLayer()\r\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsTD1HMGSN0r",
        "outputId": "cebff6ee-eb0f-4d76-d42c-4bc38fb6f281"
      },
      "source": [
        "# 我们构建一个数据样本来查看每一层的输出形状\r\n",
        "X = torch.rand(1,1,224,224)\r\n",
        "for name,blk in net.named_children():\r\n",
        "    X = blk(X)\r\n",
        "    print(name,\"out_put shape:\",X.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 out_put shape: torch.Size([1, 96, 54, 54])\n",
            "1 out_put shape: torch.Size([1, 96, 26, 26])\n",
            "2 out_put shape: torch.Size([1, 256, 26, 26])\n",
            "3 out_put shape: torch.Size([1, 256, 12, 12])\n",
            "4 out_put shape: torch.Size([1, 384, 12, 12])\n",
            "5 out_put shape: torch.Size([1, 384, 5, 5])\n",
            "6 out_put shape: torch.Size([1, 384, 5, 5])\n",
            "7 out_put shape: torch.Size([1, 10, 5, 5])\n",
            "8 out_put shape: torch.Size([1, 10, 1, 1])\n",
            "9 out_put shape: torch.Size([1, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f9d8aW3Sxsn",
        "outputId": "6f2c9330-0c6b-45b7-cf9c-538f640c729a"
      },
      "source": [
        "batch_size = 128\r\n",
        "train_iter,test_iter = load_data_fashion_mnist(batch_size,resize=224)\r\n",
        "\r\n",
        "lr, num_epochs = 0.002, 5\r\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\r\n",
        "train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training on  cuda\n",
            "epoch 1, loss 1.9695, train acc 0.261, test acc 0.443, time 61.0 sec\n",
            "epoch 2, loss 1.3820, train acc 0.482, test acc 0.524, time 63.9 sec\n",
            "epoch 3, loss 1.2889, train acc 0.523, test acc 0.525, time 64.4 sec\n",
            "epoch 4, loss 1.2549, train acc 0.527, test acc 0.528, time 64.6 sec\n",
            "epoch 5, loss 1.2351, train acc 0.531, test acc 0.528, time 64.3 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWQPI1vLTJoY"
      },
      "source": [
        "## 4.小结\r\n",
        "* NiN重复使用由卷积层和代替全连接层的1×1卷积层构成的NiN块来构建深层网络。\r\n",
        "* NiN去除了容易造成过拟合的全连接输出层，而是将其替换成输出通道数等于标签类别数的NiN块和全局平均池化层。\r\n",
        "* NiN的以上设计思想影响了后面一系列卷积神经网络的设计。"
      ]
    }
  ]
}