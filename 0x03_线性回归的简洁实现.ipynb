{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0x03_线性回归的简洁实现.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOZKFvY+tjhDBe/sbhD4M55",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Matrixmax/Dive_into_DeepLearning/blob/main/0x03_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liHnuN6Iq2ix"
      },
      "source": [
        "## 1.生成数据集"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eT4o7MPrmSR"
      },
      "source": [
        "import torch\r\n",
        "import numpy as np"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2XVuQ1hqvvB"
      },
      "source": [
        "num_inputs = 2\r\n",
        "num_examples = 1000\r\n",
        "true_w = [2,-3.4]\r\n",
        "true_b = 4.2\r\n",
        "features = torch.tensor(np.random.normal(0,1,(num_examples,num_inputs)),dtype=torch.float)\r\n",
        "labels = true_w[0]*features[:,0]+true_w[1]*features[:,1]+true_b\r\n",
        "labels += torch.tensor(np.random.normal(0,0.01,size=labels.size()),dtype=torch.float)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_5IUFwYyLZO"
      },
      "source": [
        "## 2.读取数据"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYDuhCcByPxq"
      },
      "source": [
        "PyTorch提供了data包来读取数据。由于data常用作变量名，我们将导入的data模块用Data代替。在每一次迭代中，我们将随机读取包含10个数据样本的小批量。\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TIzDtMpyNRk"
      },
      "source": [
        "import torch.utils.data as Data\r\n",
        "\r\n",
        "batch_size=10\r\n",
        "# 将训练数据的特征和标签组合\r\n",
        "dataset = Data.TensorDataset(features,labels)\r\n",
        "# 随机小批量读取\r\n",
        "data_iter = Data.DataLoader(dataset,batch_size,shuffle=True)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5T1vTZ73wDg",
        "outputId": "9249c0ff-d486-4ece-d208-9f9cbf184a9f"
      },
      "source": [
        "for X,y in data_iter:\r\n",
        "    print(X,y)\r\n",
        "    break"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.4498, -0.8084],\n",
            "        [-0.0985, -1.1785],\n",
            "        [-1.0129, -0.6857],\n",
            "        [ 1.2616,  0.0954],\n",
            "        [ 1.6323, -1.0971],\n",
            "        [-0.8086,  0.4129],\n",
            "        [ 1.2255,  0.6395],\n",
            "        [-2.0344,  1.6517],\n",
            "        [-0.0966,  0.9870],\n",
            "        [-0.8488, -1.1972]]) tensor([ 7.8391,  8.0178,  4.5189,  6.4069, 11.1824,  1.1817,  4.4724, -5.4758,\n",
            "         0.6565,  6.5803])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSx3t1vG3sYu"
      },
      "source": [
        "## 3.定义模型\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1E5iL0uW4AFr",
        "outputId": "4bc67ab7-f41b-4bfe-ec33-e6b1675eec83"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "\r\n",
        "class LinearNet(nn.Module):\r\n",
        "    def __init__(self,n_feature):\r\n",
        "        super(LinearNet,self).__init__()\r\n",
        "        self.linear = nn.Linear(n_feature,1)\r\n",
        "    \r\n",
        "    # 定义向前传播\r\n",
        "    def forward(self,x):\r\n",
        "        y = self.linear()\r\n",
        "        return y\r\n",
        "\r\n",
        "net = LinearNet(num_inputs)\r\n",
        "print(net) # 使用print可以打印出网络的结构"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LinearNet(\n",
            "  (linear): Linear(in_features=2, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQEv-BW25AIB"
      },
      "source": [
        "事实上我们还可以用nn.Sequential来更加方便地搭建网络，Sequential是一个有序的容器，网络层将按照在传入Sequential的顺序依次被添加到计算图中。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcS-mKDk5Amu",
        "outputId": "567a94a8-0e3a-4b57-833a-5b0c968775dc"
      },
      "source": [
        "# 写法1\r\n",
        "net = nn.Sequential(\r\n",
        "    nn.Linear(num_inputs,1)\r\n",
        "    # 此处还可以传入其他层\r\n",
        ")\r\n",
        "\r\n",
        "# 写法2\r\n",
        "net = nn.Sequential()\r\n",
        "net.add_module('linear',nn.Linear(num_inputs,1))\r\n",
        "\r\n",
        "# 写法3\r\n",
        "from collections import OrderedDict\r\n",
        "net = nn.Sequential(OrderedDict([\r\n",
        "    ('linear', nn.Linear(num_inputs, 1))\r\n",
        "]))\r\n",
        "\r\n",
        "print(net)\r\n",
        "print(net[0])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (linear): Linear(in_features=2, out_features=1, bias=True)\n",
            ")\n",
            "Linear(in_features=2, out_features=1, bias=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKNvtIo05rdS",
        "outputId": "af00a09c-7438-48c8-ea4a-375ead7b3b65"
      },
      "source": [
        "# 可以通过net.parameters()来查看模型所有的可学习参数，此函数将返回一个生成器。\r\n",
        "for param in net.parameters():\r\n",
        "    print(param)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[0.4065, 0.4160]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0903], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xwk6owBx54aC"
      },
      "source": [
        "回顾图3.1中线性回归在神经网络图中的表示。作为一个单层神经网络，线性回归输出层中的神经元和输入层中各个输入完全连接。因此，线性回归的输出层又叫全连接层。\r\n",
        "\r\n",
        "**torch.nn仅支持输入一个batch的样本不支持单个样本输入，如果只有单个样本，可使用input.unsqueeze(0)来添加一维。**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLJKMM3Q6IpE"
      },
      "source": [
        "## 4.初始化模型参数"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWsHsJzt6Mfc"
      },
      "source": [
        "在使用net前，我们需要初始化模型参数，如线性回归模型中的权重和偏差。PyTorch在init模块中提供了多种参数初始化方法。这里的init是initializer的缩写形式。我们通过init.normal_将权重参数每个元素初始化为随机采样于均值为0、标准差为0.01的正态分布。偏差会初始化为零。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlDi72en6Hev"
      },
      "source": [
        "from torch.nn import init\r\n",
        "\r\n",
        "init.normal_(net[0].weight,mean=0,std=0.01)\r\n",
        "init.constant_(net[0].bias,val=0) # 也可以直接修改bias的data: net[0].bias.data.fill_(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzcPpkLx6zdx"
      },
      "source": [
        "# 5.定义损失函数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bqu4hvm0626K"
      },
      "source": [
        "loss =nn.MSELoss()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PofiK6E68yb"
      },
      "source": [
        "## 6.定义优化算法"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fzuse2f87MQA"
      },
      "source": [
        "torch.optim模块提供了很多常用的优化算法比如SGD、Adam和RMSProp等。下面我们创建一个用于优化net所有参数的优化器实例，并指定学习率为0.03的小批量随机梯度下降（SGD）为优化算法。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1su3f6E672o",
        "outputId": "969ec787-b2fa-4c84-909f-345feddee55e"
      },
      "source": [
        "import torch.optim as optim\r\n",
        "\r\n",
        "optimizer = optim.SGD(net.parameters(),lr=0.03)\r\n",
        "print(optimizer)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    lr: 0.03\n",
            "    momentum: 0\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkrOZIL97T4S"
      },
      "source": [
        "我们还可以为不同子网络设置不同的学习率，这在finetune时经常用到。例："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWOsEKs8744X"
      },
      "source": [
        "optimizer =optim.SGD([\r\n",
        "                # 如果对某个参数不指定学习率，就使用最外层的默认学习率\r\n",
        "                {'params': net.subnet1.parameters()}, # lr=0.03\r\n",
        "                {'params': net.subnet2.parameters(), 'lr': 0.01}\r\n",
        "            ], lr=0.03)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5410CSO76Q4"
      },
      "source": [
        "有时候我们不想让学习率固定成一个常数，那如何调整学习率呢？主要有两种做法。一种是修改optimizer.param_groups中对应的学习率，另一种是更简单也是较为推荐的做法——新建优化器，由于optimizer十分轻量级，构建开销很小，故而可以构建新的optimizer。但是后者对于使用动量的优化器（如Adam），会丢失动量等状态信息，可能会造成损失函数的收敛出现震荡等情况。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiO5Yg-i77Sv"
      },
      "source": [
        "# 调整学习率\r\n",
        "for param_group in optimizer.param_groups:\r\n",
        "    param_group['lr'] *= 0.1 # 学习率为之前的0.1倍"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR_YLKNg7_Al"
      },
      "source": [
        "## 7.训练模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-OSVmjX8bMT"
      },
      "source": [
        "在使用Gluon训练模型时，我们通过调用optim实例的step函数来迭代模型参数。按照小批量随机梯度下降的定义，我们在step函数中指明批量大小，从而对批量中样本梯度求平均。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93khap_97-Ln",
        "outputId": "2a092774-2d06-484d-8f2b-baf25e811ae2"
      },
      "source": [
        "num_epochs=3\r\n",
        "for epoch in range(1,num_epochs+1):\r\n",
        "    for X,y in data_iter:\r\n",
        "        output = net(X)\r\n",
        "        l = loss(output,y.view(-1,1))\r\n",
        "        optimizer.zero_grad() # 梯度清零，等价于net.zero_grad()\r\n",
        "        l.backward()\r\n",
        "        optimizer.step()\r\n",
        "    print('epoch %d, loss: %f' % (epoch, l.item()))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1, loss: 0.000122\n",
            "epoch 2, loss: 0.000043\n",
            "epoch 3, loss: 0.000050\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfjrM3V28lGj"
      },
      "source": [
        "下面我们分别比较学到的模型参数和真实的模型参数。我们从net获得需要的层，并访问其权重（weight）和偏差（bias）。学到的参数和真实的参数很接近。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaKZZWqO8lhT",
        "outputId": "8d21699a-384b-4c62-c384-60ed18406fd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dense = net[0]\r\n",
        "print(true_w, dense.weight)\r\n",
        "print(true_b, dense.bias)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2, -3.4] Parameter containing:\n",
            "tensor([[ 1.9990, -3.4000]], requires_grad=True)\n",
            "4.2 Parameter containing:\n",
            "tensor([4.2002], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}