{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0x27_使用重复元素的网络（VGG）.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNUp7w2mIdQV/nSHV4SbLFi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Matrixmax/Dive_into_DeepLearning/blob/main/0x27_%E4%BD%BF%E7%94%A8%E9%87%8D%E5%A4%8D%E5%85%83%E7%B4%A0%E7%9A%84%E7%BD%91%E7%BB%9C%EF%BC%88VGG%EF%BC%89.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8uyZrvvSFCO"
      },
      "source": [
        "AlexNet在LeNet的基础上增加了3个卷积层。但AlexNet作者对它们的卷积窗口、输出通道数和构造顺序均做了大量的调整。虽然AlexNet指明了深度卷积神经网络可以取得出色的结果，但并没有提供简单的规则以指导后来的研究者如何设计新的网络。我们将在本章的后续几节里介绍几种不同的深度网络设计思路。\r\n",
        "\r\n",
        "本节介绍VGG，它的名字来源于论文作者所在的实验室Visual Geometry Group [1]。VGG提出了可以通过重复使用简单的基础块来构建深度模型的思路。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjTBeQzbSOTq"
      },
      "source": [
        "## 1. VGG块\r\n",
        "VGG块的组成规律是，连续使用数个相同的填充为1，窗口形状为3x3卷积层后街上一个步幅为2，窗口形状为2x2的最大池化层。卷积层保持输入的高和宽不变，而池化层则对其减半。我们使用vgg_block函数来实现这个基础的VGG块，它可以指定卷积层的数量和输入输出通道数\r\n",
        "\r\n",
        "对于给定的感受野（与输出有关的输入图片的局部大小），采用堆积的小卷积核优于采用大的卷积核，因为可以增加网络深度来保证学习更复杂的模式，而且代价还比较小（参数更少）。例如，在VGG中，使用了3个3x3卷积核来代替7x7卷积核，使用了2个3x3卷积核来代替5*5卷积核，这样做的主要目的是在保证具有相同感知野的条件下，提升了网络的深度，在一定程度上提升了神经网络的效果。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqOC2eGHR-20"
      },
      "source": [
        "import time\r\n",
        "import torch\r\n",
        "from torch import nn, optim\r\n",
        "\r\n",
        "import collections\r\n",
        "import math\r\n",
        "import os\r\n",
        "import random\r\n",
        "import sys\r\n",
        "import tarfile\r\n",
        "import time\r\n",
        "import json\r\n",
        "import zipfile\r\n",
        "from tqdm import tqdm\r\n",
        "from PIL import Image\r\n",
        "from collections import namedtuple\r\n",
        "\r\n",
        "from IPython import display\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "import torch\r\n",
        "from torch import nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torchvision\r\n",
        "import torchvision.transforms as transforms\r\n",
        "import torchtext\r\n",
        "import torchtext.vocab as Vocab\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "\r\n",
        "VOC_CLASSES = ['background', 'aeroplane', 'bicycle', 'bird', 'boat',\r\n",
        "               'bottle', 'bus', 'car', 'cat', 'chair', 'cow',\r\n",
        "               'diningtable', 'dog', 'horse', 'motorbike', 'person',\r\n",
        "               'potted plant', 'sheep', 'sofa', 'train', 'tv/monitor']\r\n",
        "\r\n",
        "\r\n",
        "VOC_COLORMAP = [[0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0],\r\n",
        "                [0, 0, 128], [128, 0, 128], [0, 128, 128], [128, 128, 128],\r\n",
        "                [64, 0, 0], [192, 0, 0], [64, 128, 0], [192, 128, 0],\r\n",
        "                [64, 0, 128], [192, 0, 128], [64, 128, 128], [192, 128, 128],\r\n",
        "                [0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0],\r\n",
        "                [0, 64, 128]]\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ###################### 3.2 ############################\r\n",
        "def set_figsize(figsize=(3.5, 2.5)):\r\n",
        "    use_svg_display()\r\n",
        "    # 设置图的尺寸\r\n",
        "    plt.rcParams['figure.figsize'] = figsize\r\n",
        "\r\n",
        "def use_svg_display():\r\n",
        "    \"\"\"Use svg format to display plot in jupyter\"\"\"\r\n",
        "    display.set_matplotlib_formats('svg')\r\n",
        "\r\n",
        "def data_iter(batch_size, features, labels):\r\n",
        "    num_examples = len(features)\r\n",
        "    indices = list(range(num_examples))\r\n",
        "    random.shuffle(indices)  # 样本的读取顺序是随机的\r\n",
        "    for i in range(0, num_examples, batch_size):\r\n",
        "        j = torch.LongTensor(indices[i: min(i + batch_size, num_examples)]) # 最后一次可能不足一个batch\r\n",
        "        yield  features.index_select(0, j), labels.index_select(0, j) \r\n",
        "\r\n",
        "def linreg(X, w, b):\r\n",
        "    return torch.mm(X, w) + b\r\n",
        "\r\n",
        "def squared_loss(y_hat, y): \r\n",
        "    # 注意这里返回的是向量, 另外, pytorch里的MSELoss并没有除以 2\r\n",
        "    return ((y_hat - y.view(y_hat.size())) ** 2) / 2\r\n",
        "\r\n",
        "def sgd(params, lr, batch_size):\r\n",
        "    # 为了和原书保持一致，这里除以了batch_size，但是应该是不用除的，因为一般用PyTorch计算loss时就默认已经\r\n",
        "    # 沿batch维求了平均了。\r\n",
        "    for param in params:\r\n",
        "        param.data -= lr * param.grad / batch_size # 注意这里更改param时用的param.data\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ######################3##### 3.5 #############################\r\n",
        "def get_fashion_mnist_labels(labels):\r\n",
        "    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\r\n",
        "                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\r\n",
        "    return [text_labels[int(i)] for i in labels]\r\n",
        "\r\n",
        "def show_fashion_mnist(images, labels):\r\n",
        "    use_svg_display()\r\n",
        "    # 这里的_表示我们忽略（不使用）的变量\r\n",
        "    _, figs = plt.subplots(1, len(images), figsize=(12, 12))\r\n",
        "    for f, img, lbl in zip(figs, images, labels):\r\n",
        "        f.imshow(img.view((28, 28)).numpy())\r\n",
        "        f.set_title(lbl)\r\n",
        "        f.axes.get_xaxis().set_visible(False)\r\n",
        "        f.axes.get_yaxis().set_visible(False)\r\n",
        "    # plt.show()\r\n",
        "\r\n",
        "# 5.6 修改\r\n",
        "# def load_data_fashion_mnist(batch_size, root='~/Datasets/FashionMNIST'):\r\n",
        "#     \"\"\"Download the fashion mnist dataset and then load into memory.\"\"\"\r\n",
        "#     transform = transforms.ToTensor()\r\n",
        "#     mnist_train = torchvision.datasets.FashionMNIST(root=root, train=True, download=True, transform=transform)\r\n",
        "#     mnist_test = torchvision.datasets.FashionMNIST(root=root, train=False, download=True, transform=transform)\r\n",
        "#     if sys.platform.startswith('win'):\r\n",
        "#         num_workers = 0  # 0表示不用额外的进程来加速读取数据\r\n",
        "#     else:\r\n",
        "#         num_workers = 4\r\n",
        "#     train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\r\n",
        "#     test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\r\n",
        "\r\n",
        "#     return train_iter, test_iter\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ########################### 3.6  ###############################\r\n",
        "# (3.13节修改)\r\n",
        "# def evaluate_accuracy(data_iter, net):\r\n",
        "#     acc_sum, n = 0.0, 0\r\n",
        "#     for X, y in data_iter:\r\n",
        "#         acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()\r\n",
        "#         n += y.shape[0]\r\n",
        "#     return acc_sum / n\r\n",
        "\r\n",
        "\r\n",
        "def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,\r\n",
        "              params=None, lr=None, optimizer=None):\r\n",
        "    for epoch in range(num_epochs):\r\n",
        "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\r\n",
        "        for X, y in train_iter:\r\n",
        "            y_hat = net(X)\r\n",
        "            l = loss(y_hat, y).sum()\r\n",
        "            \r\n",
        "            # 梯度清零\r\n",
        "            if optimizer is not None:\r\n",
        "                optimizer.zero_grad()\r\n",
        "            elif params is not None and params[0].grad is not None:\r\n",
        "                for param in params:\r\n",
        "                    param.grad.data.zero_()\r\n",
        "            \r\n",
        "            l.backward()\r\n",
        "            if optimizer is None:\r\n",
        "                sgd(params, lr, batch_size)\r\n",
        "            else:\r\n",
        "                optimizer.step()  # “softmax回归的简洁实现”一节将用到\r\n",
        "            \r\n",
        "            \r\n",
        "            train_l_sum += l.item()\r\n",
        "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\r\n",
        "            n += y.shape[0]\r\n",
        "        test_acc = evaluate_accuracy(test_iter, net)\r\n",
        "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\r\n",
        "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ########################### 3.7 #####################################3\r\n",
        "class FlattenLayer(torch.nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(FlattenLayer, self).__init__()\r\n",
        "    def forward(self, x): # x shape: (batch, *, *, ...)\r\n",
        "        return x.view(x.shape[0], -1)\r\n",
        "\r\n",
        "\r\n",
        "# ########################### 3.11 ###############################\r\n",
        "def semilogy(x_vals, y_vals, x_label, y_label, x2_vals=None, y2_vals=None,\r\n",
        "             legend=None, figsize=(3.5, 2.5)):\r\n",
        "    set_figsize(figsize)\r\n",
        "    plt.xlabel(x_label)\r\n",
        "    plt.ylabel(y_label)\r\n",
        "    plt.semilogy(x_vals, y_vals)\r\n",
        "    if x2_vals and y2_vals:\r\n",
        "        plt.semilogy(x2_vals, y2_vals, linestyle=':')\r\n",
        "        plt.legend(legend)\r\n",
        "    # plt.show()\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ############################# 3.13 ##############################\r\n",
        "# 5.5 修改\r\n",
        "# def evaluate_accuracy(data_iter, net):\r\n",
        "#     acc_sum, n = 0.0, 0\r\n",
        "#     for X, y in data_iter:\r\n",
        "#         if isinstance(net, torch.nn.Module):\r\n",
        "#             net.eval() # 评估模式, 这会关闭dropout\r\n",
        "#             acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()\r\n",
        "#             net.train() # 改回训练模式\r\n",
        "#         else: # 自定义的模型\r\n",
        "#             if('is_training' in net.__code__.co_varnames): # 如果有is_training这个参数\r\n",
        "#                 # 将is_training设置成False\r\n",
        "#                 acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() \r\n",
        "#             else:\r\n",
        "#                 acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() \r\n",
        "#         n += y.shape[0]\r\n",
        "#     return acc_sum / n\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ########################### 5.1 #########################\r\n",
        "def corr2d(X, K):  \r\n",
        "    h, w = K.shape\r\n",
        "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\r\n",
        "    for i in range(Y.shape[0]):\r\n",
        "        for j in range(Y.shape[1]):\r\n",
        "            Y[i, j] = (X[i: i + h, j: j + w] * K).sum()\r\n",
        "    return Y\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ############################ 5.5 #########################\r\n",
        "def evaluate_accuracy(data_iter, net, device=None):\r\n",
        "    if device is None and isinstance(net, torch.nn.Module):\r\n",
        "        # 如果没指定device就使用net的device\r\n",
        "        device = list(net.parameters())[0].device \r\n",
        "    acc_sum, n = 0.0, 0\r\n",
        "    with torch.no_grad():\r\n",
        "        for X, y in data_iter:\r\n",
        "            if isinstance(net, torch.nn.Module):\r\n",
        "                net.eval() # 评估模式, 这会关闭dropout\r\n",
        "                acc_sum += (net(X.to(device)).argmax(dim=1) == y.to(device)).float().sum().cpu().item()\r\n",
        "                net.train() # 改回训练模式\r\n",
        "            else: # 自定义的模型, 3.13节之后不会用到, 不考虑GPU\r\n",
        "                if('is_training' in net.__code__.co_varnames): # 如果有is_training这个参数\r\n",
        "                    # 将is_training设置成False\r\n",
        "                    acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() \r\n",
        "                else:\r\n",
        "                    acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() \r\n",
        "            n += y.shape[0]\r\n",
        "    return acc_sum / n\r\n",
        "\r\n",
        "def train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs):\r\n",
        "    net = net.to(device)\r\n",
        "    print(\"training on \", device)\r\n",
        "    loss = torch.nn.CrossEntropyLoss()\r\n",
        "    for epoch in range(num_epochs):\r\n",
        "        train_l_sum, train_acc_sum, n, batch_count, start = 0.0, 0.0, 0, 0, time.time()\r\n",
        "        for X, y in train_iter:\r\n",
        "            X = X.to(device)\r\n",
        "            y = y.to(device)\r\n",
        "            y_hat = net(X)\r\n",
        "            l = loss(y_hat, y)\r\n",
        "            optimizer.zero_grad()\r\n",
        "            l.backward()\r\n",
        "            optimizer.step()\r\n",
        "            train_l_sum += l.cpu().item()\r\n",
        "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\r\n",
        "            n += y.shape[0]\r\n",
        "            batch_count += 1\r\n",
        "        test_acc = evaluate_accuracy(test_iter, net)\r\n",
        "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\r\n",
        "              % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ########################## 5.6 #########################3\r\n",
        "def load_data_fashion_mnist(batch_size, resize=None, root='~/Datasets/FashionMNIST'):\r\n",
        "    \"\"\"Download the fashion mnist dataset and then load into memory.\"\"\"\r\n",
        "    trans = []\r\n",
        "    if resize:\r\n",
        "        trans.append(torchvision.transforms.Resize(size=resize))\r\n",
        "    trans.append(torchvision.transforms.ToTensor())\r\n",
        "    \r\n",
        "    transform = torchvision.transforms.Compose(trans)\r\n",
        "    mnist_train = torchvision.datasets.FashionMNIST(root=root, train=True, download=True, transform=transform)\r\n",
        "    mnist_test = torchvision.datasets.FashionMNIST(root=root, train=False, download=True, transform=transform)\r\n",
        "    if sys.platform.startswith('win'):\r\n",
        "        num_workers = 0  # 0表示不用额外的进程来加速读取数据\r\n",
        "    else:\r\n",
        "        num_workers = 4\r\n",
        "    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\r\n",
        "    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\r\n",
        "\r\n",
        "    return train_iter, test_iter\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "############################# 5.8 ##############################\r\n",
        "class GlobalAvgPool2d(nn.Module):\r\n",
        "    # 全局平均池化层可通过将池化窗口形状设置成输入的高和宽实现\r\n",
        "    def __init__(self):\r\n",
        "        super(GlobalAvgPool2d, self).__init__()\r\n",
        "    def forward(self, x):\r\n",
        "        return F.avg_pool2d(x, kernel_size=x.size()[2:])\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ########################### 5.11 ################################\r\n",
        "class Residual(nn.Module): \r\n",
        "    def __init__(self, in_channels, out_channels, use_1x1conv=False, stride=1):\r\n",
        "        super(Residual, self).__init__()\r\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride)\r\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\r\n",
        "        if use_1x1conv:\r\n",
        "            self.conv3 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride)\r\n",
        "        else:\r\n",
        "            self.conv3 = None\r\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\r\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\r\n",
        "\r\n",
        "    def forward(self, X):\r\n",
        "        Y = F.relu(self.bn1(self.conv1(X)))\r\n",
        "        Y = self.bn2(self.conv2(Y))\r\n",
        "        if self.conv3:\r\n",
        "            X = self.conv3(X)\r\n",
        "        return F.relu(Y + X)\r\n",
        "\r\n",
        "def resnet_block(in_channels, out_channels, num_residuals, first_block=False):\r\n",
        "    if first_block:\r\n",
        "        assert in_channels == out_channels # 第一个模块的通道数同输入通道数一致\r\n",
        "    blk = []\r\n",
        "    for i in range(num_residuals):\r\n",
        "        if i == 0 and not first_block:\r\n",
        "            blk.append(Residual(in_channels, out_channels, use_1x1conv=True, stride=2))\r\n",
        "        else:\r\n",
        "            blk.append(Residual(out_channels, out_channels))\r\n",
        "    return nn.Sequential(*blk)\r\n",
        "    \r\n",
        "def resnet18(output=10, in_channels=3):\r\n",
        "    net = nn.Sequential(\r\n",
        "        nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3),\r\n",
        "        nn.BatchNorm2d(64), \r\n",
        "        nn.ReLU(),\r\n",
        "        nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\r\n",
        "    net.add_module(\"resnet_block1\", resnet_block(64, 64, 2, first_block=True))\r\n",
        "    net.add_module(\"resnet_block2\", resnet_block(64, 128, 2))\r\n",
        "    net.add_module(\"resnet_block3\", resnet_block(128, 256, 2))\r\n",
        "    net.add_module(\"resnet_block4\", resnet_block(256, 512, 2))\r\n",
        "    net.add_module(\"global_avg_pool\", GlobalAvgPool2d()) # GlobalAvgPool2d的输出: (Batch, 512, 1, 1)\r\n",
        "    net.add_module(\"fc\", nn.Sequential(FlattenLayer(), nn.Linear(512, output))) \r\n",
        "    return net\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ############################## 6.3 ##################################3\r\n",
        "def load_data_jay_lyrics():\r\n",
        "    \"\"\"加载周杰伦歌词数据集\"\"\"\r\n",
        "    with zipfile.ZipFile('../../data/jaychou_lyrics.txt.zip') as zin:\r\n",
        "        with zin.open('jaychou_lyrics.txt') as f:\r\n",
        "            corpus_chars = f.read().decode('utf-8')\r\n",
        "    corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\r\n",
        "    corpus_chars = corpus_chars[0:10000]\r\n",
        "    idx_to_char = list(set(corpus_chars))\r\n",
        "    char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\r\n",
        "    vocab_size = len(char_to_idx)\r\n",
        "    corpus_indices = [char_to_idx[char] for char in corpus_chars]\r\n",
        "    return corpus_indices, char_to_idx, idx_to_char, vocab_size\r\n",
        "\r\n",
        "def data_iter_random(corpus_indices, batch_size, num_steps, device=None):\r\n",
        "    # 减1是因为输出的索引x是相应输入的索引y加1\r\n",
        "    num_examples = (len(corpus_indices) - 1) // num_steps\r\n",
        "    epoch_size = num_examples // batch_size\r\n",
        "    example_indices = list(range(num_examples))\r\n",
        "    random.shuffle(example_indices)\r\n",
        "\r\n",
        "    # 返回从pos开始的长为num_steps的序列\r\n",
        "    def _data(pos):\r\n",
        "        return corpus_indices[pos: pos + num_steps]\r\n",
        "    if device is None:\r\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "    \r\n",
        "    for i in range(epoch_size):\r\n",
        "        # 每次读取batch_size个随机样本\r\n",
        "        i = i * batch_size\r\n",
        "        batch_indices = example_indices[i: i + batch_size]\r\n",
        "        X = [_data(j * num_steps) for j in batch_indices]\r\n",
        "        Y = [_data(j * num_steps + 1) for j in batch_indices]\r\n",
        "        yield torch.tensor(X, dtype=torch.float32, device=device), torch.tensor(Y, dtype=torch.float32, device=device)\r\n",
        "\r\n",
        "def data_iter_consecutive(corpus_indices, batch_size, num_steps, device=None):\r\n",
        "    if device is None:\r\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "    corpus_indices = torch.tensor(corpus_indices, dtype=torch.float32, device=device)\r\n",
        "    data_len = len(corpus_indices)\r\n",
        "    batch_len = data_len // batch_size\r\n",
        "    indices = corpus_indices[0: batch_size*batch_len].view(batch_size, batch_len)\r\n",
        "    epoch_size = (batch_len - 1) // num_steps\r\n",
        "    for i in range(epoch_size):\r\n",
        "        i = i * num_steps\r\n",
        "        X = indices[:, i: i + num_steps]\r\n",
        "        Y = indices[:, i + 1: i + num_steps + 1]\r\n",
        "        yield X, Y\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ###################################### 6.4 ######################################\r\n",
        "def one_hot(x, n_class, dtype=torch.float32): \r\n",
        "    # X shape: (batch), output shape: (batch, n_class)\r\n",
        "    x = x.long()\r\n",
        "    res = torch.zeros(x.shape[0], n_class, dtype=dtype, device=x.device)\r\n",
        "    res.scatter_(1, x.view(-1, 1), 1)\r\n",
        "    return res\r\n",
        "\r\n",
        "def to_onehot(X, n_class):  \r\n",
        "    # X shape: (batch, seq_len), output: seq_len elements of (batch, n_class)\r\n",
        "    return [one_hot(X[:, i], n_class) for i in range(X.shape[1])]\r\n",
        "\r\n",
        "def predict_rnn(prefix, num_chars, rnn, params, init_rnn_state,\r\n",
        "                num_hiddens, vocab_size, device, idx_to_char, char_to_idx):\r\n",
        "    state = init_rnn_state(1, num_hiddens, device)\r\n",
        "    output = [char_to_idx[prefix[0]]]\r\n",
        "    for t in range(num_chars + len(prefix) - 1):\r\n",
        "        # 将上一时间步的输出作为当前时间步的输入\r\n",
        "        X = to_onehot(torch.tensor([[output[-1]]], device=device), vocab_size)\r\n",
        "        # 计算输出和更新隐藏状态\r\n",
        "        (Y, state) = rnn(X, state, params)\r\n",
        "        # 下一个时间步的输入是prefix里的字符或者当前的最佳预测字符\r\n",
        "        if t < len(prefix) - 1:\r\n",
        "            output.append(char_to_idx[prefix[t + 1]])\r\n",
        "        else:\r\n",
        "            output.append(int(Y[0].argmax(dim=1).item()))\r\n",
        "    return ''.join([idx_to_char[i] for i in output])\r\n",
        "\r\n",
        "def grad_clipping(params, theta, device):\r\n",
        "    norm = torch.tensor([0.0], device=device)\r\n",
        "    for param in params:\r\n",
        "        norm += (param.grad.data ** 2).sum()\r\n",
        "    norm = norm.sqrt().item()\r\n",
        "    if norm > theta:\r\n",
        "        for param in params:\r\n",
        "            param.grad.data *= (theta / norm)\r\n",
        "\r\n",
        "def train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\r\n",
        "                          vocab_size, device, corpus_indices, idx_to_char,\r\n",
        "                          char_to_idx, is_random_iter, num_epochs, num_steps,\r\n",
        "                          lr, clipping_theta, batch_size, pred_period,\r\n",
        "                          pred_len, prefixes):\r\n",
        "    if is_random_iter:\r\n",
        "        data_iter_fn = data_iter_random\r\n",
        "    else:\r\n",
        "        data_iter_fn = data_iter_consecutive\r\n",
        "    params = get_params()\r\n",
        "    loss = nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "    for epoch in range(num_epochs):\r\n",
        "        if not is_random_iter:  # 如使用相邻采样，在epoch开始时初始化隐藏状态\r\n",
        "            state = init_rnn_state(batch_size, num_hiddens, device)\r\n",
        "        l_sum, n, start = 0.0, 0, time.time()\r\n",
        "        data_iter = data_iter_fn(corpus_indices, batch_size, num_steps, device)\r\n",
        "        for X, Y in data_iter:\r\n",
        "            if is_random_iter:  # 如使用随机采样，在每个小批量更新前初始化隐藏状态\r\n",
        "                state = init_rnn_state(batch_size, num_hiddens, device)\r\n",
        "            else: \r\n",
        "            # 否则需要使用detach函数从计算图分离隐藏状态, 这是为了\r\n",
        "            # 使模型参数的梯度计算只依赖一次迭代读取的小批量序列(防止梯度计算开销太大)\r\n",
        "                for s in state:\r\n",
        "                    s.detach_()\r\n",
        "            \r\n",
        "            inputs = to_onehot(X, vocab_size)\r\n",
        "            # outputs有num_steps个形状为(batch_size, vocab_size)的矩阵\r\n",
        "            (outputs, state) = rnn(inputs, state, params)\r\n",
        "            # 拼接之后形状为(num_steps * batch_size, vocab_size)\r\n",
        "            outputs = torch.cat(outputs, dim=0)\r\n",
        "            # Y的形状是(batch_size, num_steps)，转置后再变成长度为\r\n",
        "            # batch * num_steps 的向量，这样跟输出的行一一对应\r\n",
        "            y = torch.transpose(Y, 0, 1).contiguous().view(-1)\r\n",
        "            # 使用交叉熵损失计算平均分类误差\r\n",
        "            l = loss(outputs, y.long())\r\n",
        "            \r\n",
        "            # 梯度清0\r\n",
        "            if params[0].grad is not None:\r\n",
        "                for param in params:\r\n",
        "                    param.grad.data.zero_()\r\n",
        "            l.backward()\r\n",
        "            grad_clipping(params, clipping_theta, device)  # 裁剪梯度\r\n",
        "            sgd(params, lr, 1)  # 因为误差已经取过均值，梯度不用再做平均\r\n",
        "            l_sum += l.item() * y.shape[0]\r\n",
        "            n += y.shape[0]\r\n",
        "\r\n",
        "        if (epoch + 1) % pred_period == 0:\r\n",
        "            print('epoch %d, perplexity %f, time %.2f sec' % (\r\n",
        "                epoch + 1, math.exp(l_sum / n), time.time() - start))\r\n",
        "            for prefix in prefixes:\r\n",
        "                print(' -', predict_rnn(prefix, pred_len, rnn, params, init_rnn_state,\r\n",
        "                    num_hiddens, vocab_size, device, idx_to_char, char_to_idx))\r\n",
        "\r\n",
        "                \r\n",
        "                \r\n",
        "                \r\n",
        "# ################################### 6.5 ################################################\r\n",
        "class RNNModel(nn.Module):\r\n",
        "    def __init__(self, rnn_layer, vocab_size):\r\n",
        "        super(RNNModel, self).__init__()\r\n",
        "        self.rnn = rnn_layer\r\n",
        "        self.hidden_size = rnn_layer.hidden_size * (2 if rnn_layer.bidirectional else 1) \r\n",
        "        self.vocab_size = vocab_size\r\n",
        "        self.dense = nn.Linear(self.hidden_size, vocab_size)\r\n",
        "        self.state = None\r\n",
        "\r\n",
        "    def forward(self, inputs, state): # inputs: (batch, seq_len)\r\n",
        "        # 获取one-hot向量表示\r\n",
        "        X = to_onehot(inputs, self.vocab_size) # X是个list\r\n",
        "        Y, self.state = self.rnn(torch.stack(X), state)\r\n",
        "        # 全连接层会首先将Y的形状变成(num_steps * batch_size, num_hiddens)，它的输出\r\n",
        "        # 形状为(num_steps * batch_size, vocab_size)\r\n",
        "        output = self.dense(Y.view(-1, Y.shape[-1]))\r\n",
        "        return output, self.state\r\n",
        "\r\n",
        "def predict_rnn_pytorch(prefix, num_chars, model, vocab_size, device, idx_to_char,\r\n",
        "                      char_to_idx):\r\n",
        "    state = None\r\n",
        "    output = [char_to_idx[prefix[0]]] # output会记录prefix加上输出\r\n",
        "    for t in range(num_chars + len(prefix) - 1):\r\n",
        "        X = torch.tensor([output[-1]], device=device).view(1, 1)\r\n",
        "        if state is not None:\r\n",
        "            if isinstance(state, tuple): # LSTM, state:(h, c)  \r\n",
        "                state = (state[0].to(device), state[1].to(device))\r\n",
        "            else:   \r\n",
        "                state = state.to(device)\r\n",
        "            \r\n",
        "        (Y, state) = model(X, state)  # 前向计算不需要传入模型参数\r\n",
        "        if t < len(prefix) - 1:\r\n",
        "            output.append(char_to_idx[prefix[t + 1]])\r\n",
        "        else:\r\n",
        "            output.append(int(Y.argmax(dim=1).item()))\r\n",
        "    return ''.join([idx_to_char[i] for i in output])\r\n",
        "\r\n",
        "def train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,\r\n",
        "                                corpus_indices, idx_to_char, char_to_idx,\r\n",
        "                                num_epochs, num_steps, lr, clipping_theta,\r\n",
        "                                batch_size, pred_period, pred_len, prefixes):\r\n",
        "    loss = nn.CrossEntropyLoss()\r\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\r\n",
        "    model.to(device)\r\n",
        "    state = None\r\n",
        "    for epoch in range(num_epochs):\r\n",
        "        l_sum, n, start = 0.0, 0, time.time()\r\n",
        "        data_iter = data_iter_consecutive(corpus_indices, batch_size, num_steps, device) # 相邻采样\r\n",
        "        for X, Y in data_iter:\r\n",
        "            if state is not None:\r\n",
        "                # 使用detach函数从计算图分离隐藏状态, 这是为了\r\n",
        "                # 使模型参数的梯度计算只依赖一次迭代读取的小批量序列(防止梯度计算开销太大)\r\n",
        "                if isinstance (state, tuple): # LSTM, state:(h, c)  \r\n",
        "                    state = (state[0].detach(), state[1].detach())\r\n",
        "                else:   \r\n",
        "                    state = state.detach()\r\n",
        "    \r\n",
        "            (output, state) = model(X, state) # output: 形状为(num_steps * batch_size, vocab_size)\r\n",
        "            \r\n",
        "            # Y的形状是(batch_size, num_steps)，转置后再变成长度为\r\n",
        "            # batch * num_steps 的向量，这样跟输出的行一一对应\r\n",
        "            y = torch.transpose(Y, 0, 1).contiguous().view(-1)\r\n",
        "            l = loss(output, y.long())\r\n",
        "            \r\n",
        "            optimizer.zero_grad()\r\n",
        "            l.backward()\r\n",
        "            # 梯度裁剪\r\n",
        "            grad_clipping(model.parameters(), clipping_theta, device)\r\n",
        "            optimizer.step()\r\n",
        "            l_sum += l.item() * y.shape[0]\r\n",
        "            n += y.shape[0]\r\n",
        "        \r\n",
        "        try:\r\n",
        "            perplexity = math.exp(l_sum / n)\r\n",
        "        except OverflowError:\r\n",
        "            perplexity = float('inf')\r\n",
        "        if (epoch + 1) % pred_period == 0:\r\n",
        "            print('epoch %d, perplexity %f, time %.2f sec' % (\r\n",
        "                epoch + 1, perplexity, time.time() - start))\r\n",
        "            for prefix in prefixes:\r\n",
        "                print(' -', predict_rnn_pytorch(\r\n",
        "                    prefix, pred_len, model, vocab_size, device, idx_to_char,\r\n",
        "                    char_to_idx))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ######################################## 7.2 ###############################################\r\n",
        "def train_2d(trainer):  \r\n",
        "    x1, x2, s1, s2 = -5, -2, 0, 0  # s1和s2是自变量状态，本章后续几节会使用\r\n",
        "    results = [(x1, x2)]\r\n",
        "    for i in range(20):\r\n",
        "        x1, x2, s1, s2 = trainer(x1, x2, s1, s2)\r\n",
        "        results.append((x1, x2))\r\n",
        "    print('epoch %d, x1 %f, x2 %f' % (i + 1, x1, x2))\r\n",
        "    return results\r\n",
        "\r\n",
        "def show_trace_2d(f, results):  \r\n",
        "    plt.plot(*zip(*results), '-o', color='#ff7f0e')\r\n",
        "    x1, x2 = np.meshgrid(np.arange(-5.5, 1.0, 0.1), np.arange(-3.0, 1.0, 0.1))\r\n",
        "    plt.contour(x1, x2, f(x1, x2), colors='#1f77b4')\r\n",
        "    plt.xlabel('x1')\r\n",
        "    plt.ylabel('x2')\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ######################################## 7.3 ###############################################\r\n",
        "def get_data_ch7():  \r\n",
        "    data = np.genfromtxt('../../data/airfoil_self_noise.dat', delimiter='\\t')\r\n",
        "    data = (data - data.mean(axis=0)) / data.std(axis=0)\r\n",
        "    return torch.tensor(data[:1500, :-1], dtype=torch.float32), \\\r\n",
        "        torch.tensor(data[:1500, -1], dtype=torch.float32) # 前1500个样本(每个样本5个特征)\r\n",
        "\r\n",
        "def train_ch7(optimizer_fn, states, hyperparams, features, labels,\r\n",
        "              batch_size=10, num_epochs=2):\r\n",
        "    # 初始化模型\r\n",
        "    net, loss = linreg, squared_loss\r\n",
        "    \r\n",
        "    w = torch.nn.Parameter(torch.tensor(np.random.normal(0, 0.01, size=(features.shape[1], 1)), dtype=torch.float32),\r\n",
        "                           requires_grad=True)\r\n",
        "    b = torch.nn.Parameter(torch.zeros(1, dtype=torch.float32), requires_grad=True)\r\n",
        "\r\n",
        "    def eval_loss():\r\n",
        "        return loss(net(features, w, b), labels).mean().item()\r\n",
        "\r\n",
        "    ls = [eval_loss()]\r\n",
        "    data_iter = torch.utils.data.DataLoader(\r\n",
        "        torch.utils.data.TensorDataset(features, labels), batch_size, shuffle=True)\r\n",
        "    \r\n",
        "    for _ in range(num_epochs):\r\n",
        "        start = time.time()\r\n",
        "        for batch_i, (X, y) in enumerate(data_iter):\r\n",
        "            l = loss(net(X, w, b), y).mean()  # 使用平均损失\r\n",
        "            \r\n",
        "            # 梯度清零\r\n",
        "            if w.grad is not None:\r\n",
        "                w.grad.data.zero_()\r\n",
        "                b.grad.data.zero_()\r\n",
        "                \r\n",
        "            l.backward()\r\n",
        "            optimizer_fn([w, b], states, hyperparams)  # 迭代模型参数\r\n",
        "            if (batch_i + 1) * batch_size % 100 == 0:\r\n",
        "                ls.append(eval_loss())  # 每100个样本记录下当前训练误差\r\n",
        "    # 打印结果和作图\r\n",
        "    print('loss: %f, %f sec per epoch' % (ls[-1], time.time() - start))\r\n",
        "    set_figsize()\r\n",
        "    plt.plot(np.linspace(0, num_epochs, len(ls)), ls)\r\n",
        "    plt.xlabel('epoch')\r\n",
        "    plt.ylabel('loss')\r\n",
        "\r\n",
        "# 本函数与原书不同的是这里第一个参数优化器函数而不是优化器的名字\r\n",
        "# 例如: optimizer_fn=torch.optim.SGD, optimizer_hyperparams={\"lr\": 0.05}\r\n",
        "def train_pytorch_ch7(optimizer_fn, optimizer_hyperparams, features, labels,\r\n",
        "                    batch_size=10, num_epochs=2):\r\n",
        "    # 初始化模型\r\n",
        "    net = nn.Sequential(\r\n",
        "        nn.Linear(features.shape[-1], 1)\r\n",
        "    )\r\n",
        "    loss = nn.MSELoss()\r\n",
        "    optimizer = optimizer_fn(net.parameters(), **optimizer_hyperparams)\r\n",
        "\r\n",
        "    def eval_loss():\r\n",
        "        return loss(net(features).view(-1), labels).item() / 2\r\n",
        "\r\n",
        "    ls = [eval_loss()]\r\n",
        "    data_iter = torch.utils.data.DataLoader(\r\n",
        "        torch.utils.data.TensorDataset(features, labels), batch_size, shuffle=True)\r\n",
        "\r\n",
        "    for _ in range(num_epochs):\r\n",
        "        start = time.time()\r\n",
        "        for batch_i, (X, y) in enumerate(data_iter):\r\n",
        "            # 除以2是为了和train_ch7保持一致, 因为squared_loss中除了2\r\n",
        "            l = loss(net(X).view(-1), y) / 2 \r\n",
        "            \r\n",
        "            optimizer.zero_grad()\r\n",
        "            l.backward()\r\n",
        "            optimizer.step()\r\n",
        "            if (batch_i + 1) * batch_size % 100 == 0:\r\n",
        "                ls.append(eval_loss())\r\n",
        "    # 打印结果和作图\r\n",
        "    print('loss: %f, %f sec per epoch' % (ls[-1], time.time() - start))\r\n",
        "    set_figsize()\r\n",
        "    plt.plot(np.linspace(0, num_epochs, len(ls)), ls)\r\n",
        "    plt.xlabel('epoch')\r\n",
        "    plt.ylabel('loss')\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "############################## 8.3 ##################################\r\n",
        "class Benchmark():\r\n",
        "    def __init__(self, prefix=None):\r\n",
        "        self.prefix = prefix + ' ' if prefix else ''\r\n",
        "\r\n",
        "    def __enter__(self):\r\n",
        "        self.start = time.time()\r\n",
        "\r\n",
        "    def __exit__(self, *args):\r\n",
        "        print('%stime: %.4f sec' % (self.prefix, time.time() - self.start))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ########################### 9.1 ########################################\r\n",
        "def show_images(imgs, num_rows, num_cols, scale=2):\r\n",
        "    figsize = (num_cols * scale, num_rows * scale)\r\n",
        "    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)\r\n",
        "    for i in range(num_rows):\r\n",
        "        for j in range(num_cols):\r\n",
        "            axes[i][j].imshow(imgs[i * num_cols + j])\r\n",
        "            axes[i][j].axes.get_xaxis().set_visible(False)\r\n",
        "            axes[i][j].axes.get_yaxis().set_visible(False)\r\n",
        "    return axes\r\n",
        "\r\n",
        "def train(train_iter, test_iter, net, loss, optimizer, device, num_epochs):\r\n",
        "    net = net.to(device)\r\n",
        "    print(\"training on \", device)\r\n",
        "    batch_count = 0\r\n",
        "    for epoch in range(num_epochs):\r\n",
        "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\r\n",
        "        for X, y in train_iter:\r\n",
        "            X = X.to(device)\r\n",
        "            y = y.to(device)\r\n",
        "            y_hat = net(X)\r\n",
        "            l = loss(y_hat, y) \r\n",
        "            optimizer.zero_grad()\r\n",
        "            l.backward()\r\n",
        "            optimizer.step()\r\n",
        "            train_l_sum += l.cpu().item()\r\n",
        "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\r\n",
        "            n += y.shape[0]\r\n",
        "            batch_count += 1\r\n",
        "        test_acc = evaluate_accuracy(test_iter, net)\r\n",
        "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\r\n",
        "              % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "############################## 9.3 #####################\r\n",
        "def bbox_to_rect(bbox, color):\r\n",
        "    # 将边界框(左上x, 左上y, 右下x, 右下y)格式转换成matplotlib格式：\r\n",
        "    # ((左上x, 左上y), 宽, 高)\r\n",
        "    return plt.Rectangle(\r\n",
        "        xy=(bbox[0], bbox[1]), width=bbox[2]-bbox[0], height=bbox[3]-bbox[1],\r\n",
        "        fill=False, edgecolor=color, linewidth=2)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "############################ 9.4 ###########################\r\n",
        "def MultiBoxPrior(feature_map, sizes=[0.75, 0.5, 0.25], ratios=[1, 2, 0.5]):\r\n",
        "    \"\"\"\r\n",
        "    # 按照「9.4.1. 生成多个锚框」所讲的实现, anchor表示成(xmin, ymin, xmax, ymax).\r\n",
        "    https://zh.d2l.ai/chapter_computer-vision/anchor.html\r\n",
        "    Args:\r\n",
        "        feature_map: torch tensor, Shape: [N, C, H, W].\r\n",
        "        sizes: List of sizes (0~1) of generated MultiBoxPriores. \r\n",
        "        ratios: List of aspect ratios (non-negative) of generated MultiBoxPriores. \r\n",
        "    Returns:\r\n",
        "        anchors of shape (1, num_anchors, 4). 由于batch里每个都一样, 所以第一维为1\r\n",
        "    \"\"\"\r\n",
        "    pairs = [] # pair of (size, sqrt(ration))\r\n",
        "    for r in ratios:\r\n",
        "        pairs.append([sizes[0], math.sqrt(r)])\r\n",
        "    for s in sizes[1:]:\r\n",
        "        pairs.append([s, math.sqrt(ratios[0])])\r\n",
        "    \r\n",
        "    pairs = np.array(pairs)\r\n",
        "    \r\n",
        "    ss1 = pairs[:, 0] * pairs[:, 1] # size * sqrt(ration)\r\n",
        "    ss2 = pairs[:, 0] / pairs[:, 1] # size / sqrt(ration)\r\n",
        "    \r\n",
        "    base_anchors = np.stack([-ss1, -ss2, ss1, ss2], axis=1) / 2\r\n",
        "    \r\n",
        "    h, w = feature_map.shape[-2:]\r\n",
        "    shifts_x = np.arange(0, w) / w\r\n",
        "    shifts_y = np.arange(0, h) / h\r\n",
        "    shift_x, shift_y = np.meshgrid(shifts_x, shifts_y)\r\n",
        "    shift_x = shift_x.reshape(-1)\r\n",
        "    shift_y = shift_y.reshape(-1)\r\n",
        "    shifts = np.stack((shift_x, shift_y, shift_x, shift_y), axis=1)\r\n",
        "    \r\n",
        "    anchors = shifts.reshape((-1, 1, 4)) + base_anchors.reshape((1, -1, 4))\r\n",
        "    \r\n",
        "    return torch.tensor(anchors, dtype=torch.float32).view(1, -1, 4)\r\n",
        "\r\n",
        "def show_bboxes(axes, bboxes, labels=None, colors=None):\r\n",
        "    def _make_list(obj, default_values=None):\r\n",
        "        if obj is None:\r\n",
        "            obj = default_values\r\n",
        "        elif not isinstance(obj, (list, tuple)):\r\n",
        "            obj = [obj]\r\n",
        "        return obj\r\n",
        "\r\n",
        "    labels = _make_list(labels)\r\n",
        "    colors = _make_list(colors, ['b', 'g', 'r', 'm', 'c'])\r\n",
        "    for i, bbox in enumerate(bboxes):\r\n",
        "        color = colors[i % len(colors)]\r\n",
        "        rect = bbox_to_rect(bbox.detach().cpu().numpy(), color)\r\n",
        "        axes.add_patch(rect)\r\n",
        "        if labels and len(labels) > i:\r\n",
        "            text_color = 'k' if color == 'w' else 'w'\r\n",
        "            axes.text(rect.xy[0], rect.xy[1], labels[i],\r\n",
        "                      va='center', ha='center', fontsize=6, color=text_color,\r\n",
        "                      bbox=dict(facecolor=color, lw=0))\r\n",
        "\r\n",
        "def compute_intersection(set_1, set_2):\r\n",
        "    \"\"\"\r\n",
        "    计算anchor之间的交集\r\n",
        "    Args:\r\n",
        "        set_1: a tensor of dimensions (n1, 4), anchor表示成(xmin, ymin, xmax, ymax)\r\n",
        "        set_2: a tensor of dimensions (n2, 4), anchor表示成(xmin, ymin, xmax, ymax)\r\n",
        "    Returns:\r\n",
        "        intersection of each of the boxes in set 1 with respect to each of the boxes in set 2, shape: (n1, n2)\r\n",
        "    \"\"\"\r\n",
        "    # PyTorch auto-broadcasts singleton dimensions\r\n",
        "    lower_bounds = torch.max(set_1[:, :2].unsqueeze(1), set_2[:, :2].unsqueeze(0))  # (n1, n2, 2)\r\n",
        "    upper_bounds = torch.min(set_1[:, 2:].unsqueeze(1), set_2[:, 2:].unsqueeze(0))  # (n1, n2, 2)\r\n",
        "    intersection_dims = torch.clamp(upper_bounds - lower_bounds, min=0)  # (n1, n2, 2)\r\n",
        "    return intersection_dims[:, :, 0] * intersection_dims[:, :, 1]  # (n1, n2)\r\n",
        "\r\n",
        "def compute_jaccard(set_1, set_2):\r\n",
        "    \"\"\"\r\n",
        "    计算anchor之间的Jaccard系数(IoU)\r\n",
        "    Args:\r\n",
        "        set_1: a tensor of dimensions (n1, 4), anchor表示成(xmin, ymin, xmax, ymax)\r\n",
        "        set_2: a tensor of dimensions (n2, 4), anchor表示成(xmin, ymin, xmax, ymax)\r\n",
        "    Returns:\r\n",
        "        Jaccard Overlap of each of the boxes in set 1 with respect to each of the boxes in set 2, shape: (n1, n2)\r\n",
        "    \"\"\"\r\n",
        "    # Find intersections\r\n",
        "    intersection = compute_intersection(set_1, set_2)  # (n1, n2)\r\n",
        "\r\n",
        "    # Find areas of each box in both sets\r\n",
        "    areas_set_1 = (set_1[:, 2] - set_1[:, 0]) * (set_1[:, 3] - set_1[:, 1])  # (n1)\r\n",
        "    areas_set_2 = (set_2[:, 2] - set_2[:, 0]) * (set_2[:, 3] - set_2[:, 1])  # (n2)\r\n",
        "\r\n",
        "    # Find the union\r\n",
        "    # PyTorch auto-broadcasts singleton dimensions\r\n",
        "    union = areas_set_1.unsqueeze(1) + areas_set_2.unsqueeze(0) - intersection  # (n1, n2)\r\n",
        "\r\n",
        "    return intersection / union  # (n1, n2)\r\n",
        "\r\n",
        "def assign_anchor(bb, anchor, jaccard_threshold=0.5):\r\n",
        "    \"\"\"\r\n",
        "    # 按照「9.4.1. 生成多个锚框」图9.3所讲为每个anchor分配真实的bb, anchor表示成归一化(xmin, ymin, xmax, ymax).\r\n",
        "    https://zh.d2l.ai/chapter_computer-vision/anchor.html\r\n",
        "    Args:\r\n",
        "        bb: 真实边界框(bounding box), shape:（nb, 4）\r\n",
        "        anchor: 待分配的anchor, shape:（na, 4）\r\n",
        "        jaccard_threshold: 预先设定的阈值\r\n",
        "    Returns:\r\n",
        "        assigned_idx: shape: (na, ), 每个anchor分配的真实bb对应的索引, 若未分配任何bb则为-1\r\n",
        "    \"\"\"\r\n",
        "    na = anchor.shape[0]\r\n",
        "    nb = bb.shape[0]\r\n",
        "    jaccard = compute_jaccard(anchor, bb).detach().cpu().numpy() # shape: (na, nb)\r\n",
        "    assigned_idx = np.ones(na) * -1  # 初始全为-1\r\n",
        "    \r\n",
        "    # 先为每个bb分配一个anchor(不要求满足jaccard_threshold)\r\n",
        "    jaccard_cp = jaccard.copy()\r\n",
        "    for j in range(nb):\r\n",
        "        i = np.argmax(jaccard_cp[:, j])\r\n",
        "        assigned_idx[i] = j\r\n",
        "        jaccard_cp[i, :] = float(\"-inf\") # 赋值为负无穷, 相当于去掉这一行\r\n",
        "     \r\n",
        "    # 处理还未被分配的anchor, 要求满足jaccard_threshold\r\n",
        "    for i in range(na):\r\n",
        "        if assigned_idx[i] == -1:\r\n",
        "            j = np.argmax(jaccard[i, :])\r\n",
        "            if jaccard[i, j] >= jaccard_threshold:\r\n",
        "                assigned_idx[i] = j\r\n",
        "    \r\n",
        "    return torch.tensor(assigned_idx, dtype=torch.long)\r\n",
        "\r\n",
        "def xy_to_cxcy(xy):\r\n",
        "    \"\"\"\r\n",
        "    将(x_min, y_min, x_max, y_max)形式的anchor转换成(center_x, center_y, w, h)形式的.\r\n",
        "    https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/utils.py\r\n",
        "    Args:\r\n",
        "        xy: bounding boxes in boundary coordinates, a tensor of size (n_boxes, 4)\r\n",
        "    Returns: \r\n",
        "        bounding boxes in center-size coordinates, a tensor of size (n_boxes, 4)\r\n",
        "    \"\"\"\r\n",
        "    return torch.cat([(xy[:, 2:] + xy[:, :2]) / 2,  # c_x, c_y\r\n",
        "                      xy[:, 2:] - xy[:, :2]], 1)  # w, h\r\n",
        "\r\n",
        "def MultiBoxTarget(anchor, label):\r\n",
        "    \"\"\"\r\n",
        "    # 按照「9.4.1. 生成多个锚框」所讲的实现, anchor表示成归一化(xmin, ymin, xmax, ymax).\r\n",
        "    https://zh.d2l.ai/chapter_computer-vision/anchor.html\r\n",
        "    Args:\r\n",
        "        anchor: torch tensor, 输入的锚框, 一般是通过MultiBoxPrior生成, shape:（1，锚框总数，4）\r\n",
        "        label: 真实标签, shape为(bn, 每张图片最多的真实锚框数, 5)\r\n",
        "               第二维中，如果给定图片没有这么多锚框, 可以先用-1填充空白, 最后一维中的元素为[类别标签, 四个坐标值]\r\n",
        "    Returns:\r\n",
        "        列表, [bbox_offset, bbox_mask, cls_labels]\r\n",
        "        bbox_offset: 每个锚框的标注偏移量，形状为(bn，锚框总数*4)\r\n",
        "        bbox_mask: 形状同bbox_offset, 每个锚框的掩码, 一一对应上面的偏移量, 负类锚框(背景)对应的掩码均为0, 正类锚框的掩码均为1\r\n",
        "        cls_labels: 每个锚框的标注类别, 其中0表示为背景, 形状为(bn，锚框总数)\r\n",
        "    \"\"\"\r\n",
        "    assert len(anchor.shape) == 3 and len(label.shape) == 3\r\n",
        "    bn = label.shape[0]\r\n",
        "    \r\n",
        "    def MultiBoxTarget_one(anc, lab, eps=1e-6):\r\n",
        "        \"\"\"\r\n",
        "        MultiBoxTarget函数的辅助函数, 处理batch中的一个\r\n",
        "        Args:\r\n",
        "            anc: shape of (锚框总数, 4)\r\n",
        "            lab: shape of (真实锚框数, 5), 5代表[类别标签, 四个坐标值]\r\n",
        "            eps: 一个极小值, 防止log0\r\n",
        "        Returns:\r\n",
        "            offset: (锚框总数*4, )\r\n",
        "            bbox_mask: (锚框总数*4, ), 0代表背景, 1代表非背景\r\n",
        "            cls_labels: (锚框总数, 4), 0代表背景\r\n",
        "        \"\"\"\r\n",
        "        an = anc.shape[0]\r\n",
        "        assigned_idx = assign_anchor(lab[:, 1:], anc) # (锚框总数, )\r\n",
        "        bbox_mask = ((assigned_idx >= 0).float().unsqueeze(-1)).repeat(1, 4) # (锚框总数, 4)\r\n",
        "\r\n",
        "        cls_labels = torch.zeros(an, dtype=torch.long) # 0表示背景\r\n",
        "        assigned_bb = torch.zeros((an, 4), dtype=torch.float32) # 所有anchor对应的bb坐标\r\n",
        "        for i in range(an):\r\n",
        "            bb_idx = assigned_idx[i]\r\n",
        "            if bb_idx >= 0: # 即非背景\r\n",
        "                cls_labels[i] = lab[bb_idx, 0].long().item() + 1 # 注意要加一\r\n",
        "                assigned_bb[i, :] = lab[bb_idx, 1:]\r\n",
        "\r\n",
        "        center_anc = xy_to_cxcy(anc) # (center_x, center_y, w, h)\r\n",
        "        center_assigned_bb = xy_to_cxcy(assigned_bb)\r\n",
        "\r\n",
        "        offset_xy = 10.0 * (center_assigned_bb[:, :2] - center_anc[:, :2]) / center_anc[:, 2:]\r\n",
        "        offset_wh = 5.0 * torch.log(eps + center_assigned_bb[:, 2:] / center_anc[:, 2:])\r\n",
        "        offset = torch.cat([offset_xy, offset_wh], dim = 1) * bbox_mask # (锚框总数, 4)\r\n",
        "\r\n",
        "        return offset.view(-1), bbox_mask.view(-1), cls_labels\r\n",
        "    \r\n",
        "    batch_offset = []\r\n",
        "    batch_mask = []\r\n",
        "    batch_cls_labels = []\r\n",
        "    for b in range(bn):\r\n",
        "        offset, bbox_mask, cls_labels = MultiBoxTarget_one(anchor[0, :, :], label[b, :, :])\r\n",
        "        \r\n",
        "        batch_offset.append(offset)\r\n",
        "        batch_mask.append(bbox_mask)\r\n",
        "        batch_cls_labels.append(cls_labels)\r\n",
        "    \r\n",
        "    bbox_offset = torch.stack(batch_offset)\r\n",
        "    bbox_mask = torch.stack(batch_mask)\r\n",
        "    cls_labels = torch.stack(batch_cls_labels)\r\n",
        "    \r\n",
        "    return [bbox_offset, bbox_mask, cls_labels]\r\n",
        "\r\n",
        "\r\n",
        "Pred_BB_Info = namedtuple(\"Pred_BB_Info\", [\"index\", \"class_id\", \"confidence\", \"xyxy\"])\r\n",
        "def non_max_suppression(bb_info_list, nms_threshold = 0.5):\r\n",
        "    \"\"\"\r\n",
        "    非极大抑制处理预测的边界框\r\n",
        "    Args:\r\n",
        "        bb_info_list: Pred_BB_Info的列表, 包含预测类别、置信度等信息\r\n",
        "        nms_threshold: 阈值\r\n",
        "    Returns:\r\n",
        "        output: Pred_BB_Info的列表, 只保留过滤后的边界框信息\r\n",
        "    \"\"\"\r\n",
        "    output = []\r\n",
        "    # 先根据置信度从高到低排序\r\n",
        "    sorted_bb_info_list = sorted(bb_info_list, key = lambda x: x.confidence, reverse=True)\r\n",
        "\r\n",
        "    while len(sorted_bb_info_list) != 0:\r\n",
        "        best = sorted_bb_info_list.pop(0)\r\n",
        "        output.append(best)\r\n",
        "        \r\n",
        "        if len(sorted_bb_info_list) == 0:\r\n",
        "            break\r\n",
        "\r\n",
        "        bb_xyxy = []\r\n",
        "        for bb in sorted_bb_info_list:\r\n",
        "            bb_xyxy.append(bb.xyxy)\r\n",
        "        \r\n",
        "        iou = compute_jaccard(torch.tensor([best.xyxy]), \r\n",
        "                              torch.tensor(bb_xyxy))[0] # shape: (len(sorted_bb_info_list), )\r\n",
        "        \r\n",
        "        n = len(sorted_bb_info_list)\r\n",
        "        sorted_bb_info_list = [sorted_bb_info_list[i] for i in range(n) if iou[i] <= nms_threshold]\r\n",
        "    return output\r\n",
        "\r\n",
        "def MultiBoxDetection(cls_prob, loc_pred, anchor, nms_threshold = 0.5):\r\n",
        "    \"\"\"\r\n",
        "    # 按照「9.4.1. 生成多个锚框」所讲的实现, anchor表示成归一化(xmin, ymin, xmax, ymax).\r\n",
        "    https://zh.d2l.ai/chapter_computer-vision/anchor.html\r\n",
        "    Args:\r\n",
        "        cls_prob: 经过softmax后得到的各个锚框的预测概率, shape:(bn, 预测总类别数+1, 锚框个数)\r\n",
        "        loc_pred: 预测的各个锚框的偏移量, shape:(bn, 锚框个数*4)\r\n",
        "        anchor: MultiBoxPrior输出的默认锚框, shape: (1, 锚框个数, 4)\r\n",
        "        nms_threshold: 非极大抑制中的阈值\r\n",
        "    Returns:\r\n",
        "        所有锚框的信息, shape: (bn, 锚框个数, 6)\r\n",
        "        每个锚框信息由[class_id, confidence, xmin, ymin, xmax, ymax]表示\r\n",
        "        class_id=-1 表示背景或在非极大值抑制中被移除了\r\n",
        "    \"\"\"\r\n",
        "    assert len(cls_prob.shape) == 3 and len(loc_pred.shape) == 2 and len(anchor.shape) == 3\r\n",
        "    bn = cls_prob.shape[0]\r\n",
        "    \r\n",
        "    def MultiBoxDetection_one(c_p, l_p, anc, nms_threshold = 0.5):\r\n",
        "        \"\"\"\r\n",
        "        MultiBoxDetection的辅助函数, 处理batch中的一个\r\n",
        "        Args:\r\n",
        "            c_p: (预测总类别数+1, 锚框个数)\r\n",
        "            l_p: (锚框个数*4, )\r\n",
        "            anc: (锚框个数, 4)\r\n",
        "            nms_threshold: 非极大抑制中的阈值\r\n",
        "        Return:\r\n",
        "            output: (锚框个数, 6)\r\n",
        "        \"\"\"\r\n",
        "        pred_bb_num = c_p.shape[1]\r\n",
        "        anc = (anc + l_p.view(pred_bb_num, 4)).detach().cpu().numpy() # 加上偏移量\r\n",
        "        \r\n",
        "        confidence, class_id = torch.max(c_p, 0)\r\n",
        "        confidence = confidence.detach().cpu().numpy()\r\n",
        "        class_id = class_id.detach().cpu().numpy()\r\n",
        "        \r\n",
        "        pred_bb_info = [Pred_BB_Info(\r\n",
        "                            index = i,\r\n",
        "                            class_id = class_id[i] - 1, # 正类label从0开始\r\n",
        "                            confidence = confidence[i],\r\n",
        "                            xyxy=[*anc[i]]) # xyxy是个列表\r\n",
        "                        for i in range(pred_bb_num)]\r\n",
        "        \r\n",
        "        # 正类的index\r\n",
        "        obj_bb_idx = [bb.index for bb in non_max_suppression(pred_bb_info, nms_threshold)]\r\n",
        "        \r\n",
        "        output = []\r\n",
        "        for bb in pred_bb_info:\r\n",
        "            output.append([\r\n",
        "                (bb.class_id if bb.index in obj_bb_idx else -1.0),\r\n",
        "                bb.confidence,\r\n",
        "                *bb.xyxy\r\n",
        "            ])\r\n",
        "            \r\n",
        "        return torch.tensor(output) # shape: (锚框个数, 6)\r\n",
        "    \r\n",
        "    batch_output = []\r\n",
        "    for b in range(bn):\r\n",
        "        batch_output.append(MultiBoxDetection_one(cls_prob[b], loc_pred[b], anchor[0], nms_threshold))\r\n",
        "    \r\n",
        "    return torch.stack(batch_output)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ################################# 9.6 ############################\r\n",
        "class PikachuDetDataset(torch.utils.data.Dataset):\r\n",
        "    \"\"\"皮卡丘检测数据集类\"\"\"\r\n",
        "    def __init__(self, data_dir, part, image_size=(256, 256)):\r\n",
        "        assert part in [\"train\", \"val\"]\r\n",
        "        self.image_size = image_size\r\n",
        "        self.image_dir = os.path.join(data_dir, part, \"images\")\r\n",
        "        \r\n",
        "        with open(os.path.join(data_dir, part, \"label.json\")) as f:\r\n",
        "            self.label = json.load(f)\r\n",
        "            \r\n",
        "        self.transform = torchvision.transforms.Compose([\r\n",
        "            # 将 PIL 图片转换成位于[0.0, 1.0]的floatTensor, shape (C x H x W)\r\n",
        "            torchvision.transforms.ToTensor()])\r\n",
        "            \r\n",
        "    def __len__(self):\r\n",
        "        return len(self.label)\r\n",
        "    \r\n",
        "    def __getitem__(self, index):\r\n",
        "        image_path = str(index + 1) + \".png\"\r\n",
        "        \r\n",
        "        cls = self.label[image_path][\"class\"]\r\n",
        "        label = np.array([cls] + self.label[image_path][\"loc\"], \r\n",
        "                         dtype=\"float32\")[None, :]\r\n",
        "        \r\n",
        "        PIL_img = Image.open(os.path.join(self.image_dir, image_path)\r\n",
        "                            ).convert('RGB').resize(self.image_size)\r\n",
        "        img = self.transform(PIL_img)\r\n",
        "        \r\n",
        "        sample = {\r\n",
        "            \"label\": label, # shape: (1, 5) [class, xmin, ymin, xmax, ymax]\r\n",
        "            \"image\": img    # shape: (3, *image_size)\r\n",
        "        }\r\n",
        "        \r\n",
        "        return sample\r\n",
        "\r\n",
        "def load_data_pikachu(batch_size, edge_size=256, data_dir = '../../data/pikachu'):  \r\n",
        "    \"\"\"edge_size：输出图像的宽和高\"\"\"\r\n",
        "    image_size = (edge_size, edge_size)\r\n",
        "    train_dataset = PikachuDetDataset(data_dir, 'train', image_size)\r\n",
        "    val_dataset = PikachuDetDataset(data_dir, 'val', image_size)\r\n",
        "    \r\n",
        "\r\n",
        "    train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \r\n",
        "                                             shuffle=True, num_workers=4)\r\n",
        "\r\n",
        "    val_iter = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size,\r\n",
        "                                           shuffle=False, num_workers=4)\r\n",
        "    return train_iter, val_iter\r\n",
        "\r\n",
        "\r\n",
        "# ################################# 9.9 #########################\r\n",
        "def read_voc_images(root=\"../../data/VOCdevkit/VOC2012\", \r\n",
        "                    is_train=True, max_num=None):\r\n",
        "    txt_fname = '%s/ImageSets/Segmentation/%s' % (\r\n",
        "        root, 'train.txt' if is_train else 'val.txt')\r\n",
        "    with open(txt_fname, 'r') as f:\r\n",
        "        images = f.read().split()\r\n",
        "    if max_num is not None:\r\n",
        "        images = images[:min(max_num, len(images))]\r\n",
        "    features, labels = [None] * len(images), [None] * len(images)\r\n",
        "    for i, fname in tqdm(enumerate(images)):\r\n",
        "        features[i] = Image.open('%s/JPEGImages/%s.jpg' % (root, fname)).convert(\"RGB\")\r\n",
        "        labels[i] = Image.open('%s/SegmentationClass/%s.png' % (root, fname)).convert(\"RGB\")\r\n",
        "    return features, labels # PIL image\r\n",
        "\r\n",
        "# colormap2label = torch.zeros(256 ** 3, dtype=torch.uint8)\r\n",
        "# for i, colormap in enumerate(VOC_COLORMAP):\r\n",
        "#     colormap2label[(colormap[0] * 256 + colormap[1]) * 256 + colormap[2]] = i\r\n",
        "def voc_label_indices(colormap, colormap2label):\r\n",
        "    \"\"\"\r\n",
        "    convert colormap (PIL image) to colormap2label (uint8 tensor).\r\n",
        "    \"\"\"\r\n",
        "    colormap = np.array(colormap.convert(\"RGB\")).astype('int32')\r\n",
        "    idx = ((colormap[:, :, 0] * 256 + colormap[:, :, 1]) * 256\r\n",
        "           + colormap[:, :, 2])\r\n",
        "    return colormap2label[idx]\r\n",
        "\r\n",
        "def voc_rand_crop(feature, label, height, width):\r\n",
        "    \"\"\"\r\n",
        "    Random crop feature (PIL image) and label (PIL image).\r\n",
        "    \"\"\"\r\n",
        "    i, j, h, w = torchvision.transforms.RandomCrop.get_params(\r\n",
        "            feature, output_size=(height, width))\r\n",
        "    \r\n",
        "    feature = torchvision.transforms.functional.crop(feature, i, j, h, w)\r\n",
        "    label = torchvision.transforms.functional.crop(label, i, j, h, w)    \r\n",
        "\r\n",
        "    return feature, label\r\n",
        "\r\n",
        "class VOCSegDataset(torch.utils.data.Dataset):\r\n",
        "    def __init__(self, is_train, crop_size, voc_dir, colormap2label, max_num=None):\r\n",
        "        \"\"\"\r\n",
        "        crop_size: (h, w)\r\n",
        "        \"\"\"\r\n",
        "        self.rgb_mean = np.array([0.485, 0.456, 0.406])\r\n",
        "        self.rgb_std = np.array([0.229, 0.224, 0.225])\r\n",
        "        self.tsf = torchvision.transforms.Compose([\r\n",
        "            torchvision.transforms.ToTensor(),\r\n",
        "            torchvision.transforms.Normalize(mean=self.rgb_mean, \r\n",
        "                                             std=self.rgb_std)\r\n",
        "        ])\r\n",
        "        \r\n",
        "        self.crop_size = crop_size # (h, w)\r\n",
        "        features, labels = read_voc_images(root=voc_dir, \r\n",
        "                                           is_train=is_train, \r\n",
        "                                           max_num=max_num)\r\n",
        "        self.features = self.filter(features) # PIL image\r\n",
        "        self.labels = self.filter(labels)     # PIL image\r\n",
        "        self.colormap2label = colormap2label\r\n",
        "        print('read ' + str(len(self.features)) + ' valid examples')\r\n",
        "\r\n",
        "    def filter(self, imgs):\r\n",
        "        return [img for img in imgs if (\r\n",
        "            img.size[1] >= self.crop_size[0] and\r\n",
        "            img.size[0] >= self.crop_size[1])]\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        feature, label = voc_rand_crop(self.features[idx], self.labels[idx],\r\n",
        "                                       *self.crop_size)\r\n",
        "        \r\n",
        "        return (self.tsf(feature),\r\n",
        "                voc_label_indices(label, self.colormap2label))\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.features)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ############################# 10.7 ##########################\r\n",
        "def read_imdb(folder='train', data_root=\"/S1/CSCL/tangss/Datasets/aclImdb\"): \r\n",
        "    data = []\r\n",
        "    for label in ['pos', 'neg']:\r\n",
        "        folder_name = os.path.join(data_root, folder, label)\r\n",
        "        for file in tqdm(os.listdir(folder_name)):\r\n",
        "            with open(os.path.join(folder_name, file), 'rb') as f:\r\n",
        "                review = f.read().decode('utf-8').replace('\\n', '').lower()\r\n",
        "                data.append([review, 1 if label == 'pos' else 0])\r\n",
        "    random.shuffle(data)\r\n",
        "    return data\r\n",
        "\r\n",
        "def get_tokenized_imdb(data):\r\n",
        "    \"\"\"\r\n",
        "    data: list of [string, label]\r\n",
        "    \"\"\"\r\n",
        "    def tokenizer(text):\r\n",
        "        return [tok.lower() for tok in text.split(' ')]\r\n",
        "    return [tokenizer(review) for review, _ in data]\r\n",
        "\r\n",
        "def get_vocab_imdb(data):\r\n",
        "    tokenized_data = get_tokenized_imdb(data)\r\n",
        "    counter = collections.Counter([tk for st in tokenized_data for tk in st])\r\n",
        "    return torchtext.vocab.Vocab(counter, min_freq=5)\r\n",
        "\r\n",
        "def preprocess_imdb(data, vocab):\r\n",
        "    max_l = 500  # 将每条评论通过截断或者补0，使得长度变成500\r\n",
        "\r\n",
        "    def pad(x):\r\n",
        "        return x[:max_l] if len(x) > max_l else x + [0] * (max_l - len(x))\r\n",
        "\r\n",
        "    tokenized_data = get_tokenized_imdb(data)\r\n",
        "    features = torch.tensor([pad([vocab.stoi[word] for word in words]) for words in tokenized_data])\r\n",
        "    labels = torch.tensor([score for _, score in data])\r\n",
        "    return features, labels\r\n",
        "\r\n",
        "def load_pretrained_embedding(words, pretrained_vocab):\r\n",
        "    \"\"\"从预训练好的vocab中提取出words对应的词向量\"\"\"\r\n",
        "    embed = torch.zeros(len(words), pretrained_vocab.vectors[0].shape[0]) # 初始化为0\r\n",
        "    oov_count = 0 # out of vocabulary\r\n",
        "    for i, word in enumerate(words):\r\n",
        "        try:\r\n",
        "            idx = pretrained_vocab.stoi[word]\r\n",
        "            embed[i, :] = pretrained_vocab.vectors[idx]\r\n",
        "        except KeyError:\r\n",
        "            oov_count += 1\r\n",
        "    if oov_count > 0:\r\n",
        "        print(\"There are %d oov words.\" % oov_count)\r\n",
        "    return embed\r\n",
        "\r\n",
        "def predict_sentiment(net, vocab, sentence):\r\n",
        "    \"\"\"sentence是词语的列表\"\"\"\r\n",
        "    device = list(net.parameters())[0].device\r\n",
        "    sentence = torch.tensor([vocab.stoi[word] for word in sentence], device=device)\r\n",
        "    label = torch.argmax(net(sentence.view((1, -1))), dim=1)\r\n",
        "    return 'positive' if label.item() == 1 else 'negative'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqkPEdPrVsLp",
        "outputId": "300a2ee8-c58c-4d4a-dbe4-9e2f6122ebc6"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Feb 16 13:20:16 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8    11W /  70W |      3MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImjTN1YwWR8v"
      },
      "source": [
        "def vgg_block(num_convs,in_channels,out_channels):\r\n",
        "    blk = []\r\n",
        "    for i in range(num_convs):\r\n",
        "        if i==0:\r\n",
        "            blk.append(nn.Conv2d(in_channels,out_channels,kernel_size=3,padding=1))\r\n",
        "        else:\r\n",
        "            blk.append(nn.Conv2d(out_channels,out_channels,kernel_size=3,padding=1))\r\n",
        "        blk.append(nn.ReLU())\r\n",
        "    blk.append(nn.MaxPool2d(kernel_size=2,stride=2))#这里会使宽和高减半\r\n",
        "    return nn.Sequential(*blk)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BX7ZNjwXGo6"
      },
      "source": [
        "## 2.VGG网络\r\n",
        "与AlexNet和LeNet一样，VGG网络由卷积层模块后接全连接层模块构成。卷积层模块串联数个vgg_block，其超参数由变量conv_arch定义。该变量指定了每个VGG块里卷积层个数和输入输出通道数。全连接模块则跟AlexNet中的一样。\r\n",
        "\r\n",
        "现在我们构造一个VGG网络。它有5个卷积块，前2块使用单卷积层，而后3块使用双卷积层。第一块的输入输出通道分别是1（因为下面要使用的Fashion-MNIST数据的通道数为1）和64，之后每次对输出通道数翻倍，直到变为512。因为这个网络使用了8个卷积层和3个全连接层，所以经常被称为VGG-11。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyt5JVhuAEUO"
      },
      "source": [
        "conv_arch = ((1, 1, 64), (1, 64, 128), (2, 128, 256), (2, 256, 512), (2, 512, 512))\r\n",
        "# 经过5个vgg_block, 宽高会减半5次, 变成 224/32 = 7\r\n",
        "fc_features = 512 * 7 * 7 # c * w * h\r\n",
        "fc_hidden_units = 4096 # 任意"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vozlZXNrA3zk"
      },
      "source": [
        "def vgg(conv_arch,fc_features,fc_hidden_units = 4096):\r\n",
        "    net = nn.Sequential()\r\n",
        "    # 卷积部分\r\n",
        "    for i,(num_convs,in_channels,out_channels) in enumerate(conv_arch):\r\n",
        "        # 每经过一个vgg_block都会使宽高减半\r\n",
        "        net.add_module('vgg_block_'+str(i+1),vgg_block(num_convs,in_channels,out_channels))\r\n",
        "    \r\n",
        "    # 全连接部分\r\n",
        "    net.add_module('fc',nn.Sequential(\r\n",
        "        FlattenLayer(),\r\n",
        "        nn.Linear(fc_features,fc_hidden_units),\r\n",
        "        nn.ReLU(),\r\n",
        "        nn.Dropout(0.5),\r\n",
        "        nn.Linear(fc_hidden_units,fc_hidden_units),\r\n",
        "        nn.ReLU(),\r\n",
        "        nn.Dropout(0.5),\r\n",
        "        nn.Linear(fc_hidden_units,10)\r\n",
        "    ))\r\n",
        "    return net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UVwoJ02CvZG",
        "outputId": "0769c895-de74-44b4-eefd-cba07fb9db08"
      },
      "source": [
        "# 构造一个高和宽均为224的单通道数据样本来观察每一层的输出形状。\r\n",
        "net = vgg(conv_arch,fc_features,fc_hidden_units)\r\n",
        "X = torch.rand(1,1,224,224)\r\n",
        "\r\n",
        "# named_children获取一级子模块及其名字(named_modules会返回所有子模块,包括子模块的子模块)\r\n",
        "for name,blk in net.named_children():\r\n",
        "    X = blk(X)\r\n",
        "    print(name,'output shape:',X.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vgg_block_1 output shape: torch.Size([1, 64, 112, 112])\n",
            "vgg_block_2 output shape: torch.Size([1, 128, 56, 56])\n",
            "vgg_block_3 output shape: torch.Size([1, 256, 28, 28])\n",
            "vgg_block_4 output shape: torch.Size([1, 512, 14, 14])\n",
            "vgg_block_5 output shape: torch.Size([1, 512, 7, 7])\n",
            "fc output shape: torch.Size([1, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtTt3DabCvGl"
      },
      "source": [
        "可以看到，每次我们将输入的高和宽减半，直到最终高和宽变成7后传入全连接层。与此同时，输出通道数每次翻倍，直到变成512。因为每个卷积层的窗口大小一样，所以每层的模型参数尺寸和计算复杂度与输入高、输入宽、输入通道数和输出通道数的乘积成正比。VGG这种高和宽减半以及通道翻倍的设计使得多数卷积层都有相同的模型参数尺寸和计算复杂度。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWY-mR21EiAe"
      },
      "source": [
        "## 3.获取数据和训练模型\r\n",
        "因为VGG-11计算上比AlexNet更加复杂，出于测试的目的我们构造一个通道数更小，或者说更窄的网络在Fashion-MNIST数据集上进行训练。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gah7OY2iDIFl",
        "outputId": "7df9d3bd-56eb-4b59-f614-770c4ac791c5"
      },
      "source": [
        "ratio = 8\r\n",
        "small_conv_arch = [(1, 1, 64//ratio), (1, 64//ratio, 128//ratio), (2, 128//ratio, 256//ratio), \r\n",
        "                   (2, 256//ratio, 512//ratio), (2, 512//ratio, 512//ratio)]\r\n",
        "net = vgg(small_conv_arch,fc_features // ratio , fc_hidden_units//ratio)\r\n",
        "print(net)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (vgg_block_1): Sequential(\n",
            "    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (vgg_block_2): Sequential(\n",
            "    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (vgg_block_3): Sequential(\n",
            "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU()\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (vgg_block_4): Sequential(\n",
            "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU()\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (vgg_block_5): Sequential(\n",
            "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU()\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    (0): FlattenLayer()\n",
            "    (1): Linear(in_features=3136, out_features=512, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Dropout(p=0.5, inplace=False)\n",
            "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Dropout(p=0.5, inplace=False)\n",
            "    (7): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLKHVVxYFZpE",
        "outputId": "637a4d38-4a34-4037-cbc1-a64cc1239d88"
      },
      "source": [
        "#模型训练\r\n",
        "batch_size = 64\r\n",
        "train_iter,test_iter = load_data_fashion_mnist(batch_size,resize=224)\r\n",
        "lr, num_epochs = 0.001, 5\r\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\r\n",
        "train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training on  cuda\n",
            "epoch 1, loss 0.5956, train acc 0.776, test acc 0.872, time 49.6 sec\n",
            "epoch 2, loss 0.3288, train acc 0.880, test acc 0.897, time 49.8 sec\n",
            "epoch 3, loss 0.2809, train acc 0.899, test acc 0.908, time 50.4 sec\n",
            "epoch 4, loss 0.2486, train acc 0.909, test acc 0.912, time 50.3 sec\n",
            "epoch 5, loss 0.2267, train acc 0.918, test acc 0.909, time 50.4 sec\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}