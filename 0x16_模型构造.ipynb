{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0x16_模型构造.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNa6sVFCr68xBWcVoC+Tabl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Matrixmax/Dive_into_DeepLearning/blob/main/0x16_%E6%A8%A1%E5%9E%8B%E6%9E%84%E9%80%A0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0JyVvGTMoDB"
      },
      "source": [
        "让我们回顾一下在3.10节（多层感知机的简洁实现）中含单隐藏层的多层感知机的实现方法。我们首先构造Sequential实例，然后依次添加两个全连接层。其中第一层的输出大小为256，即隐藏层单元个数是256；第二层的输出大小为10，即输出层单元个数是10。我们在上一章的其他节中也使用了Sequential类构造模型。这里我们介绍另外一种基于Module类的模型构造方法：它让模型构造更加灵活。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWZhoEG3Mq1W"
      },
      "source": [
        "## 1.继承module类来构造模型\r\n",
        "Module类是nn模块里提供的一个模型构造类，是所有神经网络模块的基类，我们可以继承它来定义我们想要的模型。下面继承Module类构造本节开头提到的多层感知机。这里定义的MLP类重载了Module类的__init__函数和forward函数。它们分别用于创建模型参数和定义前向计算。前向计算也即正向传播。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6TnAtZnnw6i"
      },
      "source": [
        "import torch\r\n",
        "from torch import nn\r\n",
        "\r\n",
        "class MLP(nn.Module):\r\n",
        "    # 声明带有模型参数的层，这里声明了两个全连接层\r\n",
        "    def __init__(self,**kwargs):\r\n",
        "        # 调用父类Module的构造函数来进行必要的初始化，\r\n",
        "        # 这样在构造实例时还可以指定其他函数参数\r\n",
        "        super(MLP,self).__init__(**kwargs)\r\n",
        "        self.hidden = nn.Linear(784,256) # 隐藏层\r\n",
        "        self.act = nn.ReLU()\r\n",
        "        self.output = nn.Linear(256,10) # 输出层\r\n",
        "\r\n",
        "    # 定义模型的前向计算，即如何根据输入x计算返回所需要的模型输出\r\n",
        "    def forward(self,x):\r\n",
        "        a = self.act(self.hidden(x))\r\n",
        "        return self.output(a)\r\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3jb5rbOp2bH"
      },
      "source": [
        "以上的MLP类中无须定义反向传播函数。系统将通过自动求梯度而自动生成反向传播所需的backward函数。\r\n",
        "\r\n",
        "我们可以实例化MLP类得到模型变量net。下面的代码初始化net并传入输入数据X做一次前向计算。其中，net(X)会调用MLP继承自Module类的__call__函数，这个函数将调用MLP类定义的forward函数来完成前向计算。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZE06jTYFp6qv",
        "outputId": "663414c2-a202-4476-d736-fd2aad3948a4"
      },
      "source": [
        "X = torch.rand(2,784)\r\n",
        "net = MLP()\r\n",
        "print(net)\r\n",
        "net(X)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLP(\n",
            "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (act): ReLU()\n",
            "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1357,  0.2254,  0.0521, -0.0727,  0.0298,  0.2904, -0.2034,  0.2638,\n",
              "          0.0799, -0.0787],\n",
              "        [ 0.0937,  0.2828,  0.1542, -0.0271,  0.0456,  0.4050, -0.3199,  0.1365,\n",
              "          0.0714, -0.1829]], grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZYc2ntkqt6c"
      },
      "source": [
        "注意，这里并没有将Module类命名为Layer（层）或者Model（模型）之类的名字，这是因为该类是一个可供自由组建的部件。它的子类既可以是一个层（如PyTorch提供的Linear类），又可以是一个模型（如这里定义的MLP类），或者是模型的一个部分。我们下面通过两个例子来展示它的灵活性。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBDsSEO6si1f"
      },
      "source": [
        "## 2. Module的子类\r\n",
        "我们刚刚提到，Module类是一个通用的部件。事实上，PyTorch还实现了继承自Module的可以方便构建模型的类: 如Sequential、ModuleList和ModuleDict等等。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocSUb4Bhspqq"
      },
      "source": [
        "### 2.1 Sequential类\r\n",
        "当模型的前向计算为简单串联各个层的计算时，Sequential类可以通过更加简单的方式定义模型。这正是Sequential类的目的：它可以接收一个子模块的有序字典（OrderedDict）或者一系列子模块作为参数来逐一添加Module的实例，而模型的前向计算就是将这些实例按添加的顺序逐一计算。\r\n",
        "\r\n",
        "下面我们实现一个与Sequential类有相同功能的MySequential类。这或许可以帮助读者更加清晰地理解Sequential类的工作机制。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ki-8g0zsgmf"
      },
      "source": [
        "class MySequential(nn.Module):\r\n",
        "    from collections import OrderedDict\r\n",
        "    def __init__(self,*args):\r\n",
        "        super(MySequential,self).__init__()\r\n",
        "        # 如果传入的是一个OrderedDict\r\n",
        "        if len(args) == 1 and isinstance(args[0],OrderedDict):\r\n",
        "            for key,module in args[0].items():\r\n",
        "                # add_module方法会将module添加进self._modules(一个OrderedDict)\r\n",
        "                self.add_module(key,module)\r\n",
        "        else: #传入的是一些module\r\n",
        "            for idx,module in enumerate(args):\r\n",
        "                self.add_module(str(idx),module)\r\n",
        "    def forward(self,input):\r\n",
        "        # self._modules返回一个OrderedDict，保证会按照成员添加时的顺序遍历成员\r\n",
        "        for module in self._modules.values():\r\n",
        "            input = module(input)\r\n",
        "        return input\r\n",
        "\r\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0MYnqXRu-L2",
        "outputId": "80eb1962-5815-4a74-d00b-efd49e7c6be2"
      },
      "source": [
        "net = MySequential(\r\n",
        "    nn.Linear(784,256),\r\n",
        "    nn.ReLU(),\r\n",
        "    nn.Linear(256,10),\r\n",
        ")\r\n",
        "print(net)\r\n",
        "net(X)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MySequential(\n",
            "  (0): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=256, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1608, -0.1231, -0.0101,  0.0453,  0.0110, -0.0345, -0.0875,  0.0304,\n",
              "         -0.0906,  0.1722],\n",
              "        [ 0.2375, -0.0513,  0.0890,  0.0478, -0.0646,  0.0981, -0.1138,  0.0560,\n",
              "         -0.1390,  0.2048]], grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6e8KOiMvRh6"
      },
      "source": [
        "### 2.2 ModuleList类\r\n",
        "\r\n",
        "ModuleList接收一个子模块的列表作为输入，然后也可以类似List那样进行append和extend操作"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wM2QGtQ1vcm8",
        "outputId": "82b57621-ba36-492b-9cd1-4f06a9e6d15d"
      },
      "source": [
        "net = nn.ModuleList([nn.Linear(784,256),nn.ReLU()])\r\n",
        "net.append(nn.Linear(256,10))# 类似list的append操作\r\n",
        "print(net[-1]) #类似list的索引访问\r\n",
        "print(net)\r\n",
        "# net(torch.zeros(1, 784)) # 会报NotImplementedError"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear(in_features=256, out_features=10, bias=True)\n",
            "ModuleList(\n",
            "  (0): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=256, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t51Jtw4owK5I"
      },
      "source": [
        "既然Sequential和ModuleList都可以进行列表化构造网络，那二者区别是什么呢。ModuleList仅仅是一个储存各种模块的列表，这些模块之间没有联系也没有顺序（所以不用保证相邻层的输入输出维度匹配），而且没有实现forward功能需要自己实现，所以上面执行net(torch.zeros(1, 784))会报NotImplementedError；而Sequential内的模块需要按照顺序排列，要保证相邻层的输入输出大小相匹配，内部forward功能已经实现。\r\n",
        "\r\n",
        "ModuleList的出现只是让网络定义前向传播时更加灵活，见下面官网的例子。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nL3u2QpCwLdT"
      },
      "source": [
        "class MyModule(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(MyModule,self).__init__()\r\n",
        "        self.linears = nn.ModuleList([nn.Linear(10,10) for i in range(10)])\r\n",
        "    \r\n",
        "    def forward(self,x):\r\n",
        "        # moduleList can act as an iterable, or be indexed using ints\r\n",
        "        for i,l in enumerate(self.linears):\r\n",
        "            x = self.linears[i // 2](x) + l(x)\r\n",
        "        return x"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba9nsjc90nY4"
      },
      "source": [
        "另外，ModuleList不同于一般的Python的list，加入到ModuleList里面的所有模块的参数会被自动添加到整个网络中，下面看一个例子对比一下。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYLL4Rn10pkK",
        "outputId": "30830831-77d3-4303-9f83-60f33cf3f93a"
      },
      "source": [
        "class Module_ModuleList(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(Module_ModuleList,self).__init__()\r\n",
        "        self.linears = nn.ModuleList([nn.Linear(10,10)])\r\n",
        "\r\n",
        "class Module_List(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(Module_List,self).__init__()\r\n",
        "        self.linears = [nn.Linear(10,10)]\r\n",
        "\r\n",
        "net1 = Module_ModuleList()\r\n",
        "net2 = Module_List()\r\n",
        "\r\n",
        "print(\"net1:\")\r\n",
        "for p in net1.parameters():\r\n",
        "    print(p.size())\r\n",
        "\r\n",
        "print(\"net2:\")\r\n",
        "for p in net2.parameters():\r\n",
        "    print(p)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "net1:\n",
            "torch.Size([10, 10])\n",
            "torch.Size([10])\n",
            "net2:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ps50o8gE22da"
      },
      "source": [
        "### 2.3 ModuelDict类\r\n",
        "\r\n",
        "ModuleDict接收一个子模块的字典作为输入, 然后也可以类似字典那样进行添加访问操作:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjFa6Wut1dWQ",
        "outputId": "ff7ed87f-bc7c-4bb0-ab08-db522e8225e7"
      },
      "source": [
        "net = nn.ModuleDict({\r\n",
        "    'linear':nn.Linear(784,256),\r\n",
        "    'act':nn.ReLU(),\r\n",
        "})\r\n",
        "net['output'] = nn.Linear(256,10) # 添加\r\n",
        "print(net['linear']) #访问\r\n",
        "print(net.output)\r\n",
        "print(net)\r\n",
        "# net(torch.zeros(1, 784)) # 会报NotImplementedError"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear(in_features=784, out_features=256, bias=True)\n",
            "Linear(in_features=256, out_features=10, bias=True)\n",
            "ModuleDict(\n",
            "  (linear): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (act): ReLU()\n",
            "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIxhG5TG3REI"
      },
      "source": [
        "和ModuleList一样，ModuleDict实例仅仅是存放了一些模块的字典，并没有定义forward函数需要自己定义。同样，ModuleDict也与Python的Dict有所不同，ModuleDict里的所有模块的参数会被自动添加到整个网络中。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFYwoLcw3S-O"
      },
      "source": [
        "## 3. 构造复杂的模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGPwJPZH3eqF"
      },
      "source": [
        "虽然上面介绍的这些类可以使模型构造更加简单，且不需要定义forward函数，但直接继承Module类可以极大地拓展模型构造的灵活性。下面我们构造一个稍微复杂点的网络FancyMLP。在这个网络中，我们通过get_constant函数创建训练中不被迭代的参数，即常数参数。在前向计算中，除了使用创建的常数参数外，我们还使用Tensor的函数和Python的控制流，并多次调用相同的层。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFh4BQO33RgD"
      },
      "source": [
        "class FancyMLP(nn.Module):\r\n",
        "    def __init__(self,**kwargs):\r\n",
        "        super(FancyMLP,self).__init__(**kwargs)\r\n",
        "        self.rand_weight = torch.rand((20,20),requires_grad=True)\r\n",
        "        self.linear = nn.Linear(20,20)\r\n",
        "    \r\n",
        "    def forward(self,x):\r\n",
        "        x = self.linear(x)\r\n",
        "        # 使用创建的常数参数，以及nn.function中的relu函数和mm函数\r\n",
        "        x = nn.functional.relu(torch.mm(x,self.rand_weight.data)+1)\r\n",
        "\r\n",
        "        # 复用全连接层，等价于两个全连接层共享参数\r\n",
        "        x = self.linear(x)\r\n",
        "        # 控制流，这里我们需要调用item函数来返回标量进行比较\r\n",
        "        while x.norm().item() > 1:\r\n",
        "            x /= 2\r\n",
        "        if x.norm().item() < 0.8:\r\n",
        "            x*= 10\r\n",
        "        return x.sum()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gnhrthW-Z30"
      },
      "source": [
        "在这个FancyMLP模型中，我们使用了常数权重rand_weight（注意它不是可训练模型参数）、做了矩阵乘法操作（torch.mm）并重复使用了相同的Linear层。下面我们来测试该模型的前向计算。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BR0h0vSv-cRF",
        "outputId": "cb9376f6-de98-4d47-a2c1-7098cbcecd1a"
      },
      "source": [
        "X = torch.rand(2,20)\r\n",
        "net = FancyMLP()\r\n",
        "print(net)\r\n",
        "net(X)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FancyMLP(\n",
            "  (linear): Linear(in_features=20, out_features=20, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-0.3646, grad_fn=<SumBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddqJBu6k-g6o",
        "outputId": "1a536dbd-168f-4f69-c8e6-7e449533b623"
      },
      "source": [
        "class NestMLP(nn.Module):\r\n",
        "    def __init__(self,**kwargs):\r\n",
        "        super(NestMLP,self).__init__(**kwargs)\r\n",
        "        self.net = nn.Sequential(nn.Linear(40,30),nn.ReLU())\r\n",
        "\r\n",
        "    def forward(self,x):\r\n",
        "        return self.net(x)\r\n",
        "        \r\n",
        "net = nn.Sequential(NestMLP(),nn.Linear(30,20),FancyMLP())\r\n",
        "X = torch.rand(2,40)\r\n",
        "print(net)\r\n",
        "net(X)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): NestMLP(\n",
            "    (net): Sequential(\n",
            "      (0): Linear(in_features=40, out_features=30, bias=True)\n",
            "      (1): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (1): Linear(in_features=30, out_features=20, bias=True)\n",
            "  (2): FancyMLP(\n",
            "    (linear): Linear(in_features=20, out_features=20, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.0643, grad_fn=<SumBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aLYmXuBEUBW"
      },
      "source": [
        "## 4.小结\r\n",
        "* 可以通过继承Module类来构造模型。\r\n",
        "Sequential、ModuleList、ModuleDict类都继承自Module类。\r\n",
        "* 与Sequential不同，ModuleList和ModuleDict并没有定义一个完整的网络，它们只是将不同的模块存放在一起，需要自己定义forward函数。\r\n",
        "* 虽然Sequential等类可以使模型构造更加简单，但直接继承Module类可以极大地拓展模型构造的灵活性。"
      ]
    }
  ]
}