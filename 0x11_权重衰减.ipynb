{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0x11_权重衰减.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNArVJjE/cHNZ0P93iFML1y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Matrixmax/Dive_into_DeepLearning/blob/main/0x11%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSF9Oz0_ZlFU"
      },
      "source": [
        "对应过拟合问题的常用方法：权重衰减"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s17RUxBtaJj5"
      },
      "source": [
        "## 1.方法\r\n",
        "权重衰减等价于L2范数正则化（regularization）。正则化通过为模型损失函数添加惩罚项使学出的模型参数值较小，是应对过拟合的常用手段。我们先描述L2​范数正则化，再解释它为何又称权重衰减。\r\n",
        "\r\n",
        "L2范数正则化在模型原损失函数基础上添加L2范数惩罚项，从而得到训练所需要最小化的函数。**L2范数惩罚项指的是模型权重参数每个元素的平方和与一个正的常数的乘积。**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AR2_75TIbfaX"
      },
      "source": [
        "## 2.高维线性回归实验"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUyksBSzXvAU"
      },
      "source": [
        " \r\n",
        "# This file is generated automatically through:\r\n",
        "#    d2lbook build lib\r\n",
        "# Don't edit it directly\r\n",
        "\r\n",
        "# Defined in file: ./chapter_preface/index.md\r\n",
        "import collections\r\n",
        "import hashlib\r\n",
        "import math\r\n",
        "import os\r\n",
        "import random\r\n",
        "import re\r\n",
        "import shutil\r\n",
        "import sys\r\n",
        "import tarfile\r\n",
        "import time\r\n",
        "import zipfile\r\n",
        "from collections import defaultdict\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "import requests\r\n",
        "from IPython import display\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "\r\n",
        "d2l = sys.modules[__name__]\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_preface/index.md\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "import torchvision\r\n",
        "from torch import nn\r\n",
        "from torch.nn import functional as F\r\n",
        "from torch.utils import data\r\n",
        "from torchvision import transforms\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_preliminaries/calculus.md\r\n",
        "def use_svg_display():\r\n",
        "    \"\"\"使用svg格式在Jupyter中显示绘图。\"\"\"\r\n",
        "    display.set_matplotlib_formats('svg')\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_preliminaries/calculus.md\r\n",
        "def set_figsize(figsize=(3.5, 2.5)):\r\n",
        "    \"\"\"设置matplotlib的图表大小。\"\"\"\r\n",
        "    use_svg_display()\r\n",
        "    d2l.plt.rcParams['figure.figsize'] = figsize\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_preliminaries/calculus.md\r\n",
        "def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\r\n",
        "    \"\"\"设置matplotlib的轴。\"\"\"\r\n",
        "    axes.set_xlabel(xlabel)\r\n",
        "    axes.set_ylabel(ylabel)\r\n",
        "    axes.set_xscale(xscale)\r\n",
        "    axes.set_yscale(yscale)\r\n",
        "    axes.set_xlim(xlim)\r\n",
        "    axes.set_ylim(ylim)\r\n",
        "    if legend:\r\n",
        "        axes.legend(legend)\r\n",
        "    axes.grid()\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_preliminaries/calculus.md\r\n",
        "def plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None,\r\n",
        "         ylim=None, xscale='linear', yscale='linear',\r\n",
        "         fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None):\r\n",
        "    \"\"\"绘制数据点。\"\"\"\r\n",
        "    if legend is None:\r\n",
        "        legend = []\r\n",
        "\r\n",
        "    set_figsize(figsize)\r\n",
        "    axes = axes if axes else d2l.plt.gca()\r\n",
        "\r\n",
        "    # Return True if `X` (tensor or list) has 1 axis\r\n",
        "    def has_one_axis(X):\r\n",
        "        return (hasattr(X, \"ndim\") and X.ndim == 1 or\r\n",
        "                isinstance(X, list) and not hasattr(X[0], \"__len__\"))\r\n",
        "\r\n",
        "    if has_one_axis(X):\r\n",
        "        X = [X]\r\n",
        "    if Y is None:\r\n",
        "        X, Y = [[]] * len(X), X\r\n",
        "    elif has_one_axis(Y):\r\n",
        "        Y = [Y]\r\n",
        "    if len(X) != len(Y):\r\n",
        "        X = X * len(Y)\r\n",
        "    axes.cla()\r\n",
        "    for x, y, fmt in zip(X, Y, fmts):\r\n",
        "        if len(x):\r\n",
        "            axes.plot(x, y, fmt)\r\n",
        "        else:\r\n",
        "            axes.plot(y, fmt)\r\n",
        "    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_linear-networks/linear-regression.md\r\n",
        "class Timer:\r\n",
        "    \"\"\"记录多次运行时间。\"\"\"\r\n",
        "    def __init__(self):\r\n",
        "        self.times = []\r\n",
        "        self.start()\r\n",
        "\r\n",
        "    def start(self):\r\n",
        "        \"\"\"启动计时器。\"\"\"\r\n",
        "        self.tik = time.time()\r\n",
        "\r\n",
        "    def stop(self):\r\n",
        "        \"\"\"停止计时器并将时间记录在列表中。\"\"\"\r\n",
        "        self.times.append(time.time() - self.tik)\r\n",
        "        return self.times[-1]\r\n",
        "\r\n",
        "    def avg(self):\r\n",
        "        \"\"\"返回平均时间。\"\"\"\r\n",
        "        return sum(self.times) / len(self.times)\r\n",
        "\r\n",
        "    def sum(self):\r\n",
        "        \"\"\"返回时间总和。\"\"\"\r\n",
        "        return sum(self.times)\r\n",
        "\r\n",
        "    def cumsum(self):\r\n",
        "        \"\"\"返回累计时间。\"\"\"\r\n",
        "        return np.array(self.times).cumsum().tolist()\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_linear-networks/linear-regression-scratch.md\r\n",
        "def synthetic_data(w, b, num_examples):\r\n",
        "    \"\"\"生成 y = Xw + b + 噪声。\"\"\"\r\n",
        "    X = d2l.normal(0, 1, (num_examples, len(w)))\r\n",
        "    y = d2l.matmul(X, w) + b\r\n",
        "    y += d2l.normal(0, 0.01, y.shape)\r\n",
        "    return X, d2l.reshape(y, (-1, 1))\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_linear-networks/linear-regression-scratch.md\r\n",
        "def linreg(X, w, b):\r\n",
        "    \"\"\"线性回归模型。\"\"\"\r\n",
        "    return d2l.matmul(X, w) + b\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_linear-networks/linear-regression-scratch.md\r\n",
        "def squared_loss(y_hat, y):\r\n",
        "    \"\"\"均方损失。\"\"\"\r\n",
        "    return (y_hat - d2l.reshape(y, y_hat.shape))**2 / 2\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_linear-networks/linear-regression-scratch.md\r\n",
        "def sgd(params, lr, batch_size):\r\n",
        "    \"\"\"小批量随机梯度下降。\"\"\"\r\n",
        "    with torch.no_grad():\r\n",
        "        for param in params:\r\n",
        "            param -= lr * param.grad / batch_size\r\n",
        "            param.grad.zero_()\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_linear-networks/linear-regression-concise.md\r\n",
        "def load_array(data_arrays, batch_size, is_train=True):\r\n",
        "    \"\"\"构造一个PyTorch数据迭代器。\"\"\"\r\n",
        "    dataset = data.TensorDataset(*data_arrays)\r\n",
        "    return data.DataLoader(dataset, batch_size, shuffle=is_train)\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_linear-networks/image-classification-dataset.md\r\n",
        "def get_fashion_mnist_labels(labels):\r\n",
        "    \"\"\"返回Fashion-MNIST数据集的文本标签。\"\"\"\r\n",
        "    text_labels = [\r\n",
        "        't-shirt', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt',\r\n",
        "        'sneaker', 'bag', 'ankle boot']\r\n",
        "    return [text_labels[int(i)] for i in labels]\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_linear-networks/image-classification-dataset.md\r\n",
        "def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):\r\n",
        "    \"\"\"Plot a list of images.\"\"\"\r\n",
        "    figsize = (num_cols * scale, num_rows * scale)\r\n",
        "    _, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)\r\n",
        "    axes = axes.flatten()\r\n",
        "    for i, (ax, img) in enumerate(zip(axes, imgs)):\r\n",
        "        if torch.is_tensor(img):\r\n",
        "            # 图片张量\r\n",
        "            ax.imshow(img.numpy())\r\n",
        "        else:\r\n",
        "            # PIL图片\r\n",
        "            ax.imshow(img)\r\n",
        "        ax.axes.get_xaxis().set_visible(False)\r\n",
        "        ax.axes.get_yaxis().set_visible(False)\r\n",
        "        if titles:\r\n",
        "            ax.set_title(titles[i])\r\n",
        "    return axes\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_linear-networks/image-classification-dataset.md\r\n",
        "def get_dataloader_workers():\r\n",
        "    \"\"\"使用4个进程来读取的数据。\"\"\"\r\n",
        "    return 4\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_linear-networks/image-classification-dataset.md\r\n",
        "def load_data_fashion_mnist(batch_size, resize=None):\r\n",
        "    \"\"\"下载Fashion-MNIST数据集，然后将其加载到内存中。\"\"\"\r\n",
        "    trans = [transforms.ToTensor()]\r\n",
        "    if resize:\r\n",
        "        trans.insert(0, transforms.Resize(resize))\r\n",
        "    trans = transforms.Compose(trans)\r\n",
        "    mnist_train = torchvision.datasets.FashionMNIST(root=\"../data\",\r\n",
        "                                                    train=True,\r\n",
        "                                                    transform=trans,\r\n",
        "                                                    download=True)\r\n",
        "    mnist_test = torchvision.datasets.FashionMNIST(root=\"../data\",\r\n",
        "                                                   train=False,\r\n",
        "                                                   transform=trans,\r\n",
        "                                                   download=True)\r\n",
        "    return (data.DataLoader(mnist_train, batch_size, shuffle=True,\r\n",
        "                            num_workers=get_dataloader_workers()),\r\n",
        "            data.DataLoader(mnist_test, batch_size, shuffle=False,\r\n",
        "                            num_workers=get_dataloader_workers()))\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_linear-networks/softmax-regression-scratch.md\r\n",
        "def accuracy(y_hat, y):\r\n",
        "    \"\"\"计算预测正确的数量。\"\"\"\r\n",
        "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\r\n",
        "        y_hat = d2l.argmax(y_hat, axis=1)\r\n",
        "    cmp = d2l.astype(y_hat, y.dtype) == y\r\n",
        "    return float(d2l.reduce_sum(d2l.astype(cmp, y.dtype)))\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_linear-networks/softmax-regression-scratch.md\r\n",
        "def evaluate_accuracy(net, data_iter):\r\n",
        "    \"\"\"计算在指定数据集上模型的精度。\"\"\"\r\n",
        "    if isinstance(net, torch.nn.Module):\r\n",
        "        net.eval()  # 将模型设置为评估模式\r\n",
        "    metric = Accumulator(2)  # 正确预测数、预测总数\r\n",
        "    for X, y in data_iter:\r\n",
        "        metric.add(accuracy(net(X), y), d2l.size(y))\r\n",
        "    return metric[0] / metric[1]\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_linear-networks/softmax-regression-scratch.md\r\n",
        "class Accumulator:\r\n",
        "    \"\"\"在`n`个变量上累加。\"\"\"\r\n",
        "    def __init__(self, n):\r\n",
        "        self.data = [0.0] * n\r\n",
        "\r\n",
        "    def add(self, *args):\r\n",
        "        self.data = [a + float(b) for a, b in zip(self.data, args)]\r\n",
        "\r\n",
        "    def reset(self):\r\n",
        "        self.data = [0.0] * len(self.data)\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        return self.data[idx]\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_linear-networks/softmax-regression-scratch.md\r\n",
        "def train_epoch_ch3(net, train_iter, loss, updater):\r\n",
        "    \"\"\"训练模型一个迭代周期（定义见第3章）。\"\"\"\r\n",
        "    # 将模型设置为训练模式\r\n",
        "    if isinstance(net, torch.nn.Module):\r\n",
        "        net.train()\r\n",
        "    # 训练损失总和、训练准确度总和、样本数\r\n",
        "    metric = Accumulator(3)\r\n",
        "    for X, y in train_iter:\r\n",
        "        # 计算梯度并更新参数\r\n",
        "        y_hat = net(X)\r\n",
        "        l = loss(y_hat, y)\r\n",
        "        if isinstance(updater, torch.optim.Optimizer):\r\n",
        "            # 使用PyTorch内置的优化器和损失函数\r\n",
        "            updater.zero_grad()\r\n",
        "            l.backward()\r\n",
        "            updater.step()\r\n",
        "            metric.add(\r\n",
        "                float(l) * len(y), accuracy(y_hat, y),\r\n",
        "                y.size().numel())\r\n",
        "        else:\r\n",
        "            # 使用PyTorch内置的优化器和损失函数\r\n",
        "            l.sum().backward()\r\n",
        "            updater(X.shape[0])\r\n",
        "            metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())\r\n",
        "    # 返回训练损失和训练准确率\r\n",
        "    return metric[0] / metric[2], metric[1] / metric[2]\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_linear-networks/softmax-regression-scratch.md\r\n",
        "class Animator:\r\n",
        "    \"\"\"在动画中绘制数据。\"\"\"\r\n",
        "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\r\n",
        "                 ylim=None, xscale='linear', yscale='linear',\r\n",
        "                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\r\n",
        "                 figsize=(3.5, 2.5)):\r\n",
        "        # 增量地绘制多条线\r\n",
        "        if legend is None:\r\n",
        "            legend = []\r\n",
        "        d2l.use_svg_display()\r\n",
        "        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)\r\n",
        "        if nrows * ncols == 1:\r\n",
        "            self.axes = [self.axes,]\r\n",
        "        # 使用lambda函数捕获参数\r\n",
        "        self.config_axes = lambda: d2l.set_axes(self.axes[\r\n",
        "            0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\r\n",
        "        self.X, self.Y, self.fmts = None, None, fmts\r\n",
        "\r\n",
        "    def add(self, x, y):\r\n",
        "        # 向图表中添加多个数据点\r\n",
        "        if not hasattr(y, \"__len__\"):\r\n",
        "            y = [y]\r\n",
        "        n = len(y)\r\n",
        "        if not hasattr(x, \"__len__\"):\r\n",
        "            x = [x] * n\r\n",
        "        if not self.X:\r\n",
        "            self.X = [[] for _ in range(n)]\r\n",
        "        if not self.Y:\r\n",
        "            self.Y = [[] for _ in range(n)]\r\n",
        "        for i, (a, b) in enumerate(zip(x, y)):\r\n",
        "            if a is not None and b is not None:\r\n",
        "                self.X[i].append(a)\r\n",
        "                self.Y[i].append(b)\r\n",
        "        self.axes[0].cla()\r\n",
        "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\r\n",
        "            self.axes[0].plot(x, y, fmt)\r\n",
        "        self.config_axes()\r\n",
        "        display.display(self.fig)\r\n",
        "        display.clear_output(wait=True)\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_linear-networks/softmax-regression-scratch.md\r\n",
        "def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):\r\n",
        "    \"\"\"训练模型（定义见第3章）。\"\"\"\r\n",
        "    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],\r\n",
        "                        legend=['train loss', 'train acc', 'test acc'])\r\n",
        "    for epoch in range(num_epochs):\r\n",
        "        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)\r\n",
        "        test_acc = evaluate_accuracy(net, test_iter)\r\n",
        "        animator.add(epoch + 1, train_metrics + (test_acc,))\r\n",
        "    train_loss, train_acc = train_metrics\r\n",
        "    assert train_loss < 0.5, train_loss\r\n",
        "    assert train_acc <= 1 and train_acc > 0.7, train_acc\r\n",
        "    assert test_acc <= 1 and test_acc > 0.7, test_acc\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_linear-networks/softmax-regression-scratch.md\r\n",
        "def predict_ch3(net, test_iter, n=6):\r\n",
        "    \"\"\"预测标签（定义见第3章）。\"\"\"\r\n",
        "    for X, y in test_iter:\r\n",
        "        break\r\n",
        "    trues = d2l.get_fashion_mnist_labels(y)\r\n",
        "    preds = d2l.get_fashion_mnist_labels(d2l.argmax(net(X), axis=1))\r\n",
        "    titles = [true + '\\n' + pred for true, pred in zip(trues, preds)]\r\n",
        "    d2l.show_images(d2l.reshape(X[0:n], (n, 28, 28)), 1, n,\r\n",
        "                    titles=titles[0:n])\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_multilayer-perceptrons/underfit-overfit.md\r\n",
        "def evaluate_loss(net, data_iter, loss):\r\n",
        "    \"\"\"评估给定数据集上模型的损失。\"\"\"\r\n",
        "    metric = d2l.Accumulator(2)  # 损失的总和, 样本数量\r\n",
        "    for X, y in data_iter:\r\n",
        "        out = net(X)\r\n",
        "        y = d2l.reshape(y, out.shape)\r\n",
        "        l = loss(out, y)\r\n",
        "        metric.add(d2l.reduce_sum(l), d2l.size(l))\r\n",
        "    return metric[0] / metric[1]\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_multilayer-perceptrons/kaggle-house-price.md\r\n",
        "DATA_HUB = dict()\r\n",
        "DATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/'\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_multilayer-perceptrons/kaggle-house-price.md\r\n",
        "def download(name, cache_dir=os.path.join('..', 'data')):\r\n",
        "    \"\"\"下载一个DATA_HUB中的文件，返回本地文件名。\"\"\"\r\n",
        "    assert name in DATA_HUB, f\"{name} 不存在于 {DATA_HUB}.\"\r\n",
        "    url, sha1_hash = DATA_HUB[name]\r\n",
        "    os.makedirs(cache_dir, exist_ok=True)\r\n",
        "    fname = os.path.join(cache_dir, url.split('/')[-1])\r\n",
        "    if os.path.exists(fname):\r\n",
        "        sha1 = hashlib.sha1()\r\n",
        "        with open(fname, 'rb') as f:\r\n",
        "            while True:\r\n",
        "                data = f.read(1048576)\r\n",
        "                if not data:\r\n",
        "                    break\r\n",
        "                sha1.update(data)\r\n",
        "        if sha1.hexdigest() == sha1_hash:\r\n",
        "            return fname  # Hit cache\r\n",
        "    print(f'正在从{url}下载{fname}...')\r\n",
        "    r = requests.get(url, stream=True, verify=True)\r\n",
        "    with open(fname, 'wb') as f:\r\n",
        "        f.write(r.content)\r\n",
        "    return fname\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_multilayer-perceptrons/kaggle-house-price.md\r\n",
        "def download_extract(name, folder=None):\r\n",
        "    \"\"\"下载并解压zip/tar文件。\"\"\"\r\n",
        "    fname = download(name)\r\n",
        "    base_dir = os.path.dirname(fname)\r\n",
        "    data_dir, ext = os.path.splitext(fname)\r\n",
        "    if ext == '.zip':\r\n",
        "        fp = zipfile.ZipFile(fname, 'r')\r\n",
        "    elif ext in ('.tar', '.gz'):\r\n",
        "        fp = tarfile.open(fname, 'r')\r\n",
        "    else:\r\n",
        "        assert False, '只有zip/tar文件可以被解压缩。'\r\n",
        "    fp.extractall(base_dir)\r\n",
        "    return os.path.join(base_dir, folder) if folder else data_dir\r\n",
        "\r\n",
        "def download_all():\r\n",
        "    \"\"\"下载DATA_HUB中的所有文件。\"\"\"\r\n",
        "    for name in DATA_HUB:\r\n",
        "        download(name)\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_multilayer-perceptrons/kaggle-house-price.md\r\n",
        "DATA_HUB['kaggle_house_train'] = (DATA_URL + 'kaggle_house_pred_train.csv',\r\n",
        "                                  '585e9cc93e70b39160e7921475f9bcd7d31219ce')\r\n",
        "\r\n",
        "DATA_HUB['kaggle_house_test'] = (DATA_URL + 'kaggle_house_pred_test.csv',\r\n",
        "                                 'fa19780a7b011d9b009e8bff8e99922a8ee2eb90')\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_deep-learning-computation/use-gpu.md\r\n",
        "def try_gpu(i=0):\r\n",
        "    \"\"\"如果存在，则返回gpu(i)，否则返回cpu()。\"\"\"\r\n",
        "    if torch.cuda.device_count() >= i + 1:\r\n",
        "        return torch.device(f'cuda:{i}')\r\n",
        "    return torch.device('cpu')\r\n",
        "\r\n",
        "def try_all_gpus():\r\n",
        "    \"\"\"返回所有可用的GPU，如果没有GPU，则返回[cpu(),]。\"\"\"\r\n",
        "    devices = [\r\n",
        "        torch.device(f'cuda:{i}') for i in range(torch.cuda.device_count())]\r\n",
        "    return devices if devices else [torch.device('cpu')]\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_convolutional-neural-networks/conv-layer.md\r\n",
        "def corr2d(X, K):\r\n",
        "    \"\"\"计算二维互相关运算。\"\"\"\r\n",
        "    h, w = K.shape\r\n",
        "    Y = d2l.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\r\n",
        "    for i in range(Y.shape[0]):\r\n",
        "        for j in range(Y.shape[1]):\r\n",
        "            Y[i, j] = d2l.reduce_sum((X[i:i + h, j:j + w] * K))\r\n",
        "    return Y\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_convolutional-neural-networks/lenet.md\r\n",
        "def evaluate_accuracy_gpu(net, data_iter, device=None):\r\n",
        "    \"\"\"使用GPU计算模型在数据集上的精度。\"\"\"\r\n",
        "    if isinstance(net, torch.nn.Module):\r\n",
        "        net.eval()  # 设置为评估模式\r\n",
        "        if not device:\r\n",
        "            device = next(iter(net.parameters())).device\r\n",
        "    # 正确预测的数量，总预测的数量\r\n",
        "    metric = d2l.Accumulator(2)\r\n",
        "    for X, y in data_iter:\r\n",
        "        if isinstance(X, list):\r\n",
        "            # BERT微调所需的（之后将介绍）\r\n",
        "            X = [x.to(device) for x in X]\r\n",
        "        else:\r\n",
        "            X = X.to(device)\r\n",
        "        y = y.to(device)\r\n",
        "        metric.add(d2l.accuracy(net(X), y), d2l.size(y))\r\n",
        "    return metric[0] / metric[1]\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_convolutional-neural-networks/lenet.md\r\n",
        "def train_ch6(net, train_iter, test_iter, num_epochs, lr, device):\r\n",
        "    \"\"\"Train a model with a GPU (defined in Chapter 6).\"\"\"\r\n",
        "    def init_weights(m):\r\n",
        "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\r\n",
        "            nn.init.xavier_uniform_(m.weight)\r\n",
        "\r\n",
        "    net.apply(init_weights)\r\n",
        "    print('training on', device)\r\n",
        "    net.to(device)\r\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\r\n",
        "    loss = nn.CrossEntropyLoss()\r\n",
        "    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\r\n",
        "                            legend=['train loss', 'train acc', 'test acc'])\r\n",
        "    timer, num_batches = d2l.Timer(), len(train_iter)\r\n",
        "    for epoch in range(num_epochs):\r\n",
        "        # 训练损失之和，训练准确率之和，范例数\r\n",
        "        metric = d2l.Accumulator(3)\r\n",
        "        net.train()\r\n",
        "        for i, (X, y) in enumerate(train_iter):\r\n",
        "            timer.start()\r\n",
        "            optimizer.zero_grad()\r\n",
        "            X, y = X.to(device), y.to(device)\r\n",
        "            y_hat = net(X)\r\n",
        "            l = loss(y_hat, y)\r\n",
        "            l.backward()\r\n",
        "            optimizer.step()\r\n",
        "            with torch.no_grad():\r\n",
        "                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\r\n",
        "            timer.stop()\r\n",
        "            train_l = metric[0] / metric[2]\r\n",
        "            train_acc = metric[1] / metric[2]\r\n",
        "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\r\n",
        "                animator.add(epoch + (i + 1) / num_batches,\r\n",
        "                             (train_l, train_acc, None))\r\n",
        "        test_acc = evaluate_accuracy_gpu(net, test_iter)\r\n",
        "        animator.add(epoch + 1, (None, None, test_acc))\r\n",
        "    print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '\r\n",
        "          f'test acc {test_acc:.3f}')\r\n",
        "    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '\r\n",
        "          f'on {str(device)}')\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_convolutional-modern/resnet.md\r\n",
        "class Residual(nn.Module):\r\n",
        "    def __init__(self, input_channels, num_channels, use_1x1conv=False,\r\n",
        "                 strides=1):\r\n",
        "        super().__init__()\r\n",
        "        self.conv1 = nn.Conv2d(input_channels, num_channels, kernel_size=3,\r\n",
        "                               padding=1, stride=strides)\r\n",
        "        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3,\r\n",
        "                               padding=1)\r\n",
        "        if use_1x1conv:\r\n",
        "            self.conv3 = nn.Conv2d(input_channels, num_channels,\r\n",
        "                                   kernel_size=1, stride=strides)\r\n",
        "        else:\r\n",
        "            self.conv3 = None\r\n",
        "        self.bn1 = nn.BatchNorm2d(num_channels)\r\n",
        "        self.bn2 = nn.BatchNorm2d(num_channels)\r\n",
        "        self.relu = nn.ReLU(inplace=True)\r\n",
        "\r\n",
        "    def forward(self, X):\r\n",
        "        Y = F.relu(self.bn1(self.conv1(X)))\r\n",
        "        Y = self.bn2(self.conv2(Y))\r\n",
        "        if self.conv3:\r\n",
        "            X = self.conv3(X)\r\n",
        "        Y += X\r\n",
        "        return F.relu(Y)\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_recurrent-neural-networks/text-preprocessing.md\r\n",
        "d2l.DATA_HUB['time_machine'] = (d2l.DATA_URL + 'timemachine.txt',\r\n",
        "                                '090b5e7e70c295757f55df93cb0a180b9691891a')\r\n",
        "\r\n",
        "def read_time_machine():\r\n",
        "    \"\"\"Load the time machine dataset into a list of text lines.\"\"\"\r\n",
        "    with open(d2l.download('time_machine'), 'r') as f:\r\n",
        "        lines = f.readlines()\r\n",
        "    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_recurrent-neural-networks/text-preprocessing.md\r\n",
        "def tokenize(lines, token='word'):\r\n",
        "    \"\"\"Split text lines into word or character tokens.\"\"\"\r\n",
        "    if token == 'word':\r\n",
        "        return [line.split() for line in lines]\r\n",
        "    elif token == 'char':\r\n",
        "        return [list(line) for line in lines]\r\n",
        "    else:\r\n",
        "        print('ERROR: unknown token type: ' + token)\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_recurrent-neural-networks/text-preprocessing.md\r\n",
        "class Vocab:\r\n",
        "    \"\"\"Vocabulary for text.\"\"\"\r\n",
        "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\r\n",
        "        if tokens is None:\r\n",
        "            tokens = []\r\n",
        "        if reserved_tokens is None:\r\n",
        "            reserved_tokens = []\r\n",
        "        # Sort according to frequencies\r\n",
        "        counter = count_corpus(tokens)\r\n",
        "        self.token_freqs = sorted(counter.items(), key=lambda x: x[0])\r\n",
        "        self.token_freqs.sort(key=lambda x: x[1], reverse=True)\r\n",
        "        # The index for the unknown token is 0\r\n",
        "        self.unk, uniq_tokens = 0, ['<unk>'] + reserved_tokens\r\n",
        "        uniq_tokens += [\r\n",
        "            token for token, freq in self.token_freqs\r\n",
        "            if freq >= min_freq and token not in uniq_tokens]\r\n",
        "        self.idx_to_token, self.token_to_idx = [], dict()\r\n",
        "        for token in uniq_tokens:\r\n",
        "            self.idx_to_token.append(token)\r\n",
        "            self.token_to_idx[token] = len(self.idx_to_token) - 1\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.idx_to_token)\r\n",
        "\r\n",
        "    def __getitem__(self, tokens):\r\n",
        "        if not isinstance(tokens, (list, tuple)):\r\n",
        "            return self.token_to_idx.get(tokens, self.unk)\r\n",
        "        return [self.__getitem__(token) for token in tokens]\r\n",
        "\r\n",
        "    def to_tokens(self, indices):\r\n",
        "        if not isinstance(indices, (list, tuple)):\r\n",
        "            return self.idx_to_token[indices]\r\n",
        "        return [self.idx_to_token[index] for index in indices]\r\n",
        "\r\n",
        "def count_corpus(tokens):\r\n",
        "    \"\"\"Count token frequencies.\"\"\"\r\n",
        "    # Here `tokens` is a 1D list or 2D list\r\n",
        "    if len(tokens) == 0 or isinstance(tokens[0], list):\r\n",
        "        # Flatten a list of token lists into a list of tokens\r\n",
        "        tokens = [token for line in tokens for token in line]\r\n",
        "    return collections.Counter(tokens)\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_recurrent-neural-networks/text-preprocessing.md\r\n",
        "def load_corpus_time_machine(max_tokens=-1):\r\n",
        "    \"\"\"Return token indices and the vocabulary of the time machine dataset.\"\"\"\r\n",
        "    lines = read_time_machine()\r\n",
        "    tokens = tokenize(lines, 'char')\r\n",
        "    vocab = Vocab(tokens)\r\n",
        "    # Since each text line in the time machine dataset is not necessarily a\r\n",
        "    # sentence or a paragraph, flatten all the text lines into a single list\r\n",
        "    corpus = [vocab[token] for line in tokens for token in line]\r\n",
        "    if max_tokens > 0:\r\n",
        "        corpus = corpus[:max_tokens]\r\n",
        "    return corpus, vocab\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_recurrent-neural-networks/language-models-and-dataset.md\r\n",
        "def seq_data_iter_random(corpus, batch_size, num_steps):\r\n",
        "    \"\"\"Generate a minibatch of subsequences using random sampling.\"\"\"\r\n",
        "    # Start with a random offset to partition a sequence\r\n",
        "    corpus = corpus[random.randint(0, num_steps):]\r\n",
        "    # Subtract 1 since we need to account for labels\r\n",
        "    num_subseqs = (len(corpus) - 1) // num_steps\r\n",
        "    # The starting indices for subsequences of length `num_steps`\r\n",
        "    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))\r\n",
        "    # In random sampling, the subsequences from two adjacent random\r\n",
        "    # minibatches during iteration are not necessarily adjacent on the\r\n",
        "    # original sequence\r\n",
        "    random.shuffle(initial_indices)\r\n",
        "\r\n",
        "    def data(pos):\r\n",
        "        # Return a sequence of length `num_steps` starting from `pos`\r\n",
        "        return corpus[pos:pos + num_steps]\r\n",
        "\r\n",
        "    num_subseqs_per_example = num_subseqs // batch_size\r\n",
        "    for i in range(0, batch_size * num_subseqs_per_example, batch_size):\r\n",
        "        # Here, `initial_indices` contains randomized starting indices for\r\n",
        "        # subsequences\r\n",
        "        initial_indices_per_batch = initial_indices[i:i + batch_size]\r\n",
        "        X = [data(j) for j in initial_indices_per_batch]\r\n",
        "        Y = [data(j + 1) for j in initial_indices_per_batch]\r\n",
        "        yield d2l.tensor(X), d2l.tensor(Y)\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_recurrent-neural-networks/language-models-and-dataset.md\r\n",
        "def seq_data_iter_sequential(corpus, batch_size, num_steps):\r\n",
        "    \"\"\"Generate a minibatch of subsequences using sequential partitioning.\"\"\"\r\n",
        "    # Start with a random offset to partition a sequence\r\n",
        "    offset = random.randint(0, num_steps)\r\n",
        "    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size\r\n",
        "    Xs = d2l.tensor(corpus[offset:offset + num_tokens])\r\n",
        "    Ys = d2l.tensor(corpus[offset + 1:offset + 1 + num_tokens])\r\n",
        "    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)\r\n",
        "    num_batches = Xs.shape[1] // num_steps\r\n",
        "    for i in range(0, num_batches * num_steps, num_steps):\r\n",
        "        X = Xs[:, i:i + num_steps]\r\n",
        "        Y = Ys[:, i:i + num_steps]\r\n",
        "        yield X, Y\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_recurrent-neural-networks/language-models-and-dataset.md\r\n",
        "class SeqDataLoader:\r\n",
        "    \"\"\"An iterator to load sequence data.\"\"\"\r\n",
        "    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):\r\n",
        "        if use_random_iter:\r\n",
        "            self.data_iter_fn = d2l.seq_data_iter_random\r\n",
        "        else:\r\n",
        "            self.data_iter_fn = d2l.seq_data_iter_sequential\r\n",
        "        self.corpus, self.vocab = d2l.load_corpus_time_machine(max_tokens)\r\n",
        "        self.batch_size, self.num_steps = batch_size, num_steps\r\n",
        "\r\n",
        "    def __iter__(self):\r\n",
        "        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_recurrent-neural-networks/language-models-and-dataset.md\r\n",
        "def load_data_time_machine(batch_size, num_steps, use_random_iter=False,\r\n",
        "                           max_tokens=10000):\r\n",
        "    \"\"\"Return the iterator and the vocabulary of the time machine dataset.\"\"\"\r\n",
        "    data_iter = SeqDataLoader(batch_size, num_steps, use_random_iter,\r\n",
        "                              max_tokens)\r\n",
        "    return data_iter, data_iter.vocab\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_recurrent-neural-networks/rnn-scratch.md\r\n",
        "class RNNModelScratch:\r\n",
        "    \"\"\"A RNN Model implemented from scratch.\"\"\"\r\n",
        "    def __init__(self, vocab_size, num_hiddens, device, get_params,\r\n",
        "                 init_state, forward_fn):\r\n",
        "        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens\r\n",
        "        self.params = get_params(vocab_size, num_hiddens, device)\r\n",
        "        self.init_state, self.forward_fn = init_state, forward_fn\r\n",
        "\r\n",
        "    def __call__(self, X, state):\r\n",
        "        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)\r\n",
        "        return self.forward_fn(X, state, self.params)\r\n",
        "\r\n",
        "    def begin_state(self, batch_size, device):\r\n",
        "        return self.init_state(batch_size, self.num_hiddens, device)\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_recurrent-neural-networks/rnn-scratch.md\r\n",
        "def predict_ch8(prefix, num_preds, model, vocab, device):\r\n",
        "    \"\"\"Generate new characters following the `prefix`.\"\"\"\r\n",
        "    state = model.begin_state(batch_size=1, device=device)\r\n",
        "    outputs = [vocab[prefix[0]]]\r\n",
        "    get_input = lambda: d2l.reshape(d2l.tensor([outputs[-1]], device=device),\r\n",
        "                                    (1, 1))\r\n",
        "    for y in prefix[1:]:  # Warm-up period\r\n",
        "        _, state = model(get_input(), state)\r\n",
        "        outputs.append(vocab[y])\r\n",
        "    for _ in range(num_preds):  # Predict `num_preds` steps\r\n",
        "        y, state = model(get_input(), state)\r\n",
        "        outputs.append(int(y.argmax(dim=1).reshape(1)))\r\n",
        "    return ''.join([vocab.idx_to_token[i] for i in outputs])\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_recurrent-neural-networks/rnn-scratch.md\r\n",
        "def grad_clipping(model, theta):\r\n",
        "    \"\"\"Clip the gradient.\"\"\"\r\n",
        "    if isinstance(model, nn.Module):\r\n",
        "        params = [p for p in model.parameters() if p.requires_grad]\r\n",
        "    else:\r\n",
        "        params = model.params\r\n",
        "    norm = torch.sqrt(sum(torch.sum((p.grad**2)) for p in params))\r\n",
        "    if norm > theta:\r\n",
        "        for param in params:\r\n",
        "            param.grad[:] *= theta / norm\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_recurrent-neural-networks/rnn-scratch.md\r\n",
        "def train_epoch_ch8(model, train_iter, loss, updater, device,\r\n",
        "                    use_random_iter):\r\n",
        "    \"\"\"Train a model within one epoch (defined in Chapter 8).\"\"\"\r\n",
        "    state, timer = None, d2l.Timer()\r\n",
        "    metric = d2l.Accumulator(2)  # Sum of training loss, no. of tokens\r\n",
        "    for X, Y in train_iter:\r\n",
        "        if state is None or use_random_iter:\r\n",
        "            # Initialize `state` when either it is the first iteration or\r\n",
        "            # using random sampling\r\n",
        "            state = model.begin_state(batch_size=X.shape[0], device=device)\r\n",
        "        else:\r\n",
        "            if isinstance(model, nn.Module) and not isinstance(state, tuple):\r\n",
        "                # `state` is a tensor for `nn.GRU`\r\n",
        "                state.detach_()\r\n",
        "            else:\r\n",
        "                # `state` is a tuple of tensors for `nn.LSTM` and\r\n",
        "                # for our custom scratch implementation\r\n",
        "                for s in state:\r\n",
        "                    s.detach_()\r\n",
        "        y = Y.T.reshape(-1)\r\n",
        "        X, y = X.to(device), y.to(device)\r\n",
        "        y_hat, state = model(X, state)\r\n",
        "        l = loss(y_hat, y.long()).mean()\r\n",
        "        if isinstance(updater, torch.optim.Optimizer):\r\n",
        "            updater.zero_grad()\r\n",
        "            l.backward()\r\n",
        "            grad_clipping(model, 1)\r\n",
        "            updater.step()\r\n",
        "        else:\r\n",
        "            l.backward()\r\n",
        "            grad_clipping(model, 1)\r\n",
        "            # Since the `mean` function has been invoked\r\n",
        "            updater(batch_size=1)\r\n",
        "        metric.add(l * d2l.size(y), d2l.size(y))\r\n",
        "    return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_recurrent-neural-networks/rnn-scratch.md\r\n",
        "def train_ch8(model, train_iter, vocab, lr, num_epochs, device,\r\n",
        "              use_random_iter=False):\r\n",
        "    \"\"\"Train a model (defined in Chapter 8).\"\"\"\r\n",
        "    loss = nn.CrossEntropyLoss()\r\n",
        "    animator = d2l.Animator(xlabel='epoch', ylabel='perplexity',\r\n",
        "                            legend=['train'], xlim=[10, num_epochs])\r\n",
        "    # Initialize\r\n",
        "    if isinstance(model, nn.Module):\r\n",
        "        updater = torch.optim.SGD(model.parameters(), lr)\r\n",
        "    else:\r\n",
        "        updater = lambda batch_size: d2l.sgd(model.params, lr, batch_size)\r\n",
        "    predict = lambda prefix: predict_ch8(prefix, 50, model, vocab, device)\r\n",
        "    # Train and predict\r\n",
        "    for epoch in range(num_epochs):\r\n",
        "        ppl, speed = train_epoch_ch8(model, train_iter, loss, updater, device,\r\n",
        "                                     use_random_iter)\r\n",
        "        if (epoch + 1) % 10 == 0:\r\n",
        "            print(predict('time traveller'))\r\n",
        "            animator.add(epoch + 1, [ppl])\r\n",
        "    print(f'perplexity {ppl:.1f}, {speed:.1f} tokens/sec on {str(device)}')\r\n",
        "    print(predict('time traveller'))\r\n",
        "    print(predict('traveller'))\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_recurrent-neural-networks/rnn-concise.md\r\n",
        "class RNNModel(nn.Module):\r\n",
        "    \"\"\"The RNN model.\"\"\"\r\n",
        "    def __init__(self, rnn_layer, vocab_size, **kwargs):\r\n",
        "        super(RNNModel, self).__init__(**kwargs)\r\n",
        "        self.rnn = rnn_layer\r\n",
        "        self.vocab_size = vocab_size\r\n",
        "        self.num_hiddens = self.rnn.hidden_size\r\n",
        "        # If the RNN is bidirectional (to be introduced later),\r\n",
        "        # `num_directions` should be 2, else it should be 1.\r\n",
        "        if not self.rnn.bidirectional:\r\n",
        "            self.num_directions = 1\r\n",
        "            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)\r\n",
        "        else:\r\n",
        "            self.num_directions = 2\r\n",
        "            self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)\r\n",
        "\r\n",
        "    def forward(self, inputs, state):\r\n",
        "        X = F.one_hot(inputs.T.long(), self.vocab_size)\r\n",
        "        X = X.to(torch.float32)\r\n",
        "        Y, state = self.rnn(X, state)\r\n",
        "        # The fully connected layer will first change the shape of `Y` to\r\n",
        "        # (`num_steps` * `batch_size`, `num_hiddens`). Its output shape is\r\n",
        "        # (`num_steps` * `batch_size`, `vocab_size`).\r\n",
        "        output = self.linear(Y.reshape((-1, Y.shape[-1])))\r\n",
        "        return output, state\r\n",
        "\r\n",
        "    def begin_state(self, device, batch_size=1):\r\n",
        "        if not isinstance(self.rnn, nn.LSTM):\r\n",
        "            # `nn.GRU` takes a tensor as hidden state\r\n",
        "            return torch.zeros((self.num_directions * self.rnn.num_layers,\r\n",
        "                                batch_size, self.num_hiddens), device=device)\r\n",
        "        else:\r\n",
        "            # `nn.LSTM` takes a tuple of hidden states\r\n",
        "            return (torch.zeros((self.num_directions * self.rnn.num_layers,\r\n",
        "                                 batch_size, self.num_hiddens),\r\n",
        "                                device=device),\r\n",
        "                    torch.zeros((self.num_directions * self.rnn.num_layers,\r\n",
        "                                 batch_size, self.num_hiddens),\r\n",
        "                                device=device))\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_recurrent-modern/machine-translation-and-dataset.md\r\n",
        "d2l.DATA_HUB['fra-eng'] = (d2l.DATA_URL + 'fra-eng.zip',\r\n",
        "                           '94646ad1522d915e7b0f9296181140edcf86a4f5')\r\n",
        "\r\n",
        "def read_data_nmt():\r\n",
        "    \"\"\"Load the English-French dataset.\"\"\"\r\n",
        "    data_dir = d2l.download_extract('fra-eng')\r\n",
        "    with open(os.path.join(data_dir, 'fra.txt'), 'r') as f:\r\n",
        "        return f.read()\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_recurrent-modern/machine-translation-and-dataset.md\r\n",
        "def preprocess_nmt(text):\r\n",
        "    \"\"\"Preprocess the English-French dataset.\"\"\"\r\n",
        "    def no_space(char, prev_char):\r\n",
        "        return char in set(',.!?') and prev_char != ' '\r\n",
        "\r\n",
        "    # Replace non-breaking space with space, and convert uppercase letters to\r\n",
        "    # lowercase ones\r\n",
        "    text = text.replace('\\u202f', ' ').replace('\\xa0', ' ').lower()\r\n",
        "    # Insert space between words and punctuation marks\r\n",
        "    out = [\r\n",
        "        ' ' + char if i > 0 and no_space(char, text[i - 1]) else char\r\n",
        "        for i, char in enumerate(text)]\r\n",
        "    return ''.join(out)\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_recurrent-modern/machine-translation-and-dataset.md\r\n",
        "def tokenize_nmt(text, num_examples=None):\r\n",
        "    \"\"\"Tokenize the English-French dataset.\"\"\"\r\n",
        "    source, target = [], []\r\n",
        "    for i, line in enumerate(text.split('\\n')):\r\n",
        "        if num_examples and i > num_examples:\r\n",
        "            break\r\n",
        "        parts = line.split('\\t')\r\n",
        "        if len(parts) == 2:\r\n",
        "            source.append(parts[0].split(' '))\r\n",
        "            target.append(parts[1].split(' '))\r\n",
        "    return source, target\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_recurrent-modern/machine-translation-and-dataset.md\r\n",
        "def truncate_pad(line, num_steps, padding_token):\r\n",
        "    \"\"\"Truncate or pad sequences.\"\"\"\r\n",
        "    if len(line) > num_steps:\r\n",
        "        return line[:num_steps]  # Truncate\r\n",
        "    return line + [padding_token] * (num_steps - len(line))  # Pad\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_recurrent-modern/machine-translation-and-dataset.md\r\n",
        "def build_array_nmt(lines, vocab, num_steps):\r\n",
        "    \"\"\"Transform text sequences of machine translation into minibatches.\"\"\"\r\n",
        "    lines = [vocab[l] for l in lines]\r\n",
        "    lines = [l + [vocab['<eos>']] for l in lines]\r\n",
        "    array = d2l.tensor([\r\n",
        "        truncate_pad(l, num_steps, vocab['<pad>']) for l in lines])\r\n",
        "    valid_len = d2l.reduce_sum(d2l.astype(array != vocab['<pad>'], d2l.int32),\r\n",
        "                               1)\r\n",
        "    return array, valid_len\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_recurrent-modern/machine-translation-and-dataset.md\r\n",
        "def load_data_nmt(batch_size, num_steps, num_examples=600):\r\n",
        "    \"\"\"Return the iterator and the vocabularies of the translation dataset.\"\"\"\r\n",
        "    text = preprocess_nmt(read_data_nmt())\r\n",
        "    source, target = tokenize_nmt(text, num_examples)\r\n",
        "    src_vocab = d2l.Vocab(source, min_freq=2,\r\n",
        "                          reserved_tokens=['<pad>', '<bos>', '<eos>'])\r\n",
        "    tgt_vocab = d2l.Vocab(target, min_freq=2,\r\n",
        "                          reserved_tokens=['<pad>', '<bos>', '<eos>'])\r\n",
        "    src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)\r\n",
        "    tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps)\r\n",
        "    data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)\r\n",
        "    data_iter = d2l.load_array(data_arrays, batch_size)\r\n",
        "    return data_iter, src_vocab, tgt_vocab\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_recurrent-modern/encoder-decoder.md\r\n",
        "class Encoder(nn.Module):\r\n",
        "    \"\"\"The base encoder interface for the encoder-decoder architecture.\"\"\"\r\n",
        "    def __init__(self, **kwargs):\r\n",
        "        super(Encoder, self).__init__(**kwargs)\r\n",
        "\r\n",
        "    def forward(self, X, *args):\r\n",
        "        raise NotImplementedError\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_recurrent-modern/encoder-decoder.md\r\n",
        "class Decoder(nn.Module):\r\n",
        "    \"\"\"The base decoder interface for the encoder-decoder architecture.\"\"\"\r\n",
        "    def __init__(self, **kwargs):\r\n",
        "        super(Decoder, self).__init__(**kwargs)\r\n",
        "\r\n",
        "    def init_state(self, enc_outputs, *args):\r\n",
        "        raise NotImplementedError\r\n",
        "\r\n",
        "    def forward(self, X, state):\r\n",
        "        raise NotImplementedError\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_recurrent-modern/encoder-decoder.md\r\n",
        "class EncoderDecoder(nn.Module):\r\n",
        "    \"\"\"The base class for the encoder-decoder architecture.\"\"\"\r\n",
        "    def __init__(self, encoder, decoder, **kwargs):\r\n",
        "        super(EncoderDecoder, self).__init__(**kwargs)\r\n",
        "        self.encoder = encoder\r\n",
        "        self.decoder = decoder\r\n",
        "\r\n",
        "    def forward(self, enc_X, dec_X, *args):\r\n",
        "        enc_outputs = self.encoder(enc_X, *args)\r\n",
        "        dec_state = self.decoder.init_state(enc_outputs, *args)\r\n",
        "        return self.decoder(dec_X, dec_state)\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_recurrent-modern/seq2seq.md\r\n",
        "class Seq2SeqEncoder(d2l.Encoder):\r\n",
        "    \"\"\"The RNN encoder for sequence to sequence learning.\"\"\"\r\n",
        "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\r\n",
        "                 dropout=0, **kwargs):\r\n",
        "        super(Seq2SeqEncoder, self).__init__(**kwargs)\r\n",
        "        # Embedding layer\r\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\r\n",
        "        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers,\r\n",
        "                          dropout=dropout)\r\n",
        "\r\n",
        "    def forward(self, X, *args):\r\n",
        "        # The output `X` shape: (`batch_size`, `num_steps`, `embed_size`)\r\n",
        "        X = self.embedding(X)\r\n",
        "        # In RNN models, the first axis corresponds to time steps\r\n",
        "        X = X.permute(1, 0, 2)\r\n",
        "        # When state is not mentioned, it defaults to zeros\r\n",
        "        output, state = self.rnn(X)\r\n",
        "        # `output` shape: (`num_steps`, `batch_size`, `num_hiddens`)\r\n",
        "        # `state` shape: (`num_layers`, `batch_size`, `num_hiddens`)\r\n",
        "        return output, state\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_recurrent-modern/seq2seq.md\r\n",
        "def sequence_mask(X, valid_len, value=0):\r\n",
        "    \"\"\"Mask irrelevant entries in sequences.\"\"\"\r\n",
        "    maxlen = X.size(1)\r\n",
        "    mask = torch.arange((maxlen), dtype=torch.float32,\r\n",
        "                        device=X.device)[None, :] < valid_len[:, None]\r\n",
        "    X[~mask] = value\r\n",
        "    return X\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_recurrent-modern/seq2seq.md\r\n",
        "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\r\n",
        "    \"\"\"The softmax cross-entropy loss with masks.\"\"\"\r\n",
        "\r\n",
        "    # `pred` shape: (`batch_size`, `num_steps`, `vocab_size`)\r\n",
        "    # `label` shape: (`batch_size`, `num_steps`)\r\n",
        "    # `valid_len` shape: (`batch_size`,)\r\n",
        "    def forward(self, pred, label, valid_len):\r\n",
        "        weights = torch.ones_like(label)\r\n",
        "        weights = sequence_mask(weights, valid_len)\r\n",
        "        self.reduction = 'none'\r\n",
        "        unweighted_loss = super(MaskedSoftmaxCELoss,\r\n",
        "                                self).forward(pred.permute(0, 2, 1), label)\r\n",
        "        weighted_loss = (unweighted_loss * weights).mean(dim=1)\r\n",
        "        return weighted_loss\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_recurrent-modern/seq2seq.md\r\n",
        "def train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device):\r\n",
        "    \"\"\"Train a model for sequence to sequence.\"\"\"\r\n",
        "    def xavier_init_weights(m):\r\n",
        "        if type(m) == nn.Linear:\r\n",
        "            nn.init.xavier_uniform_(m.weight)\r\n",
        "        if type(m) == nn.GRU:\r\n",
        "            for param in m._flat_weights_names:\r\n",
        "                if \"weight\" in param:\r\n",
        "                    nn.init.xavier_uniform_(m._parameters[param])\r\n",
        "\r\n",
        "    net.apply(xavier_init_weights)\r\n",
        "    net.to(device)\r\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\r\n",
        "    loss = MaskedSoftmaxCELoss()\r\n",
        "    net.train()\r\n",
        "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\r\n",
        "                            xlim=[10, num_epochs])\r\n",
        "    for epoch in range(num_epochs):\r\n",
        "        timer = d2l.Timer()\r\n",
        "        metric = d2l.Accumulator(2)  # Sum of training loss, no. of tokens\r\n",
        "        for batch in data_iter:\r\n",
        "            X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]\r\n",
        "            bos = torch.tensor([tgt_vocab['<bos>']] * Y.shape[0],\r\n",
        "                               device=device).reshape(-1, 1)\r\n",
        "            dec_input = d2l.concat([bos, Y[:, :-1]], 1)  # Teacher forcing\r\n",
        "            Y_hat, _ = net(X, dec_input, X_valid_len)\r\n",
        "            l = loss(Y_hat, Y, Y_valid_len)\r\n",
        "            l.sum().backward()  # Make the loss scalar for `backward`\r\n",
        "            d2l.grad_clipping(net, 1)\r\n",
        "            num_tokens = Y_valid_len.sum()\r\n",
        "            optimizer.step()\r\n",
        "            with torch.no_grad():\r\n",
        "                metric.add(l.sum(), num_tokens)\r\n",
        "        if (epoch + 1) % 10 == 0:\r\n",
        "            animator.add(epoch + 1, (metric[0] / metric[1],))\r\n",
        "    print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} '\r\n",
        "          f'tokens/sec on {str(device)}')\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_recurrent-modern/seq2seq.md\r\n",
        "def predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, num_steps,\r\n",
        "                    device, save_attention_weights=False):\r\n",
        "    \"\"\"Predict for sequence to sequence.\"\"\"\r\n",
        "    # Set `net` to eval mode for inference\r\n",
        "    net.eval()\r\n",
        "    src_tokens = src_vocab[src_sentence.lower().split(' ')] + [\r\n",
        "        src_vocab['<eos>']]\r\n",
        "    enc_valid_len = torch.tensor([len(src_tokens)], device=device)\r\n",
        "    src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])\r\n",
        "    # Add the batch axis\r\n",
        "    enc_X = torch.unsqueeze(\r\n",
        "        torch.tensor(src_tokens, dtype=torch.long, device=device), dim=0)\r\n",
        "    enc_outputs = net.encoder(enc_X, enc_valid_len)\r\n",
        "    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)\r\n",
        "    # Add the batch axis\r\n",
        "    dec_X = torch.unsqueeze(\r\n",
        "        torch.tensor([tgt_vocab['<bos>']], dtype=torch.long, device=device),\r\n",
        "        dim=0)\r\n",
        "    output_seq, attention_weight_seq = [], []\r\n",
        "    for _ in range(num_steps):\r\n",
        "        Y, dec_state = net.decoder(dec_X, dec_state)\r\n",
        "        # We use the token with the highest prediction likelihood as the input\r\n",
        "        # of the decoder at the next time step\r\n",
        "        dec_X = Y.argmax(dim=2)\r\n",
        "        pred = dec_X.squeeze(dim=0).type(torch.int32).item()\r\n",
        "        # Save attention weights (to be covered later)\r\n",
        "        if save_attention_weights:\r\n",
        "            attention_weight_seq.append(net.decoder.attention_weights)\r\n",
        "        # Once the end-of-sequence token is predicted, the generation of the\r\n",
        "        # output sequence is complete\r\n",
        "        if pred == tgt_vocab['<eos>']:\r\n",
        "            break\r\n",
        "        output_seq.append(pred)\r\n",
        "    return ' '.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_recurrent-modern/seq2seq.md\r\n",
        "def bleu(pred_seq, label_seq, k):\r\n",
        "    \"\"\"Compute the BLEU.\"\"\"\r\n",
        "    pred_tokens, label_tokens = pred_seq.split(' '), label_seq.split(' ')\r\n",
        "    len_pred, len_label = len(pred_tokens), len(label_tokens)\r\n",
        "    score = math.exp(min(0, 1 - len_label / len_pred))\r\n",
        "    for n in range(1, k + 1):\r\n",
        "        num_matches, label_subs = 0, collections.defaultdict(int)\r\n",
        "        for i in range(len_label - n + 1):\r\n",
        "            label_subs[''.join(label_tokens[i:i + n])] += 1\r\n",
        "        for i in range(len_pred - n + 1):\r\n",
        "            if label_subs[''.join(pred_tokens[i:i + n])] > 0:\r\n",
        "                num_matches += 1\r\n",
        "                label_subs[''.join(pred_tokens[i:i + n])] -= 1\r\n",
        "        score *= math.pow(num_matches / (len_pred - n + 1), math.pow(0.5, n))\r\n",
        "    return score\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_attention-mechanisms/attention-cues.md\r\n",
        "def show_heatmaps(matrices, xlabel, ylabel, titles=None, figsize=(2.5, 2.5),\r\n",
        "                  cmap='Reds'):\r\n",
        "    d2l.use_svg_display()\r\n",
        "    num_rows, num_cols = matrices.shape[0], matrices.shape[1]\r\n",
        "    fig, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize,\r\n",
        "                                 sharex=True, sharey=True, squeeze=False)\r\n",
        "    for i, (row_axes, row_matrices) in enumerate(zip(axes, matrices)):\r\n",
        "        for j, (ax, matrix) in enumerate(zip(row_axes, row_matrices)):\r\n",
        "            pcm = ax.imshow(d2l.numpy(matrix), cmap=cmap)\r\n",
        "            if i == num_rows - 1:\r\n",
        "                ax.set_xlabel(xlabel)\r\n",
        "            if j == 0:\r\n",
        "                ax.set_ylabel(ylabel)\r\n",
        "            if titles:\r\n",
        "                ax.set_title(titles[j])\r\n",
        "    fig.colorbar(pcm, ax=axes, shrink=0.6)\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_attention-mechanisms/attention-scoring-functions.md\r\n",
        "def masked_softmax(X, valid_lens):\r\n",
        "    \"\"\"Perform softmax operation by masking elements on the last axis.\"\"\"\r\n",
        "    # `X`: 3D tensor, `valid_lens`: 1D or 2D tensor\r\n",
        "    if valid_lens is None:\r\n",
        "        return nn.functional.softmax(X, dim=-1)\r\n",
        "    else:\r\n",
        "        shape = X.shape\r\n",
        "        if valid_lens.dim() == 1:\r\n",
        "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\r\n",
        "        else:\r\n",
        "            valid_lens = valid_lens.reshape(-1)\r\n",
        "        # On the last axis, replace masked elements with a very large negative\r\n",
        "        # value, whose exponentiation outputs 0\r\n",
        "        X = d2l.sequence_mask(X.reshape(-1, shape[-1]), valid_lens,\r\n",
        "                              value=-1e6)\r\n",
        "        return nn.functional.softmax(X.reshape(shape), dim=-1)\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_attention-mechanisms/attention-scoring-functions.md\r\n",
        "class AdditiveAttention(nn.Module):\r\n",
        "    def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs):\r\n",
        "        super(AdditiveAttention, self).__init__(**kwargs)\r\n",
        "        self.W_k = nn.Linear(key_size, num_hiddens, bias=False)\r\n",
        "        self.W_q = nn.Linear(query_size, num_hiddens, bias=False)\r\n",
        "        self.w_v = nn.Linear(num_hiddens, 1, bias=False)\r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "\r\n",
        "    def forward(self, queries, keys, values, valid_lens):\r\n",
        "        queries, keys = self.W_q(queries), self.W_k(keys)\r\n",
        "        # After dimension expansion, shape of `queries`: (`batch_size`, no. of\r\n",
        "        # queries, 1, `num_hiddens`) and shape of `keys`: (`batch_size`, 1,\r\n",
        "        # no. of key-value pairs, `num_hiddens`). Sum them up with\r\n",
        "        # broadcasting\r\n",
        "        features = queries.unsqueeze(2) + keys.unsqueeze(1)\r\n",
        "        features = torch.tanh(features)\r\n",
        "        # There is only one output of `self.w_v`, so we remove the last\r\n",
        "        # one-dimensional entry from the shape. Shape of `scores`:\r\n",
        "        # (`batch_size`, no. of queries, no. of key-value pairs)\r\n",
        "        scores = self.w_v(features).squeeze(-1)\r\n",
        "        self.attention_weights = masked_softmax(scores, valid_lens)\r\n",
        "        # Shape of `values`: (`batch_size`, no. of key-value pairs, value\r\n",
        "        # dimension)\r\n",
        "        return torch.bmm(self.dropout(self.attention_weights), values)\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_attention-mechanisms/attention-scoring-functions.md\r\n",
        "class DotProductAttention(nn.Module):\r\n",
        "    \"\"\"Scaled dot product attention.\"\"\"\r\n",
        "    def __init__(self, dropout, **kwargs):\r\n",
        "        super(DotProductAttention, self).__init__(**kwargs)\r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "\r\n",
        "    # Shape of `queries`: (`batch_size`, no. of queries, `d`)\r\n",
        "    # Shape of `keys`: (`batch_size`, no. of key-value pairs, `d`)\r\n",
        "    # Shape of `values`: (`batch_size`, no. of key-value pairs, value\r\n",
        "    # dimension)\r\n",
        "    # Shape of `valid_lens`: (`batch_size`,) or (`batch_size`, no. of queries)\r\n",
        "    def forward(self, queries, keys, values, valid_lens=None):\r\n",
        "        d = queries.shape[-1]\r\n",
        "        # Set `transpose_b=True` to swap the last two dimensions of `keys`\r\n",
        "        scores = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(d)\r\n",
        "        self.attention_weights = masked_softmax(scores, valid_lens)\r\n",
        "        return torch.bmm(self.dropout(self.attention_weights), values)\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_attention-mechanisms/bahdanau-attention.md\r\n",
        "class AttentionDecoder(d2l.Decoder):\r\n",
        "    \"\"\"The base attention-based decoder interface.\"\"\"\r\n",
        "    def __init__(self, **kwargs):\r\n",
        "        super(AttentionDecoder, self).__init__(**kwargs)\r\n",
        "\r\n",
        "    @property\r\n",
        "    def attention_weights(self):\r\n",
        "        raise NotImplementedError\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_attention-mechanisms/multihead-attention.md\r\n",
        "class MultiHeadAttention(nn.Module):\r\n",
        "    def __init__(self, key_size, query_size, value_size, num_hiddens,\r\n",
        "                 num_heads, dropout, bias=False, **kwargs):\r\n",
        "        super(MultiHeadAttention, self).__init__(**kwargs)\r\n",
        "        self.num_heads = num_heads\r\n",
        "        self.attention = d2l.DotProductAttention(dropout)\r\n",
        "        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)\r\n",
        "        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)\r\n",
        "        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)\r\n",
        "        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)\r\n",
        "\r\n",
        "    def forward(self, queries, keys, values, valid_lens):\r\n",
        "        # Shape of `queries`, `keys`, or `values`:\r\n",
        "        # (`batch_size`, no. of queries or key-value pairs, `num_hiddens`)\r\n",
        "        # Shape of `valid_lens`:\r\n",
        "        # (`batch_size`,) or (`batch_size`, no. of queries)\r\n",
        "        # After transposing, shape of output `queries`, `keys`, or `values`:\r\n",
        "        # (`batch_size` * `num_heads`, no. of queries or key-value pairs,\r\n",
        "        # `num_hiddens` / `num_heads`)\r\n",
        "        queries = transpose_qkv(self.W_q(queries), self.num_heads)\r\n",
        "        keys = transpose_qkv(self.W_k(keys), self.num_heads)\r\n",
        "        values = transpose_qkv(self.W_v(values), self.num_heads)\r\n",
        "\r\n",
        "        if valid_lens is not None:\r\n",
        "            # On axis 0, copy the first item (scalar or vector) for\r\n",
        "            # `num_heads` times, then copy the next item, and so on\r\n",
        "            valid_lens = torch.repeat_interleave(valid_lens,\r\n",
        "                                                 repeats=self.num_heads,\r\n",
        "                                                 dim=0)\r\n",
        "\r\n",
        "        # Shape of `output`: (`batch_size` * `num_heads`, no. of queries,\r\n",
        "        # `num_hiddens` / `num_heads`)\r\n",
        "        output = self.attention(queries, keys, values, valid_lens)\r\n",
        "\r\n",
        "        # Shape of `output_concat`:\r\n",
        "        # (`batch_size`, no. of queries, `num_hiddens`)\r\n",
        "        output_concat = transpose_output(output, self.num_heads)\r\n",
        "        return self.W_o(output_concat)\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_attention-mechanisms/multihead-attention.md\r\n",
        "def transpose_qkv(X, num_heads):\r\n",
        "    # Shape of input `X`:\r\n",
        "    # (`batch_size`, no. of queries or key-value pairs, `num_hiddens`).\r\n",
        "    # Shape of output `X`:\r\n",
        "    # (`batch_size`, no. of queries or key-value pairs, `num_heads`,\r\n",
        "    # `num_hiddens` / `num_heads`)\r\n",
        "    X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)\r\n",
        "\r\n",
        "    # Shape of output `X`:\r\n",
        "    # (`batch_size`, `num_heads`, no. of queries or key-value pairs,\r\n",
        "    # `num_hiddens` / `num_heads`)\r\n",
        "    X = X.permute(0, 2, 1, 3)\r\n",
        "\r\n",
        "    # Shape of `output`:\r\n",
        "    # (`batch_size` * `num_heads`, no. of queries or key-value pairs,\r\n",
        "    # `num_hiddens` / `num_heads`)\r\n",
        "    return X.reshape(-1, X.shape[2], X.shape[3])\r\n",
        "\r\n",
        "def transpose_output(X, num_heads):\r\n",
        "    \"\"\"Reverse the operation of `transpose_qkv`\"\"\"\r\n",
        "    X = X.reshape(-1, num_heads, X.shape[1], X.shape[2])\r\n",
        "    X = X.permute(0, 2, 1, 3)\r\n",
        "    return X.reshape(X.shape[0], X.shape[1], -1)\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_attention-mechanisms/self-attention-and-positional-encoding.md\r\n",
        "class PositionalEncoding(nn.Module):\r\n",
        "    def __init__(self, num_hiddens, dropout, max_len=1000):\r\n",
        "        super(PositionalEncoding, self).__init__()\r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        # Create a long enough `P`\r\n",
        "        self.P = d2l.zeros((1, max_len, num_hiddens))\r\n",
        "        X = d2l.arange(max_len, dtype=torch.float32).reshape(\r\n",
        "            -1, 1) / torch.pow(\r\n",
        "                10000,\r\n",
        "                torch.arange(0, num_hiddens, 2, dtype=torch.float32) /\r\n",
        "                num_hiddens)\r\n",
        "        self.P[:, :, 0::2] = torch.sin(X)\r\n",
        "        self.P[:, :, 1::2] = torch.cos(X)\r\n",
        "\r\n",
        "    def forward(self, X):\r\n",
        "        X = X + self.P[:, :X.shape[1], :].to(X.device)\r\n",
        "        return self.dropout(X)\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_attention-mechanisms/transformer.md\r\n",
        "class PositionWiseFFN(nn.Module):\r\n",
        "    def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs,\r\n",
        "                 **kwargs):\r\n",
        "        super(PositionWiseFFN, self).__init__(**kwargs)\r\n",
        "        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)\r\n",
        "        self.relu = nn.ReLU()\r\n",
        "        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)\r\n",
        "\r\n",
        "    def forward(self, X):\r\n",
        "        return self.dense2(self.relu(self.dense1(X)))\r\n",
        "\r\n",
        "def semilogy(x_vals, y_vals, x_label, y_label, x2_vals=None, y2_vals=None,\r\n",
        "             legend=None, figsize=(3.5, 2.5)):\r\n",
        "    set_figsize(figsize)\r\n",
        "    plt.xlabel(x_label)\r\n",
        "    plt.ylabel(y_label)\r\n",
        "    plt.semilogy(x_vals, y_vals)\r\n",
        "    if x2_vals and y2_vals:\r\n",
        "        plt.semilogy(x2_vals, y2_vals, linestyle=':')\r\n",
        "        plt.legend(legend)\r\n",
        "    # plt.show()\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_attention-mechanisms/transformer.md\r\n",
        "class AddNorm(nn.Module):\r\n",
        "    def __init__(self, normalized_shape, dropout, **kwargs):\r\n",
        "        super(AddNorm, self).__init__(**kwargs)\r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        self.ln = nn.LayerNorm(normalized_shape)\r\n",
        "\r\n",
        "    def forward(self, X, Y):\r\n",
        "        return self.ln(self.dropout(Y) + X)\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_attention-mechanisms/transformer.md\r\n",
        "class EncoderBlock(nn.Module):\r\n",
        "    def __init__(self, key_size, query_size, value_size, num_hiddens,\r\n",
        "                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,\r\n",
        "                 dropout, use_bias=False, **kwargs):\r\n",
        "        super(EncoderBlock, self).__init__(**kwargs)\r\n",
        "        self.attention = d2l.MultiHeadAttention(key_size, query_size,\r\n",
        "                                                value_size, num_hiddens,\r\n",
        "                                                num_heads, dropout, use_bias)\r\n",
        "        self.addnorm1 = AddNorm(norm_shape, dropout)\r\n",
        "        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens,\r\n",
        "                                   num_hiddens)\r\n",
        "        self.addnorm2 = AddNorm(norm_shape, dropout)\r\n",
        "\r\n",
        "    def forward(self, X, valid_lens):\r\n",
        "        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))\r\n",
        "        return self.addnorm2(Y, self.ffn(Y))\r\n",
        "\r\n",
        "\r\n",
        "# Defined in file: ./chapter_attention-mechanisms/transformer.md\r\n",
        "class TransformerEncoder(d2l.Encoder):\r\n",
        "    def __init__(self, vocab_size, key_size, query_size, value_size,\r\n",
        "                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,\r\n",
        "                 num_heads, num_layers, dropout, use_bias=False, **kwargs):\r\n",
        "        super(TransformerEncoder, self).__init__(**kwargs)\r\n",
        "        self.num_hiddens = num_hiddens\r\n",
        "        self.embedding = nn.Embedding(vocab_size, num_hiddens)\r\n",
        "        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)\r\n",
        "        self.blks = nn.Sequential()\r\n",
        "        for i in range(num_layers):\r\n",
        "            self.blks.add_module(\r\n",
        "                \"block\" + str(i),\r\n",
        "                EncoderBlock(key_size, query_size, value_size, num_hiddens,\r\n",
        "                             norm_shape, ffn_num_input, ffn_num_hiddens,\r\n",
        "                             num_heads, dropout, use_bias))\r\n",
        "\r\n",
        "    def forward(self, X, valid_lens, *args):\r\n",
        "        # Since positional encoding values are between -1 and 1, the embedding\r\n",
        "        # values are multiplied by the square root of the embedding dimension\r\n",
        "        # to rescale before they are summed up\r\n",
        "        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))\r\n",
        "        self.attention_weights = [None] * len(self.blks)\r\n",
        "        for i, blk in enumerate(self.blks):\r\n",
        "            X = blk(X, valid_lens)\r\n",
        "            self.attention_weights[\r\n",
        "                i] = blk.attention.attention.attention_weights\r\n",
        "        return X\r\n",
        "\r\n",
        "\r\n",
        "# Alias defined in config.ini\r\n",
        "\r\n",
        "\r\n",
        "ones = torch.ones\r\n",
        "zeros = torch.zeros\r\n",
        "tensor = torch.tensor\r\n",
        "arange = torch.arange\r\n",
        "meshgrid = torch.meshgrid\r\n",
        "sin = torch.sin\r\n",
        "sinh = torch.sinh\r\n",
        "cos = torch.cos\r\n",
        "cosh = torch.cosh\r\n",
        "tanh = torch.tanh\r\n",
        "linspace = torch.linspace\r\n",
        "exp = torch.exp\r\n",
        "log = torch.log\r\n",
        "normal = torch.normal\r\n",
        "rand = torch.rand\r\n",
        "matmul = torch.matmul\r\n",
        "int32 = torch.int32\r\n",
        "float32 = torch.float32\r\n",
        "concat = torch.cat\r\n",
        "stack = torch.stack\r\n",
        "abs = torch.abs\r\n",
        "eye = torch.eye\r\n",
        "numpy = lambda x, *args, **kwargs: x.detach().numpy(*args, **kwargs)\r\n",
        "size = lambda x, *args, **kwargs: x.numel(*args, **kwargs)\r\n",
        "reshape = lambda x, *args, **kwargs: x.reshape(*args, **kwargs)\r\n",
        "to = lambda x, *args, **kwargs: x.to(*args, **kwargs)\r\n",
        "reduce_sum = lambda x, *args, **kwargs: x.sum(*args, **kwargs)\r\n",
        "argmax = lambda x, *args, **kwargs: x.argmax(*args, **kwargs)\r\n",
        "astype = lambda x, *args, **kwargs: x.type(*args, **kwargs)\r\n",
        "transpose = lambda x, *args, **kwargs: x.t(*args, **kwargs)\r\n",
        "\r\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3vPBvwlbofg"
      },
      "source": [
        "n_train,n_test,num_inputs = 20,100,200\r\n",
        "true_w,true_b = torch.ones(num_inputs,1)*0.01,0.05\r\n",
        "\r\n",
        "features = torch.randn((n_train+n_test,num_inputs))\r\n",
        "labels = torch.matmul(features,true_w)+true_b\r\n",
        "\r\n",
        "labels += torch.tensor(np.random.normal(0,0.01,size=labels.size()),dtype=torch.float)\r\n",
        "train_features,test_features = features[:n_train,:],features[n_train:,:]\r\n",
        "train_labels,test_labels = labels[:n_train],labels[n_train:]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdFc1OhDfZXn"
      },
      "source": [
        "## 3.从零开始实现\r\n",
        "### 3.1 初始化模型参数\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22-8kkVXfgac"
      },
      "source": [
        "def init_params():\r\n",
        "    w = torch.randn((num_inputs,1),requires_grad=True)\r\n",
        "    b = torch.zeros(1,requires_grad=True)\r\n",
        "    return [w,b]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQT4AZqtft_a"
      },
      "source": [
        "### 3.2 定义$L_{2}$范数惩罚项\r\n",
        "这里只惩罚模型的权重参数。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEwoUYoff4U8"
      },
      "source": [
        "def l2_penalty(w):\r\n",
        "    return (w**2).sum()/2"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-b5YV5mEhhXE"
      },
      "source": [
        "### 3.3 定义训练和测试\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GEEA-IAhr6H"
      },
      "source": [
        "batch_size,num_epochs,lr = 1,100,0.003\r\n",
        "net,loss = linreg,squared_loss\r\n",
        "\r\n",
        "dataset = torch.utils.data.TensorDataset(train_features,train_labels)\r\n",
        "train_iter = torch.utils.data.DataLoader(dataset,batch_size,shuffle=True)\r\n",
        "\r\n",
        "def fit_and_plot(lambd):\r\n",
        "    w,b = init_params()\r\n",
        "    train_ls,test_ls = [],[]\r\n",
        "    for _ in range(num_epochs):\r\n",
        "        for X,y in train_iter:\r\n",
        "            # 添加L2范数惩罚项\r\n",
        "            l = loss(net(X,w,b),y)+lambd*l2_penalty(w)\r\n",
        "            l = l.sum()\r\n",
        "\r\n",
        "            if w.grad is not None:\r\n",
        "                w.grad.data.zero_()\r\n",
        "                b.grad.data.zero_()\r\n",
        "            \r\n",
        "            l.backward()\r\n",
        "\r\n",
        "            sgd([w,b],lr,batch_size)\r\n",
        "        train_ls.append(loss(net(train_features, w, b), train_labels).mean().item())\r\n",
        "        test_ls.append(loss(net(test_features,w,b),test_labels).mean().item())\r\n",
        "    semilogy(range(1, num_epochs + 1), train_ls, 'epochs', 'loss',\r\n",
        "                 range(1, num_epochs + 1), test_ls, ['train', 'test'])\r\n",
        "    print('L2 norm of w:', w.norm().item())"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCamIT9sN-LS"
      },
      "source": [
        "### 3.4 观察过拟合\r\n",
        "\r\n",
        "接下来，让我们训练并测试高维线性回归模型。当lambd设为0时，我们没有使用权重衰减。结果训练误差远小于测试集上的误差。这是典型的过拟合现象。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "bPOBdVdXOHOA",
        "outputId": "99eaf111-a954-4ce5-b7ab-5001693d8454"
      },
      "source": [
        "fit_and_plot(lambd=0)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "L2 norm of w: 13.613017082214355\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 252x180 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"180.65625pt\" version=\"1.1\" viewBox=\"0 0 258.944602 180.65625\" width=\"258.944602pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 180.65625 \nL 258.944602 180.65625 \nL 258.944602 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 55.778125 143.1 \nL 251.078125 143.1 \nL 251.078125 7.2 \nL 55.778125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mece93e5a61\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"62.862009\" xlink:href=\"#mece93e5a61\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(59.680759 157.698438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"98.729778\" xlink:href=\"#mece93e5a61\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(92.367278 157.698438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"134.597546\" xlink:href=\"#mece93e5a61\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 40 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(128.235046 157.698438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"170.465315\" xlink:href=\"#mece93e5a61\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 60 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(164.102815 157.698438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"206.333084\" xlink:href=\"#mece93e5a61\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 80 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(199.970584 157.698438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"242.200852\" xlink:href=\"#mece93e5a61\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 100 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(232.657102 157.698438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- epochs -->\n     <defs>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     </defs>\n     <g transform=\"translate(135.595313 171.376563)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\n      <use x=\"304.541016\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mcbe5002df2\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"55.778125\" xlink:href=\"#mcbe5002df2\" y=\"120.543613\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- $\\mathdefault{10^{-10}}$ -->\n      <defs>\n       <path d=\"M 10.59375 35.5 \nL 73.1875 35.5 \nL 73.1875 27.203125 \nL 10.59375 27.203125 \nz\n\" id=\"DejaVuSans-8722\"/>\n      </defs>\n      <g transform=\"translate(20.878125 124.342832)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-49\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-48\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-8722\"/>\n       <use transform=\"translate(186.855469 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-49\"/>\n       <use transform=\"translate(231.391602 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"55.778125\" xlink:href=\"#mcbe5002df2\" y=\"93.63667\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- $\\mathdefault{10^{-7}}$ -->\n      <defs>\n       <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n      </defs>\n      <g transform=\"translate(25.278125 97.435889)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-49\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-48\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-8722\"/>\n       <use transform=\"translate(186.855469 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-55\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"55.778125\" xlink:href=\"#mcbe5002df2\" y=\"66.729727\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- $\\mathdefault{10^{-4}}$ -->\n      <g transform=\"translate(25.278125 70.528945)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-49\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-48\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-8722\"/>\n       <use transform=\"translate(186.855469 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"55.778125\" xlink:href=\"#mcbe5002df2\" y=\"39.822783\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- $\\mathdefault{10^{-1}}$ -->\n      <g transform=\"translate(25.278125 43.622002)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-49\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-48\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-8722\"/>\n       <use transform=\"translate(186.855469 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"55.778125\" xlink:href=\"#mcbe5002df2\" y=\"12.91584\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- $\\mathdefault{10^{2}}$ -->\n      <g transform=\"translate(31.178125 16.715059)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-49\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-48\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_13\">\n     <!-- loss -->\n     <defs>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n     </defs>\n     <g transform=\"translate(14.798438 84.807812)rotate(-90)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_12\">\n    <path clip-path=\"url(#p657f8d7342)\" d=\"M 64.655398 19.957119 \nL 66.448786 25.875039 \nL 68.242175 31.130588 \nL 70.035563 35.782466 \nL 71.828951 40.12843 \nL 73.62234 44.288994 \nL 75.415728 48.430747 \nL 77.209117 52.438618 \nL 79.002505 56.387353 \nL 80.795894 60.334215 \nL 82.589282 64.104264 \nL 84.38267 68.100607 \nL 86.176059 71.98024 \nL 87.969447 75.786459 \nL 89.762836 79.61098 \nL 91.556224 83.29861 \nL 93.349613 86.860348 \nL 95.143001 90.536136 \nL 96.936389 94.130848 \nL 98.729778 97.689883 \nL 100.523166 101.248727 \nL 102.316555 104.791881 \nL 104.109943 108.203004 \nL 105.903332 111.707612 \nL 107.69672 115.290942 \nL 109.490108 118.647376 \nL 111.283497 121.914235 \nL 113.076885 125.319205 \nL 114.870274 127.510936 \nL 116.663662 129.801911 \nL 118.457051 130.903482 \nL 120.250439 131.297775 \nL 122.043827 132.341928 \nL 123.837216 132.957713 \nL 125.630604 132.84369 \nL 127.423993 132.943913 \nL 129.217381 133.132711 \nL 131.01077 133.55061 \nL 132.804158 135.194569 \nL 134.597546 135.324224 \nL 136.390935 135.62107 \nL 138.184323 135.852546 \nL 139.977712 135.471604 \nL 141.7711 135.902624 \nL 143.564489 135.429428 \nL 145.357877 135.751788 \nL 147.151265 136.121337 \nL 148.944654 135.490957 \nL 150.738042 136.078951 \nL 152.531431 135.799527 \nL 154.324819 135.573688 \nL 156.118208 135.624452 \nL 157.911596 135.768754 \nL 159.704985 135.930288 \nL 161.498373 136.267459 \nL 163.291761 136.374272 \nL 165.08515 136.119717 \nL 166.878538 135.907754 \nL 168.671927 135.809457 \nL 170.465315 135.823089 \nL 172.258704 135.518723 \nL 174.052092 135.39714 \nL 175.84548 135.511961 \nL 177.638869 135.924969 \nL 179.432257 135.651285 \nL 181.225646 135.894408 \nL 183.019034 135.852055 \nL 184.812423 135.967492 \nL 186.605811 135.930236 \nL 188.399199 136.020111 \nL 190.192588 136.100452 \nL 191.985976 136.228068 \nL 193.779365 136.26643 \nL 195.572753 135.841078 \nL 197.366142 135.704984 \nL 199.15953 136.002707 \nL 200.952918 135.93636 \nL 202.746307 135.748952 \nL 204.539695 136.091598 \nL 206.333084 136.14531 \nL 208.126472 135.931056 \nL 209.919861 136.096982 \nL 211.713249 135.994898 \nL 213.506637 136.618601 \nL 215.300026 136.922727 \nL 217.093414 136.764727 \nL 218.886803 136.599966 \nL 220.680191 136.574929 \nL 222.47358 136.024951 \nL 224.266968 136.507726 \nL 226.060356 135.608992 \nL 227.853745 135.428062 \nL 229.647133 135.048642 \nL 231.440522 135.922785 \nL 233.23391 135.756751 \nL 235.027299 135.872218 \nL 236.820687 135.632002 \nL 238.614075 135.687908 \nL 240.407464 135.724875 \nL 242.200852 136.25366 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_13\">\n    <path clip-path=\"url(#p657f8d7342)\" d=\"M 64.655398 13.388381 \nL 66.448786 13.394983 \nL 68.242175 13.380842 \nL 70.035563 13.377406 \nL 71.828951 13.378566 \nL 73.62234 13.377273 \nL 75.415728 13.377909 \nL 77.209117 13.37826 \nL 79.002505 13.378322 \nL 80.795894 13.378426 \nL 82.589282 13.378494 \nL 84.38267 13.378473 \nL 86.176059 13.378512 \nL 87.969447 13.378535 \nL 89.762836 13.378526 \nL 91.556224 13.378533 \nL 93.349613 13.378536 \nL 95.143001 13.37854 \nL 96.936389 13.378544 \nL 98.729778 13.378544 \nL 100.523166 13.378546 \nL 102.316555 13.378546 \nL 104.109943 13.378548 \nL 105.903332 13.378548 \nL 107.69672 13.378549 \nL 109.490108 13.378549 \nL 111.283497 13.378549 \nL 113.076885 13.378549 \nL 114.870274 13.378549 \nL 116.663662 13.378549 \nL 118.457051 13.378549 \nL 120.250439 13.378549 \nL 122.043827 13.378549 \nL 123.837216 13.378549 \nL 125.630604 13.378549 \nL 127.423993 13.378549 \nL 129.217381 13.37855 \nL 131.01077 13.378549 \nL 132.804158 13.378549 \nL 134.597546 13.378549 \nL 136.390935 13.378549 \nL 138.184323 13.378549 \nL 139.977712 13.378549 \nL 141.7711 13.378549 \nL 143.564489 13.378549 \nL 145.357877 13.378549 \nL 147.151265 13.378549 \nL 148.944654 13.378549 \nL 150.738042 13.378549 \nL 152.531431 13.378549 \nL 154.324819 13.378549 \nL 156.118208 13.378549 \nL 157.911596 13.378549 \nL 159.704985 13.378549 \nL 161.498373 13.37855 \nL 163.291761 13.37855 \nL 165.08515 13.37855 \nL 166.878538 13.37855 \nL 168.671927 13.37855 \nL 170.465315 13.37855 \nL 172.258704 13.37855 \nL 174.052092 13.37855 \nL 175.84548 13.37855 \nL 177.638869 13.37855 \nL 179.432257 13.37855 \nL 181.225646 13.37855 \nL 183.019034 13.378549 \nL 184.812423 13.378549 \nL 186.605811 13.378549 \nL 188.399199 13.378549 \nL 190.192588 13.378549 \nL 191.985976 13.378549 \nL 193.779365 13.378549 \nL 195.572753 13.378549 \nL 197.366142 13.378549 \nL 199.15953 13.378549 \nL 200.952918 13.378549 \nL 202.746307 13.378549 \nL 204.539695 13.378549 \nL 206.333084 13.37855 \nL 208.126472 13.37855 \nL 209.919861 13.37855 \nL 211.713249 13.37855 \nL 213.506637 13.37855 \nL 215.300026 13.37855 \nL 217.093414 13.37855 \nL 218.886803 13.37855 \nL 220.680191 13.37855 \nL 222.47358 13.37855 \nL 224.266968 13.37855 \nL 226.060356 13.37855 \nL 227.853745 13.37855 \nL 229.647133 13.37855 \nL 231.440522 13.37855 \nL 233.23391 13.37855 \nL 235.027299 13.37855 \nL 236.820687 13.37855 \nL 238.614075 13.37855 \nL 240.407464 13.37855 \nL 242.200852 13.37855 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-dasharray:1.5,2.475;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 55.778125 143.1 \nL 55.778125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 251.078125 143.1 \nL 251.078125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 55.778125 143.1 \nL 251.078125 143.1 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 55.778125 7.2 \nL 251.078125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 188.803125 91.328125 \nL 244.078125 91.328125 \nQ 246.078125 91.328125 246.078125 89.328125 \nL 246.078125 60.971875 \nQ 246.078125 58.971875 244.078125 58.971875 \nL 188.803125 58.971875 \nQ 186.803125 58.971875 186.803125 60.971875 \nL 186.803125 89.328125 \nQ 186.803125 91.328125 188.803125 91.328125 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_14\">\n     <path d=\"M 190.803125 67.070312 \nL 210.803125 67.070312 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_15\"/>\n    <g id=\"text_14\">\n     <!-- train -->\n     <defs>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n     </defs>\n     <g transform=\"translate(218.803125 70.570312)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\n     </g>\n    </g>\n    <g id=\"line2d_16\">\n     <path d=\"M 190.803125 81.748437 \nL 210.803125 81.748437 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-dasharray:1.5,2.475;stroke-dashoffset:0;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_17\"/>\n    <g id=\"text_15\">\n     <!-- test -->\n     <g transform=\"translate(218.803125 85.248437)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"100.732422\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"152.832031\" xlink:href=\"#DejaVuSans-116\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p657f8d7342\">\n   <rect height=\"135.9\" width=\"195.3\" x=\"55.778125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmImr7KlP86e"
      },
      "source": [
        "### 3.5 使用权重衰减\r\n",
        "\r\n",
        "可以看出，训练误差虽然有所提高，但测试集上的误差有所下降。过拟合现象得到一定程度的缓解。另外，权重参数的L2L \r\n",
        "2\r\n",
        "​\t\r\n",
        " 范数比不使用权重衰减时的更小，此时的权重参数更接近0。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "P_loKc-VQDXQ",
        "outputId": "f2579c7a-a955-4831-b5a5-17c519552089"
      },
      "source": [
        "fit_and_plot(lambd=3)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "L2 norm of w: 0.03728552162647247\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 252x180 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"182.20516pt\" version=\"1.1\" viewBox=\"0 0 254.544602 182.20516\" width=\"254.544602pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 182.20516 \nL 254.544602 182.20516 \nL 254.544602 -0 \nL 0 -0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 51.378125 144.64891 \nL 246.678125 144.64891 \nL 246.678125 8.74891 \nL 51.378125 8.74891 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m6a3f5fd8a4\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"58.462009\" xlink:href=\"#m6a3f5fd8a4\" y=\"144.64891\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(55.280759 159.247347)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"94.329778\" xlink:href=\"#m6a3f5fd8a4\" y=\"144.64891\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(87.967278 159.247347)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"130.197546\" xlink:href=\"#m6a3f5fd8a4\" y=\"144.64891\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 40 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(123.835046 159.247347)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"166.065315\" xlink:href=\"#m6a3f5fd8a4\" y=\"144.64891\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 60 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(159.702815 159.247347)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"201.933084\" xlink:href=\"#m6a3f5fd8a4\" y=\"144.64891\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 80 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(195.570584 159.247347)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"237.800852\" xlink:href=\"#m6a3f5fd8a4\" y=\"144.64891\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 100 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(228.257102 159.247347)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- epochs -->\n     <defs>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     </defs>\n     <g transform=\"translate(131.195312 172.925472)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\n      <use x=\"304.541016\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m72b899967e\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#m72b899967e\" y=\"103.493524\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- $\\mathdefault{10^{-2}}$ -->\n      <defs>\n       <path d=\"M 10.59375 35.5 \nL 73.1875 35.5 \nL 73.1875 27.203125 \nL 10.59375 27.203125 \nz\n\" id=\"DejaVuSans-8722\"/>\n      </defs>\n      <g transform=\"translate(20.878125 107.292742)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-49\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-48\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-8722\"/>\n       <use transform=\"translate(186.855469 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#m72b899967e\" y=\"57.246371\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- $\\mathdefault{10^{0}}$ -->\n      <g transform=\"translate(26.778125 61.04559)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-49\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-48\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#m72b899967e\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- $\\mathdefault{10^{2}}$ -->\n      <g transform=\"translate(26.778125 14.798437)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-49\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-48\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_11\">\n     <!-- loss -->\n     <defs>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n     </defs>\n     <g transform=\"translate(14.798437 86.356722)rotate(-90)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_10\">\n    <path clip-path=\"url(#p6127273a95)\" d=\"M 60.255398 33.153 \nL 62.048786 49.537432 \nL 63.842175 64.58285 \nL 65.635563 79.417589 \nL 67.428951 93.829293 \nL 69.22234 107.114627 \nL 71.015728 116.875576 \nL 72.809117 121.594511 \nL 74.602505 122.553071 \nL 76.395894 122.184467 \nL 78.189282 122.612157 \nL 79.98267 122.994414 \nL 81.776059 122.803144 \nL 83.569447 122.58141 \nL 85.362836 122.619809 \nL 87.156224 122.891358 \nL 88.949613 123.580255 \nL 90.743001 123.241142 \nL 92.536389 123.777123 \nL 94.329778 124.139727 \nL 96.123166 124.788169 \nL 97.916555 125.864334 \nL 99.709943 126.306252 \nL 101.503332 125.249888 \nL 103.29672 126.316767 \nL 105.090108 124.638791 \nL 106.883497 124.970681 \nL 108.676885 125.233535 \nL 110.470274 124.670646 \nL 112.263662 125.449392 \nL 114.057051 125.542477 \nL 115.850439 127.063872 \nL 117.643827 126.73432 \nL 119.437216 126.46968 \nL 121.230604 126.710142 \nL 123.023993 126.413593 \nL 124.817381 128.000847 \nL 126.61077 127.989948 \nL 128.404158 127.848244 \nL 130.197546 129.362838 \nL 131.990935 129.223797 \nL 133.784323 128.094886 \nL 135.577712 127.962218 \nL 137.3711 128.248272 \nL 139.164489 129.851887 \nL 140.957877 130.913132 \nL 142.751265 130.486093 \nL 144.544654 129.520619 \nL 146.338042 129.234298 \nL 148.131431 129.799144 \nL 149.924819 128.744672 \nL 151.718208 129.01411 \nL 153.511596 129.422071 \nL 155.304985 130.225486 \nL 157.098373 130.760213 \nL 158.891761 132.301526 \nL 160.68515 131.881157 \nL 162.478538 131.599093 \nL 164.271927 133.238647 \nL 166.065315 132.246674 \nL 167.858704 132.086548 \nL 169.652092 132.067881 \nL 171.44548 131.165872 \nL 173.238869 132.01506 \nL 175.032257 132.51941 \nL 176.825646 133.142425 \nL 178.619034 132.804948 \nL 180.412423 134.312419 \nL 182.205811 132.649696 \nL 183.999199 131.968833 \nL 185.792588 132.115186 \nL 187.585976 132.690638 \nL 189.379365 132.428266 \nL 191.172753 133.475774 \nL 192.966142 133.89157 \nL 194.75953 135.173196 \nL 196.552918 134.258579 \nL 198.346307 134.805695 \nL 200.139695 133.565328 \nL 201.933084 133.792643 \nL 203.726472 135.282753 \nL 205.519861 135.673335 \nL 207.313249 134.554397 \nL 209.106637 135.771896 \nL 210.900026 133.879542 \nL 212.693414 135.351782 \nL 214.486803 136.898336 \nL 216.280191 135.271867 \nL 218.07358 135.429213 \nL 219.866968 135.35962 \nL 221.660356 135.75829 \nL 223.453745 134.455916 \nL 225.247133 136.264655 \nL 227.040522 136.333879 \nL 228.83391 135.463307 \nL 230.627299 137.041971 \nL 232.420687 136.653895 \nL 234.214075 138.471637 \nL 236.007464 136.219501 \nL 237.800852 136.906888 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_11\">\n    <path clip-path=\"url(#p6127273a95)\" d=\"M 60.255398 14.926183 \nL 62.048786 18.468245 \nL 63.842175 22.049835 \nL 65.635563 25.644247 \nL 67.428951 29.234089 \nL 69.22234 32.836126 \nL 71.015728 36.435748 \nL 72.809117 40.034554 \nL 74.602505 43.623337 \nL 76.395894 47.198414 \nL 78.189282 50.751623 \nL 79.98267 54.268566 \nL 81.776059 57.748579 \nL 83.569447 61.173965 \nL 85.362836 64.524807 \nL 87.156224 67.775957 \nL 88.949613 70.886551 \nL 90.743001 73.839929 \nL 92.536389 76.590251 \nL 94.329778 79.100841 \nL 96.123166 81.355961 \nL 97.916555 83.316909 \nL 99.709943 84.974112 \nL 101.503332 86.347711 \nL 103.29672 87.499172 \nL 105.090108 88.46218 \nL 106.883497 89.241783 \nL 108.676885 89.861729 \nL 110.470274 90.400915 \nL 112.263662 90.792914 \nL 114.057051 91.182321 \nL 115.850439 91.478178 \nL 117.643827 91.827234 \nL 119.437216 92.077956 \nL 121.230604 92.312786 \nL 123.023993 92.560809 \nL 124.817381 92.772217 \nL 126.61077 92.99541 \nL 128.404158 93.199359 \nL 130.197546 93.416767 \nL 131.990935 93.601168 \nL 133.784323 93.789497 \nL 135.577712 93.98242 \nL 137.3711 94.18983 \nL 139.164489 94.36712 \nL 140.957877 94.535266 \nL 142.751265 94.719994 \nL 144.544654 94.942929 \nL 146.338042 95.097893 \nL 148.131431 95.289498 \nL 149.924819 95.461617 \nL 151.718208 95.645591 \nL 153.511596 95.763222 \nL 155.304985 95.939861 \nL 157.098373 96.133523 \nL 158.891761 96.282751 \nL 160.68515 96.416897 \nL 162.478538 96.60627 \nL 164.271927 96.752303 \nL 166.065315 96.924514 \nL 167.858704 97.064087 \nL 169.652092 97.26824 \nL 171.44548 97.405507 \nL 173.238869 97.540383 \nL 175.032257 97.693126 \nL 176.825646 97.822161 \nL 178.619034 97.976666 \nL 180.412423 98.110334 \nL 182.205811 98.250269 \nL 183.999199 98.380286 \nL 185.792588 98.525072 \nL 187.585976 98.68894 \nL 189.379365 98.799442 \nL 191.172753 98.903043 \nL 192.966142 99.033487 \nL 194.75953 99.151023 \nL 196.552918 99.257255 \nL 198.346307 99.397642 \nL 200.139695 99.536927 \nL 201.933084 99.638174 \nL 203.726472 99.757625 \nL 205.519861 99.86941 \nL 207.313249 99.974101 \nL 209.106637 100.061931 \nL 210.900026 100.150305 \nL 212.693414 100.231371 \nL 214.486803 100.354166 \nL 216.280191 100.446124 \nL 218.07358 100.562808 \nL 219.866968 100.63812 \nL 221.660356 100.720489 \nL 223.453745 100.805648 \nL 225.247133 100.930437 \nL 227.040522 100.999287 \nL 228.83391 101.104214 \nL 230.627299 101.180671 \nL 232.420687 101.233599 \nL 234.214075 101.325356 \nL 236.007464 101.382677 \nL 237.800852 101.470896 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-dasharray:1.5,2.475;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 51.378125 144.64891 \nL 51.378125 8.74891 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 246.678125 144.64891 \nL 246.678125 8.74891 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 51.378125 144.64891 \nL 246.678125 144.64891 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 51.378125 8.74891 \nL 246.678125 8.74891 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 184.403125 46.10516 \nL 239.678125 46.10516 \nQ 241.678125 46.10516 241.678125 44.10516 \nL 241.678125 15.74891 \nQ 241.678125 13.74891 239.678125 13.74891 \nL 184.403125 13.74891 \nQ 182.403125 13.74891 182.403125 15.74891 \nL 182.403125 44.10516 \nQ 182.403125 46.10516 184.403125 46.10516 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_12\">\n     <path d=\"M 186.403125 21.847347 \nL 206.403125 21.847347 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_13\"/>\n    <g id=\"text_12\">\n     <!-- train -->\n     <defs>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n     </defs>\n     <g transform=\"translate(214.403125 25.347347)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\n     </g>\n    </g>\n    <g id=\"line2d_14\">\n     <path d=\"M 186.403125 36.525472 \nL 206.403125 36.525472 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-dasharray:1.5,2.475;stroke-dashoffset:0;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_15\"/>\n    <g id=\"text_13\">\n     <!-- test -->\n     <g transform=\"translate(214.403125 40.025472)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"100.732422\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"152.832031\" xlink:href=\"#DejaVuSans-116\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p6127273a95\">\n   <rect height=\"135.9\" width=\"195.3\" x=\"51.378125\" y=\"8.74891\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YftW2UpLQKU-"
      },
      "source": [
        "## 4.简洁实现\r\n",
        "\r\n",
        "这里我们直接在构造优化器实例时通过weight_decay参数来指定权重衰减超参数。默认下，PyTorch会对权重和偏差同时衰减。我们可以分别对权重和偏差构造优化器实例，从而只对权重衰减。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbH8KzK4QT3T"
      },
      "source": [
        "def fit_and_plot_pytorch(wd):\r\n",
        "    # 对于权重参数衰减，权重名称一般是weight结尾\r\n",
        "    net = nn.Linear(num_inputs,1)\r\n",
        "    nn.init.normal_(net.weight,mean=0,std=1)\r\n",
        "    nn.init.normal_(net.bias,mean=0,std=1)\r\n",
        "    optimizer_w = torch.optim.SGD(params=[net.weight],lr=lr,weight_decay=wd)# 对权重参数衰减\r\n",
        "    optimizer_b = torch.optim.SGD(params=[net.bias],lr=lr) # 不对偏差参数衰减\r\n",
        "\r\n",
        "    train_ls,test_ls = [],[]\r\n",
        "    for _ in range(num_epochs):\r\n",
        "        for X,y in train_iter:\r\n",
        "            l = loss(net(X),y).mean()\r\n",
        "            optimizer_w.zero_grad()\r\n",
        "            optimizer_b.zero_grad()\r\n",
        "\r\n",
        "            l.backward()\r\n",
        "\r\n",
        "            # 对于两个optimizer实例分别调用step函数，从而分别更新权重和偏差\r\n",
        "            optimizer_w.step()\r\n",
        "            optimizer_b.step()\r\n",
        "        train_ls.append(loss(net(train_features),train_labels).mean().item())\r\n",
        "        test_ls.append(loss(net(test_features),test_labels).mean().item())\r\n",
        "    semilogy(range(1, num_epochs + 1), train_ls, 'epochs', 'loss',\r\n",
        "                 range(1, num_epochs + 1), test_ls, ['train', 'test'])\r\n",
        "    print('L2 norm of w:', net.weight.data.norm().item())\r\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "t1c9affkTnQ1",
        "outputId": "36e73124-3c8c-4ed8-c534-85d92a2befe0"
      },
      "source": [
        "fit_and_plot_pytorch(3)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "L2 norm of w: 0.034775085747241974\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 252x180 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"181.876566pt\" version=\"1.1\" viewBox=\"0 0 254.544602 181.876566\" width=\"254.544602pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 181.876566 \nL 254.544602 181.876566 \nL 254.544602 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 51.378125 144.320316 \nL 246.678125 144.320316 \nL 246.678125 8.420316 \nL 51.378125 8.420316 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m52622bedf7\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"58.462009\" xlink:href=\"#m52622bedf7\" y=\"144.320316\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(55.280759 158.918754)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"94.329778\" xlink:href=\"#m52622bedf7\" y=\"144.320316\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(87.967278 158.918754)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"130.197546\" xlink:href=\"#m52622bedf7\" y=\"144.320316\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 40 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(123.835046 158.918754)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"166.065315\" xlink:href=\"#m52622bedf7\" y=\"144.320316\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 60 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(159.702815 158.918754)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"201.933084\" xlink:href=\"#m52622bedf7\" y=\"144.320316\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 80 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(195.570584 158.918754)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"237.800852\" xlink:href=\"#m52622bedf7\" y=\"144.320316\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 100 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(228.257102 158.918754)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- epochs -->\n     <defs>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     </defs>\n     <g transform=\"translate(131.195312 172.596879)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\n      <use x=\"304.541016\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m6accdc81d9\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#m6accdc81d9\" y=\"101.304905\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- $\\mathdefault{10^{-2}}$ -->\n      <defs>\n       <path d=\"M 10.59375 35.5 \nL 73.1875 35.5 \nL 73.1875 27.203125 \nL 10.59375 27.203125 \nz\n\" id=\"DejaVuSans-8722\"/>\n      </defs>\n      <g transform=\"translate(20.878125 105.104124)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-49\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-48\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-8722\"/>\n       <use transform=\"translate(186.855469 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#m6accdc81d9\" y=\"56.152062\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- $\\mathdefault{10^{0}}$ -->\n      <g transform=\"translate(26.778125 59.951281)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-49\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-48\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#m6accdc81d9\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- $\\mathdefault{10^{2}}$ -->\n      <g transform=\"translate(26.778125 14.798437)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-49\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-48\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_11\">\n     <!-- loss -->\n     <defs>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n     </defs>\n     <g transform=\"translate(14.798437 86.028129)rotate(-90)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_10\">\n    <path clip-path=\"url(#p6eff73f4f9)\" d=\"M 60.255398 31.98069 \nL 62.048786 52.288492 \nL 63.842175 70.234319 \nL 65.635563 86.353021 \nL 67.428951 100.363438 \nL 69.22234 110.957489 \nL 71.015728 118.863333 \nL 72.809117 121.157405 \nL 74.602505 123.266248 \nL 76.395894 123.490968 \nL 78.189282 123.336283 \nL 79.98267 124.843008 \nL 81.776059 125.502425 \nL 83.569447 125.016749 \nL 85.362836 124.488316 \nL 87.156224 124.9094 \nL 88.949613 125.449343 \nL 90.743001 125.631754 \nL 92.536389 125.598867 \nL 94.329778 127.145135 \nL 96.123166 126.603143 \nL 97.916555 127.568304 \nL 99.709943 126.672498 \nL 101.503332 128.072751 \nL 103.29672 127.781169 \nL 105.090108 128.093813 \nL 106.883497 126.176183 \nL 108.676885 127.336155 \nL 110.470274 128.282194 \nL 112.263662 127.233832 \nL 114.057051 128.681563 \nL 115.850439 129.936931 \nL 117.643827 127.74187 \nL 119.437216 128.29454 \nL 121.230604 127.295459 \nL 123.023993 127.613461 \nL 124.817381 127.411075 \nL 126.61077 128.502128 \nL 128.404158 128.868295 \nL 130.197546 130.533175 \nL 131.990935 129.538253 \nL 133.784323 132.199151 \nL 135.577712 130.278932 \nL 137.3711 131.133658 \nL 139.164489 131.913634 \nL 140.957877 129.58629 \nL 142.751265 130.035211 \nL 144.544654 131.204755 \nL 146.338042 131.83243 \nL 148.131431 133.020397 \nL 149.924819 132.999403 \nL 151.718208 131.39238 \nL 153.511596 130.629625 \nL 155.304985 133.591339 \nL 157.098373 133.716074 \nL 158.891761 132.533857 \nL 160.68515 132.902775 \nL 162.478538 132.487254 \nL 164.271927 133.02366 \nL 166.065315 131.791233 \nL 167.858704 131.894848 \nL 169.652092 133.661291 \nL 171.44548 131.729234 \nL 173.238869 132.260235 \nL 175.032257 133.021271 \nL 176.825646 134.374829 \nL 178.619034 135.519566 \nL 180.412423 136.016778 \nL 182.205811 135.667812 \nL 183.999199 133.150377 \nL 185.792588 134.004664 \nL 187.585976 134.107743 \nL 189.379365 134.369688 \nL 191.172753 134.559919 \nL 192.966142 133.709682 \nL 194.75953 134.970126 \nL 196.552918 134.286126 \nL 198.346307 134.367856 \nL 200.139695 132.825834 \nL 201.933084 136.335626 \nL 203.726472 134.083233 \nL 205.519861 135.640845 \nL 207.313249 137.181923 \nL 209.106637 136.532412 \nL 210.900026 134.020685 \nL 212.693414 134.884907 \nL 214.486803 138.143043 \nL 216.280191 135.357027 \nL 218.07358 136.605453 \nL 219.866968 135.808687 \nL 221.660356 132.774516 \nL 223.453745 135.190969 \nL 225.247133 135.387338 \nL 227.040522 135.251972 \nL 228.83391 135.753919 \nL 230.627299 134.798941 \nL 232.420687 137.502622 \nL 234.214075 137.10631 \nL 236.007464 136.913015 \nL 237.800852 136.969568 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_11\">\n    <path clip-path=\"url(#p6eff73f4f9)\" d=\"M 60.255398 14.597589 \nL 62.048786 18.435784 \nL 63.842175 22.062165 \nL 65.635563 25.646286 \nL 67.428951 29.202223 \nL 69.22234 32.751902 \nL 71.015728 36.29201 \nL 72.809117 39.829488 \nL 74.602505 43.355475 \nL 76.395894 46.87357 \nL 78.189282 50.376109 \nL 79.98267 53.859743 \nL 81.776059 57.307001 \nL 83.569447 60.717165 \nL 85.362836 64.045924 \nL 87.156224 67.316402 \nL 88.949613 70.464401 \nL 90.743001 73.456678 \nL 92.536389 76.310719 \nL 94.329778 78.897575 \nL 96.123166 81.189112 \nL 97.916555 83.204407 \nL 99.709943 84.979922 \nL 101.503332 86.374965 \nL 103.29672 87.49284 \nL 105.090108 88.457361 \nL 106.883497 89.163513 \nL 108.676885 89.737054 \nL 110.470274 90.209515 \nL 112.263662 90.552819 \nL 114.057051 90.864513 \nL 115.850439 91.123819 \nL 117.643827 91.30344 \nL 119.437216 91.516582 \nL 121.230604 91.673055 \nL 123.023993 91.845623 \nL 124.817381 91.988134 \nL 126.61077 92.163224 \nL 128.404158 92.336979 \nL 130.197546 92.461581 \nL 131.990935 92.627904 \nL 133.784323 92.787169 \nL 135.577712 92.892861 \nL 137.3711 93.036411 \nL 139.164489 93.198955 \nL 140.957877 93.334889 \nL 142.751265 93.435681 \nL 144.544654 93.584174 \nL 146.338042 93.705957 \nL 148.131431 93.836579 \nL 149.924819 93.99021 \nL 151.718208 94.102887 \nL 153.511596 94.208847 \nL 155.304985 94.370773 \nL 157.098373 94.47196 \nL 158.891761 94.595792 \nL 160.68515 94.702034 \nL 162.478538 94.843503 \nL 164.271927 94.939361 \nL 166.065315 95.049355 \nL 167.858704 95.166282 \nL 169.652092 95.289463 \nL 171.44548 95.395668 \nL 173.238869 95.52805 \nL 175.032257 95.651802 \nL 176.825646 95.755467 \nL 178.619034 95.866525 \nL 180.412423 95.957028 \nL 182.205811 96.073736 \nL 183.999199 96.146381 \nL 185.792588 96.260652 \nL 187.585976 96.326387 \nL 189.379365 96.40155 \nL 191.172753 96.500445 \nL 192.966142 96.632025 \nL 194.75953 96.738093 \nL 196.552918 96.826319 \nL 198.346307 96.898783 \nL 200.139695 96.974493 \nL 201.933084 97.069224 \nL 203.726472 97.15277 \nL 205.519861 97.230071 \nL 207.313249 97.342879 \nL 209.106637 97.409076 \nL 210.900026 97.471667 \nL 212.693414 97.560914 \nL 214.486803 97.662275 \nL 216.280191 97.710857 \nL 218.07358 97.787258 \nL 219.866968 97.864632 \nL 221.660356 97.893742 \nL 223.453745 97.99003 \nL 225.247133 98.059658 \nL 227.040522 98.101638 \nL 228.83391 98.176046 \nL 230.627299 98.230482 \nL 232.420687 98.345473 \nL 234.214075 98.373976 \nL 236.007464 98.466253 \nL 237.800852 98.517918 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-dasharray:1.5,2.475;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 51.378125 144.320316 \nL 51.378125 8.420316 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 246.678125 144.320316 \nL 246.678125 8.420316 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 51.378125 144.320316 \nL 246.678125 144.320316 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 51.378125 8.420316 \nL 246.678125 8.420316 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 184.403125 45.776566 \nL 239.678125 45.776566 \nQ 241.678125 45.776566 241.678125 43.776566 \nL 241.678125 15.420316 \nQ 241.678125 13.420316 239.678125 13.420316 \nL 184.403125 13.420316 \nQ 182.403125 13.420316 182.403125 15.420316 \nL 182.403125 43.776566 \nQ 182.403125 45.776566 184.403125 45.776566 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_12\">\n     <path d=\"M 186.403125 21.518754 \nL 206.403125 21.518754 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_13\"/>\n    <g id=\"text_12\">\n     <!-- train -->\n     <defs>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n     </defs>\n     <g transform=\"translate(214.403125 25.018754)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\n     </g>\n    </g>\n    <g id=\"line2d_14\">\n     <path d=\"M 186.403125 36.196879 \nL 206.403125 36.196879 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-dasharray:1.5,2.475;stroke-dashoffset:0;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_15\"/>\n    <g id=\"text_13\">\n     <!-- test -->\n     <g transform=\"translate(214.403125 39.696879)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"100.732422\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"152.832031\" xlink:href=\"#DejaVuSans-116\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p6eff73f4f9\">\n   <rect height=\"135.9\" width=\"195.3\" x=\"51.378125\" y=\"8.420316\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKGgdFe8T4ET"
      },
      "source": [
        "正则化通过为模型损失函数添加惩罚项使学出的模型参数值较小，是应对过拟合的常用手段。\r\n",
        "\r\n",
        "权重衰减等价于L2​范数正则化，通常会使学到的权重参数的元素较接近0。\r\n",
        "\r\n",
        "权重衰减可以通过优化器中的weight_decay超参数来指定。\r\n",
        "\r\n",
        "可以定义多个优化器实例对不同的模型参数使用不同的迭代方法。"
      ]
    }
  ]
}
