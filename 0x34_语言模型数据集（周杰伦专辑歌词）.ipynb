{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0x34_语言模型数据集（周杰伦专辑歌词）.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1DqsXCzbMyocyLG73RNeYTPHEkRFRz2ge",
      "authorship_tag": "ABX9TyPYMwzgQKXFN1q4BP43gvvI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Matrixmax/Dive_into_DeepLearning/blob/main/0x34_%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%88%E5%91%A8%E6%9D%B0%E4%BC%A6%E4%B8%93%E8%BE%91%E6%AD%8C%E8%AF%8D%EF%BC%89.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgo7XvNQbKpA"
      },
      "source": [
        "本节将介绍如何预处理一个语言模型数据集，并将其转换成字符级循环神经网络所需要的输入格式。为此，我们收集了周杰伦从第一张专辑《Jay》到第十张专辑《跨时代》中的歌词，并在后面几节里应用循环神经网络来训练一个语言模型。当模型训练好后，我们就可以用这个模型来创作歌词。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oN4pguDWbNtf"
      },
      "source": [
        "## 1.读取数据集"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOKuUXjichmg",
        "outputId": "f4db2a4c-ca80-437f-d0df-4c078a8aeb0e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "80ZZN5kubEHt",
        "outputId": "4000ff12-8e45-46f5-a7f1-ed783b49f3ea"
      },
      "source": [
        "import torch\r\n",
        "import random\r\n",
        "import zipfile\r\n",
        "\r\n",
        "with zipfile.ZipFile(\"/content/drive/MyDrive/Colab Notebooks/Dive_Into_Deep_Learning/data/jaychou_lyrics.txt.zip\") as zin:\r\n",
        "    with zin.open('jaychou_lyrics.txt') as f:\r\n",
        "        corpus_chars = f.read().decode('utf-8')\r\n",
        "\r\n",
        "corpus_chars[:40]# 看一下前40个字符"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'想要有直升机\\n想要和你飞到宇宙去\\n想要和你融化在一起\\n融化在宇宙里\\n我每天每天每'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Xy-b5y9dCix"
      },
      "source": [
        "# 这个数据集有6万多个字符。\r\n",
        "# 为了打印方便，我们把换行符替换成空格，然后仅使用前1万个字符来训练模型。\r\n",
        "\r\n",
        "corpus_chars = corpus_chars.replace('\\n',' ').replace('\\r',' ')\r\n",
        "corpus_chars = corpus_chars[0:10000]"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A43Vy0yodTVh"
      },
      "source": [
        "## 2.建立字符索引\r\n",
        "我们将每个字符映射成一个从0开始的连续整数，又称索引，来方便之后的数据处理。为了得到索引，我们将数据集里所有不同字符取出来，然后将其逐一映射到索引来构造词典。接着，打印vocab_size，即词典中不同字符的个数，又称词典大小。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2uDsjg8dcHk",
        "outputId": "724cfcd9-5730-48b5-d0d3-8ac79345a8b6"
      },
      "source": [
        "idx_to_char = list(set(corpus_chars))\r\n",
        "char_to_idx = dict([(char,i) for i ,char in enumerate(idx_to_char)])\r\n",
        "vocab_size = len(char_to_idx)\r\n",
        "vocab_size # 1027"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1027"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIWJKgPcfL05",
        "outputId": "5a5d9b8d-1f78-4eb4-d9b6-7a21f32ce4ad"
      },
      "source": [
        "# 之后将训练数据集中每个字符转化为索引，并且打印前20个字符对应的索引\r\n",
        "corpus_indices = [char_to_idx[char] for char in corpus_chars]\r\n",
        "sample = corpus_indices[:20]\r\n",
        "print('chars:', ''.join([idx_to_char[idx] for idx in sample]))\r\n",
        "print('indices:', sample)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "chars: 想要有直升机 想要和你飞到宇宙去 想要和\n",
            "indices: [905, 465, 249, 440, 611, 198, 226, 905, 465, 876, 284, 269, 646, 678, 421, 612, 226, 905, 465, 876]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhzNvHiChTFo"
      },
      "source": [
        "## 3.时序数据的采样\r\n",
        "在训练中，我们需要每次随机读取小批量样本和标签，与之前章节的实验数据不同的是，时序数据的一个样本通常包含连续的字符。假设时间步数为5，样本序列为5个字符，即“想”“要”“有”“直”“升”。该样本的标签序列为这些字符分别在训练集中的下一个字符，即“要”“有”“直”“升”“机”。我们有两种方式对时序数据进行采样，分别是随机采样和相邻采样。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fehgCVuiAN_"
      },
      "source": [
        "### 3.1 随机采样\r\n",
        "下面的代码每次从数据里随机采样一个小批量。其中批量大小batch_size指每个小批量的样本数，num_steps为每个样本所包含的时间步数。 在随机采样中，每个样本是原始序列上任意截取的一段序列。相邻的两个随机小批量在原始序列上的位置不一定相毗邻。因此，我们无法用一个小批量最终时间步的隐藏状态来初始化下一个小批量的隐藏状态。在训练模型时，每次随机采样前都需要重新初始化隐藏状态。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VF3_j-anhumN"
      },
      "source": [
        "def data_iter_random(corpus_indices,batch_size,num_steps,device=None):\r\n",
        "    # 减1是因为输出的索引X是相应输入的索引y加1\r\n",
        "    num_examples = (len(corpus_indices) -1) //num_steps\r\n",
        "    epoch_size = num_examples //batch_size\r\n",
        "    example_indices = list(range(num_examples))\r\n",
        "    random.shuffle(example_indices)\r\n",
        "    \r\n",
        "    # 返回从pos开始的长为num_steps的序列(返回一个example)\r\n",
        "    def _data(pos):\r\n",
        "        return corpus_indices[pos:pos+num_steps]\r\n",
        "    if device is None:\r\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "    \r\n",
        "    for i in range(epoch_size):\r\n",
        "        # 每次读取batch_size个随机样本\r\n",
        "        batch_indices = example_indices[i:i+batch_size] # 从i开始要读取的batch_size个样本的序号\r\n",
        "        # 根据序号，读取对应序号的样本中的数据\r\n",
        "        X = [_data(j*num_steps) for j in batch_indices] # j*num_steps是在原序列中找到对应样本的位置（原序列并没有对应shuffle）\r\n",
        "        Y = [_data(j*num_steps+1) for j in batch_indices]\r\n",
        "        yield torch.tensor(X,dtype=torch.float32,device=device),torch.tensor(Y,dtype=torch.float32,device=device)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjEqFo3ZlJOp",
        "outputId": "333cb39a-dc52-471d-9589-17cce0cd9b54"
      },
      "source": [
        "my_seq = list(range(30))\r\n",
        "for X,Y in data_iter_random(my_seq,batch_size=2,num_steps=6):\r\n",
        "    print('X: ', X, '\\nY:', Y, '\\n')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X:  tensor([[ 0.,  1.,  2.,  3.,  4.,  5.],\n",
            "        [12., 13., 14., 15., 16., 17.]]) \n",
            "Y: tensor([[ 1.,  2.,  3.,  4.,  5.,  6.],\n",
            "        [13., 14., 15., 16., 17., 18.]]) \n",
            "\n",
            "X:  tensor([[12., 13., 14., 15., 16., 17.],\n",
            "        [18., 19., 20., 21., 22., 23.]]) \n",
            "Y: tensor([[13., 14., 15., 16., 17., 18.],\n",
            "        [19., 20., 21., 22., 23., 24.]]) \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkofYBJGtgHu"
      },
      "source": [
        "### 3.2.相邻采样\r\n",
        "除对原始序列做随机采样之外，我们还可以令相邻的两个随机小批量在原始序列上的位置相毗邻。这时候，我们就可以用一个小批量最终时间步的隐藏状态来初始化下一个小批量的隐藏状态，从而使下一个小批量的输出也取决于当前小批量的输入，并如此循环下去。这对实现循环神经网络造成了两方面影响：一方面， 在训练模型时，我们只需在每一个迭代周期开始时初始化隐藏状态；另一方面，当多个相邻小批量通过传递隐藏状态串联起来时，模型参数的梯度计算将依赖所有串联起来的小批量序列。同一迭代周期中，随着迭代次数的增加，梯度的计算开销会越来越大。 为了使模型参数的梯度计算只依赖一次迭代读取的小批量序列，我们可以在每次读取小批量前将隐藏状态从计算图中分离出来。我们将在下一节（循环神经网络的从零开始实现）的实现中了解这种处理方式。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wm_zmjZtlY5"
      },
      "source": [
        "def data_iter_consecutive(corpus_indices,batch_size,num_steps,device=None):\r\n",
        "    # 不分样本，直接把数据切分成batch\r\n",
        "    if device is None:\r\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "    corpus_indices = torch.tensor(corpus_indices, dtype=torch.float32, device=device)\r\n",
        "    data_len = len(corpus_indices)\r\n",
        "    batch_len = data_len // batch_size\r\n",
        "    indices = corpus_indices[0:batch_size*batch_len].view(batch_size,batch_len)\r\n",
        "    epoch_size = (batch_len-1)//num_steps\r\n",
        "    for i in range(epoch_size):\r\n",
        "        i = i*num_steps\r\n",
        "        X = indices[:,i:i+num_steps]\r\n",
        "        Y = indices[:, i + 1: i + num_steps + 1]\r\n",
        "        yield X, Y"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZWc6dNovL5J",
        "outputId": "5dbd85e4-89ef-4ae7-c08b-dc49955be46d"
      },
      "source": [
        "# 同样的设置下，打印相邻采样每次读取的小批量样本的输入X和标签Y。相邻的两个随机小批量在原始序列上的位置相毗邻。\r\n",
        "for X, Y in data_iter_consecutive(my_seq, batch_size=2, num_steps=6):\r\n",
        "    print('X: ', X, '\\nY:', Y, '\\n')\r\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X:  tensor([[ 0.,  1.,  2.,  3.,  4.,  5.],\n",
            "        [15., 16., 17., 18., 19., 20.]]) \n",
            "Y: tensor([[ 1.,  2.,  3.,  4.,  5.,  6.],\n",
            "        [16., 17., 18., 19., 20., 21.]]) \n",
            "\n",
            "X:  tensor([[ 6.,  7.,  8.,  9., 10., 11.],\n",
            "        [21., 22., 23., 24., 25., 26.]]) \n",
            "Y: tensor([[ 7.,  8.,  9., 10., 11., 12.],\n",
            "        [22., 23., 24., 25., 26., 27.]]) \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}