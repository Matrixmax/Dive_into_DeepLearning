{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0x23_多输入通道和多输出通道.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMWJNANr+T4LeIasqBw1UoR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Matrixmax/Dive_into_DeepLearning/blob/main/0x23_%E5%A4%9A%E8%BE%93%E5%85%A5%E9%80%9A%E9%81%93%E5%92%8C%E5%A4%9A%E8%BE%93%E5%87%BA%E9%80%9A%E9%81%93.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFh2huKWmqHD"
      },
      "source": [
        "前面两节里我们用到的输入和输出都是二维数组，但真实数据的维度经常更高，如，彩色图像在高和宽2个维度外还有RGB（红、绿、蓝）3个颜色通道。假设彩色图像的高和宽分别是hh和ww（像素），那么它可以表示为一个3×h×w3×h×w的多维数组。**我们将大小为3的这一维称为通道（channel）维。**本节我们将介绍含多个输入通道或多个输出通道的卷积核。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MaURoZFl3R2"
      },
      "source": [
        "## 1. 多输入通道\r\n",
        "当输入数据包含多个通道时，我们需要构造一个输入通道数与输入数据的通道数相同的卷积核，从而能够与含多通道的输入数据做互相关运算。\r\n",
        "\r\n",
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWApvP4Kmmrf"
      },
      "source": [
        "import collections\r\n",
        "import math\r\n",
        "import os\r\n",
        "import random\r\n",
        "import sys\r\n",
        "import tarfile\r\n",
        "import time\r\n",
        "import json\r\n",
        "import zipfile\r\n",
        "from tqdm import tqdm\r\n",
        "from PIL import Image\r\n",
        "from collections import namedtuple\r\n",
        "\r\n",
        "from IPython import display\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "import torch\r\n",
        "from torch import nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torchvision\r\n",
        "import torchvision.transforms as transforms\r\n",
        "import torchtext\r\n",
        "import torchtext.vocab as Vocab\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "\r\n",
        "VOC_CLASSES = ['background', 'aeroplane', 'bicycle', 'bird', 'boat',\r\n",
        "               'bottle', 'bus', 'car', 'cat', 'chair', 'cow',\r\n",
        "               'diningtable', 'dog', 'horse', 'motorbike', 'person',\r\n",
        "               'potted plant', 'sheep', 'sofa', 'train', 'tv/monitor']\r\n",
        "\r\n",
        "\r\n",
        "VOC_COLORMAP = [[0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0],\r\n",
        "                [0, 0, 128], [128, 0, 128], [0, 128, 128], [128, 128, 128],\r\n",
        "                [64, 0, 0], [192, 0, 0], [64, 128, 0], [192, 128, 0],\r\n",
        "                [64, 0, 128], [192, 0, 128], [64, 128, 128], [192, 128, 128],\r\n",
        "                [0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0],\r\n",
        "                [0, 64, 128]]\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ###################### 3.2 ############################\r\n",
        "def set_figsize(figsize=(3.5, 2.5)):\r\n",
        "    use_svg_display()\r\n",
        "    # 设置图的尺寸\r\n",
        "    plt.rcParams['figure.figsize'] = figsize\r\n",
        "\r\n",
        "def use_svg_display():\r\n",
        "    \"\"\"Use svg format to display plot in jupyter\"\"\"\r\n",
        "    display.set_matplotlib_formats('svg')\r\n",
        "\r\n",
        "def data_iter(batch_size, features, labels):\r\n",
        "    num_examples = len(features)\r\n",
        "    indices = list(range(num_examples))\r\n",
        "    random.shuffle(indices)  # 样本的读取顺序是随机的\r\n",
        "    for i in range(0, num_examples, batch_size):\r\n",
        "        j = torch.LongTensor(indices[i: min(i + batch_size, num_examples)]) # 最后一次可能不足一个batch\r\n",
        "        yield  features.index_select(0, j), labels.index_select(0, j) \r\n",
        "\r\n",
        "def linreg(X, w, b):\r\n",
        "    return torch.mm(X, w) + b\r\n",
        "\r\n",
        "def squared_loss(y_hat, y): \r\n",
        "    # 注意这里返回的是向量, 另外, pytorch里的MSELoss并没有除以 2\r\n",
        "    return ((y_hat - y.view(y_hat.size())) ** 2) / 2\r\n",
        "\r\n",
        "def sgd(params, lr, batch_size):\r\n",
        "    # 为了和原书保持一致，这里除以了batch_size，但是应该是不用除的，因为一般用PyTorch计算loss时就默认已经\r\n",
        "    # 沿batch维求了平均了。\r\n",
        "    for param in params:\r\n",
        "        param.data -= lr * param.grad / batch_size # 注意这里更改param时用的param.data\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ######################3##### 3.5 #############################\r\n",
        "def get_fashion_mnist_labels(labels):\r\n",
        "    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\r\n",
        "                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\r\n",
        "    return [text_labels[int(i)] for i in labels]\r\n",
        "\r\n",
        "def show_fashion_mnist(images, labels):\r\n",
        "    use_svg_display()\r\n",
        "    # 这里的_表示我们忽略（不使用）的变量\r\n",
        "    _, figs = plt.subplots(1, len(images), figsize=(12, 12))\r\n",
        "    for f, img, lbl in zip(figs, images, labels):\r\n",
        "        f.imshow(img.view((28, 28)).numpy())\r\n",
        "        f.set_title(lbl)\r\n",
        "        f.axes.get_xaxis().set_visible(False)\r\n",
        "        f.axes.get_yaxis().set_visible(False)\r\n",
        "    # plt.show()\r\n",
        "\r\n",
        "# 5.6 修改\r\n",
        "# def load_data_fashion_mnist(batch_size, root='~/Datasets/FashionMNIST'):\r\n",
        "#     \"\"\"Download the fashion mnist dataset and then load into memory.\"\"\"\r\n",
        "#     transform = transforms.ToTensor()\r\n",
        "#     mnist_train = torchvision.datasets.FashionMNIST(root=root, train=True, download=True, transform=transform)\r\n",
        "#     mnist_test = torchvision.datasets.FashionMNIST(root=root, train=False, download=True, transform=transform)\r\n",
        "#     if sys.platform.startswith('win'):\r\n",
        "#         num_workers = 0  # 0表示不用额外的进程来加速读取数据\r\n",
        "#     else:\r\n",
        "#         num_workers = 4\r\n",
        "#     train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\r\n",
        "#     test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\r\n",
        "\r\n",
        "#     return train_iter, test_iter\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ########################### 3.6  ###############################\r\n",
        "# (3.13节修改)\r\n",
        "# def evaluate_accuracy(data_iter, net):\r\n",
        "#     acc_sum, n = 0.0, 0\r\n",
        "#     for X, y in data_iter:\r\n",
        "#         acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()\r\n",
        "#         n += y.shape[0]\r\n",
        "#     return acc_sum / n\r\n",
        "\r\n",
        "\r\n",
        "def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,\r\n",
        "              params=None, lr=None, optimizer=None):\r\n",
        "    for epoch in range(num_epochs):\r\n",
        "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\r\n",
        "        for X, y in train_iter:\r\n",
        "            y_hat = net(X)\r\n",
        "            l = loss(y_hat, y).sum()\r\n",
        "            \r\n",
        "            # 梯度清零\r\n",
        "            if optimizer is not None:\r\n",
        "                optimizer.zero_grad()\r\n",
        "            elif params is not None and params[0].grad is not None:\r\n",
        "                for param in params:\r\n",
        "                    param.grad.data.zero_()\r\n",
        "            \r\n",
        "            l.backward()\r\n",
        "            if optimizer is None:\r\n",
        "                sgd(params, lr, batch_size)\r\n",
        "            else:\r\n",
        "                optimizer.step()  # “softmax回归的简洁实现”一节将用到\r\n",
        "            \r\n",
        "            \r\n",
        "            train_l_sum += l.item()\r\n",
        "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\r\n",
        "            n += y.shape[0]\r\n",
        "        test_acc = evaluate_accuracy(test_iter, net)\r\n",
        "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\r\n",
        "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ########################### 3.7 #####################################3\r\n",
        "class FlattenLayer(torch.nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(FlattenLayer, self).__init__()\r\n",
        "    def forward(self, x): # x shape: (batch, *, *, ...)\r\n",
        "        return x.view(x.shape[0], -1)\r\n",
        "\r\n",
        "\r\n",
        "# ########################### 3.11 ###############################\r\n",
        "def semilogy(x_vals, y_vals, x_label, y_label, x2_vals=None, y2_vals=None,\r\n",
        "             legend=None, figsize=(3.5, 2.5)):\r\n",
        "    set_figsize(figsize)\r\n",
        "    plt.xlabel(x_label)\r\n",
        "    plt.ylabel(y_label)\r\n",
        "    plt.semilogy(x_vals, y_vals)\r\n",
        "    if x2_vals and y2_vals:\r\n",
        "        plt.semilogy(x2_vals, y2_vals, linestyle=':')\r\n",
        "        plt.legend(legend)\r\n",
        "    # plt.show()\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ############################# 3.13 ##############################\r\n",
        "# 5.5 修改\r\n",
        "# def evaluate_accuracy(data_iter, net):\r\n",
        "#     acc_sum, n = 0.0, 0\r\n",
        "#     for X, y in data_iter:\r\n",
        "#         if isinstance(net, torch.nn.Module):\r\n",
        "#             net.eval() # 评估模式, 这会关闭dropout\r\n",
        "#             acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()\r\n",
        "#             net.train() # 改回训练模式\r\n",
        "#         else: # 自定义的模型\r\n",
        "#             if('is_training' in net.__code__.co_varnames): # 如果有is_training这个参数\r\n",
        "#                 # 将is_training设置成False\r\n",
        "#                 acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() \r\n",
        "#             else:\r\n",
        "#                 acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() \r\n",
        "#         n += y.shape[0]\r\n",
        "#     return acc_sum / n\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ########################### 5.1 #########################\r\n",
        "def corr2d(X, K):  \r\n",
        "    h, w = K.shape\r\n",
        "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\r\n",
        "    for i in range(Y.shape[0]):\r\n",
        "        for j in range(Y.shape[1]):\r\n",
        "            Y[i, j] = (X[i: i + h, j: j + w] * K).sum()\r\n",
        "    return Y\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ############################ 5.5 #########################\r\n",
        "def evaluate_accuracy(data_iter, net, device=None):\r\n",
        "    if device is None and isinstance(net, torch.nn.Module):\r\n",
        "        # 如果没指定device就使用net的device\r\n",
        "        device = list(net.parameters())[0].device \r\n",
        "    acc_sum, n = 0.0, 0\r\n",
        "    with torch.no_grad():\r\n",
        "        for X, y in data_iter:\r\n",
        "            if isinstance(net, torch.nn.Module):\r\n",
        "                net.eval() # 评估模式, 这会关闭dropout\r\n",
        "                acc_sum += (net(X.to(device)).argmax(dim=1) == y.to(device)).float().sum().cpu().item()\r\n",
        "                net.train() # 改回训练模式\r\n",
        "            else: # 自定义的模型, 3.13节之后不会用到, 不考虑GPU\r\n",
        "                if('is_training' in net.__code__.co_varnames): # 如果有is_training这个参数\r\n",
        "                    # 将is_training设置成False\r\n",
        "                    acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() \r\n",
        "                else:\r\n",
        "                    acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() \r\n",
        "            n += y.shape[0]\r\n",
        "    return acc_sum / n\r\n",
        "\r\n",
        "def train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs):\r\n",
        "    net = net.to(device)\r\n",
        "    print(\"training on \", device)\r\n",
        "    loss = torch.nn.CrossEntropyLoss()\r\n",
        "    for epoch in range(num_epochs):\r\n",
        "        train_l_sum, train_acc_sum, n, batch_count, start = 0.0, 0.0, 0, 0, time.time()\r\n",
        "        for X, y in train_iter:\r\n",
        "            X = X.to(device)\r\n",
        "            y = y.to(device)\r\n",
        "            y_hat = net(X)\r\n",
        "            l = loss(y_hat, y)\r\n",
        "            optimizer.zero_grad()\r\n",
        "            l.backward()\r\n",
        "            optimizer.step()\r\n",
        "            train_l_sum += l.cpu().item()\r\n",
        "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\r\n",
        "            n += y.shape[0]\r\n",
        "            batch_count += 1\r\n",
        "        test_acc = evaluate_accuracy(test_iter, net)\r\n",
        "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\r\n",
        "              % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ########################## 5.6 #########################3\r\n",
        "def load_data_fashion_mnist(batch_size, resize=None, root='~/Datasets/FashionMNIST'):\r\n",
        "    \"\"\"Download the fashion mnist dataset and then load into memory.\"\"\"\r\n",
        "    trans = []\r\n",
        "    if resize:\r\n",
        "        trans.append(torchvision.transforms.Resize(size=resize))\r\n",
        "    trans.append(torchvision.transforms.ToTensor())\r\n",
        "    \r\n",
        "    transform = torchvision.transforms.Compose(trans)\r\n",
        "    mnist_train = torchvision.datasets.FashionMNIST(root=root, train=True, download=True, transform=transform)\r\n",
        "    mnist_test = torchvision.datasets.FashionMNIST(root=root, train=False, download=True, transform=transform)\r\n",
        "    if sys.platform.startswith('win'):\r\n",
        "        num_workers = 0  # 0表示不用额外的进程来加速读取数据\r\n",
        "    else:\r\n",
        "        num_workers = 4\r\n",
        "    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\r\n",
        "    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\r\n",
        "\r\n",
        "    return train_iter, test_iter\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "############################# 5.8 ##############################\r\n",
        "class GlobalAvgPool2d(nn.Module):\r\n",
        "    # 全局平均池化层可通过将池化窗口形状设置成输入的高和宽实现\r\n",
        "    def __init__(self):\r\n",
        "        super(GlobalAvgPool2d, self).__init__()\r\n",
        "    def forward(self, x):\r\n",
        "        return F.avg_pool2d(x, kernel_size=x.size()[2:])\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ########################### 5.11 ################################\r\n",
        "class Residual(nn.Module): \r\n",
        "    def __init__(self, in_channels, out_channels, use_1x1conv=False, stride=1):\r\n",
        "        super(Residual, self).__init__()\r\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride)\r\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\r\n",
        "        if use_1x1conv:\r\n",
        "            self.conv3 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride)\r\n",
        "        else:\r\n",
        "            self.conv3 = None\r\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\r\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\r\n",
        "\r\n",
        "    def forward(self, X):\r\n",
        "        Y = F.relu(self.bn1(self.conv1(X)))\r\n",
        "        Y = self.bn2(self.conv2(Y))\r\n",
        "        if self.conv3:\r\n",
        "            X = self.conv3(X)\r\n",
        "        return F.relu(Y + X)\r\n",
        "\r\n",
        "def resnet_block(in_channels, out_channels, num_residuals, first_block=False):\r\n",
        "    if first_block:\r\n",
        "        assert in_channels == out_channels # 第一个模块的通道数同输入通道数一致\r\n",
        "    blk = []\r\n",
        "    for i in range(num_residuals):\r\n",
        "        if i == 0 and not first_block:\r\n",
        "            blk.append(Residual(in_channels, out_channels, use_1x1conv=True, stride=2))\r\n",
        "        else:\r\n",
        "            blk.append(Residual(out_channels, out_channels))\r\n",
        "    return nn.Sequential(*blk)\r\n",
        "    \r\n",
        "def resnet18(output=10, in_channels=3):\r\n",
        "    net = nn.Sequential(\r\n",
        "        nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3),\r\n",
        "        nn.BatchNorm2d(64), \r\n",
        "        nn.ReLU(),\r\n",
        "        nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\r\n",
        "    net.add_module(\"resnet_block1\", resnet_block(64, 64, 2, first_block=True))\r\n",
        "    net.add_module(\"resnet_block2\", resnet_block(64, 128, 2))\r\n",
        "    net.add_module(\"resnet_block3\", resnet_block(128, 256, 2))\r\n",
        "    net.add_module(\"resnet_block4\", resnet_block(256, 512, 2))\r\n",
        "    net.add_module(\"global_avg_pool\", GlobalAvgPool2d()) # GlobalAvgPool2d的输出: (Batch, 512, 1, 1)\r\n",
        "    net.add_module(\"fc\", nn.Sequential(FlattenLayer(), nn.Linear(512, output))) \r\n",
        "    return net\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ############################## 6.3 ##################################3\r\n",
        "def load_data_jay_lyrics():\r\n",
        "    \"\"\"加载周杰伦歌词数据集\"\"\"\r\n",
        "    with zipfile.ZipFile('../../data/jaychou_lyrics.txt.zip') as zin:\r\n",
        "        with zin.open('jaychou_lyrics.txt') as f:\r\n",
        "            corpus_chars = f.read().decode('utf-8')\r\n",
        "    corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\r\n",
        "    corpus_chars = corpus_chars[0:10000]\r\n",
        "    idx_to_char = list(set(corpus_chars))\r\n",
        "    char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\r\n",
        "    vocab_size = len(char_to_idx)\r\n",
        "    corpus_indices = [char_to_idx[char] for char in corpus_chars]\r\n",
        "    return corpus_indices, char_to_idx, idx_to_char, vocab_size\r\n",
        "\r\n",
        "def data_iter_random(corpus_indices, batch_size, num_steps, device=None):\r\n",
        "    # 减1是因为输出的索引x是相应输入的索引y加1\r\n",
        "    num_examples = (len(corpus_indices) - 1) // num_steps\r\n",
        "    epoch_size = num_examples // batch_size\r\n",
        "    example_indices = list(range(num_examples))\r\n",
        "    random.shuffle(example_indices)\r\n",
        "\r\n",
        "    # 返回从pos开始的长为num_steps的序列\r\n",
        "    def _data(pos):\r\n",
        "        return corpus_indices[pos: pos + num_steps]\r\n",
        "    if device is None:\r\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "    \r\n",
        "    for i in range(epoch_size):\r\n",
        "        # 每次读取batch_size个随机样本\r\n",
        "        i = i * batch_size\r\n",
        "        batch_indices = example_indices[i: i + batch_size]\r\n",
        "        X = [_data(j * num_steps) for j in batch_indices]\r\n",
        "        Y = [_data(j * num_steps + 1) for j in batch_indices]\r\n",
        "        yield torch.tensor(X, dtype=torch.float32, device=device), torch.tensor(Y, dtype=torch.float32, device=device)\r\n",
        "\r\n",
        "def data_iter_consecutive(corpus_indices, batch_size, num_steps, device=None):\r\n",
        "    if device is None:\r\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "    corpus_indices = torch.tensor(corpus_indices, dtype=torch.float32, device=device)\r\n",
        "    data_len = len(corpus_indices)\r\n",
        "    batch_len = data_len // batch_size\r\n",
        "    indices = corpus_indices[0: batch_size*batch_len].view(batch_size, batch_len)\r\n",
        "    epoch_size = (batch_len - 1) // num_steps\r\n",
        "    for i in range(epoch_size):\r\n",
        "        i = i * num_steps\r\n",
        "        X = indices[:, i: i + num_steps]\r\n",
        "        Y = indices[:, i + 1: i + num_steps + 1]\r\n",
        "        yield X, Y\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ###################################### 6.4 ######################################\r\n",
        "def one_hot(x, n_class, dtype=torch.float32): \r\n",
        "    # X shape: (batch), output shape: (batch, n_class)\r\n",
        "    x = x.long()\r\n",
        "    res = torch.zeros(x.shape[0], n_class, dtype=dtype, device=x.device)\r\n",
        "    res.scatter_(1, x.view(-1, 1), 1)\r\n",
        "    return res\r\n",
        "\r\n",
        "def to_onehot(X, n_class):  \r\n",
        "    # X shape: (batch, seq_len), output: seq_len elements of (batch, n_class)\r\n",
        "    return [one_hot(X[:, i], n_class) for i in range(X.shape[1])]\r\n",
        "\r\n",
        "def predict_rnn(prefix, num_chars, rnn, params, init_rnn_state,\r\n",
        "                num_hiddens, vocab_size, device, idx_to_char, char_to_idx):\r\n",
        "    state = init_rnn_state(1, num_hiddens, device)\r\n",
        "    output = [char_to_idx[prefix[0]]]\r\n",
        "    for t in range(num_chars + len(prefix) - 1):\r\n",
        "        # 将上一时间步的输出作为当前时间步的输入\r\n",
        "        X = to_onehot(torch.tensor([[output[-1]]], device=device), vocab_size)\r\n",
        "        # 计算输出和更新隐藏状态\r\n",
        "        (Y, state) = rnn(X, state, params)\r\n",
        "        # 下一个时间步的输入是prefix里的字符或者当前的最佳预测字符\r\n",
        "        if t < len(prefix) - 1:\r\n",
        "            output.append(char_to_idx[prefix[t + 1]])\r\n",
        "        else:\r\n",
        "            output.append(int(Y[0].argmax(dim=1).item()))\r\n",
        "    return ''.join([idx_to_char[i] for i in output])\r\n",
        "\r\n",
        "def grad_clipping(params, theta, device):\r\n",
        "    norm = torch.tensor([0.0], device=device)\r\n",
        "    for param in params:\r\n",
        "        norm += (param.grad.data ** 2).sum()\r\n",
        "    norm = norm.sqrt().item()\r\n",
        "    if norm > theta:\r\n",
        "        for param in params:\r\n",
        "            param.grad.data *= (theta / norm)\r\n",
        "\r\n",
        "def train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\r\n",
        "                          vocab_size, device, corpus_indices, idx_to_char,\r\n",
        "                          char_to_idx, is_random_iter, num_epochs, num_steps,\r\n",
        "                          lr, clipping_theta, batch_size, pred_period,\r\n",
        "                          pred_len, prefixes):\r\n",
        "    if is_random_iter:\r\n",
        "        data_iter_fn = data_iter_random\r\n",
        "    else:\r\n",
        "        data_iter_fn = data_iter_consecutive\r\n",
        "    params = get_params()\r\n",
        "    loss = nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "    for epoch in range(num_epochs):\r\n",
        "        if not is_random_iter:  # 如使用相邻采样，在epoch开始时初始化隐藏状态\r\n",
        "            state = init_rnn_state(batch_size, num_hiddens, device)\r\n",
        "        l_sum, n, start = 0.0, 0, time.time()\r\n",
        "        data_iter = data_iter_fn(corpus_indices, batch_size, num_steps, device)\r\n",
        "        for X, Y in data_iter:\r\n",
        "            if is_random_iter:  # 如使用随机采样，在每个小批量更新前初始化隐藏状态\r\n",
        "                state = init_rnn_state(batch_size, num_hiddens, device)\r\n",
        "            else: \r\n",
        "            # 否则需要使用detach函数从计算图分离隐藏状态, 这是为了\r\n",
        "            # 使模型参数的梯度计算只依赖一次迭代读取的小批量序列(防止梯度计算开销太大)\r\n",
        "                for s in state:\r\n",
        "                    s.detach_()\r\n",
        "            \r\n",
        "            inputs = to_onehot(X, vocab_size)\r\n",
        "            # outputs有num_steps个形状为(batch_size, vocab_size)的矩阵\r\n",
        "            (outputs, state) = rnn(inputs, state, params)\r\n",
        "            # 拼接之后形状为(num_steps * batch_size, vocab_size)\r\n",
        "            outputs = torch.cat(outputs, dim=0)\r\n",
        "            # Y的形状是(batch_size, num_steps)，转置后再变成长度为\r\n",
        "            # batch * num_steps 的向量，这样跟输出的行一一对应\r\n",
        "            y = torch.transpose(Y, 0, 1).contiguous().view(-1)\r\n",
        "            # 使用交叉熵损失计算平均分类误差\r\n",
        "            l = loss(outputs, y.long())\r\n",
        "            \r\n",
        "            # 梯度清0\r\n",
        "            if params[0].grad is not None:\r\n",
        "                for param in params:\r\n",
        "                    param.grad.data.zero_()\r\n",
        "            l.backward()\r\n",
        "            grad_clipping(params, clipping_theta, device)  # 裁剪梯度\r\n",
        "            sgd(params, lr, 1)  # 因为误差已经取过均值，梯度不用再做平均\r\n",
        "            l_sum += l.item() * y.shape[0]\r\n",
        "            n += y.shape[0]\r\n",
        "\r\n",
        "        if (epoch + 1) % pred_period == 0:\r\n",
        "            print('epoch %d, perplexity %f, time %.2f sec' % (\r\n",
        "                epoch + 1, math.exp(l_sum / n), time.time() - start))\r\n",
        "            for prefix in prefixes:\r\n",
        "                print(' -', predict_rnn(prefix, pred_len, rnn, params, init_rnn_state,\r\n",
        "                    num_hiddens, vocab_size, device, idx_to_char, char_to_idx))\r\n",
        "\r\n",
        "                \r\n",
        "                \r\n",
        "                \r\n",
        "# ################################### 6.5 ################################################\r\n",
        "class RNNModel(nn.Module):\r\n",
        "    def __init__(self, rnn_layer, vocab_size):\r\n",
        "        super(RNNModel, self).__init__()\r\n",
        "        self.rnn = rnn_layer\r\n",
        "        self.hidden_size = rnn_layer.hidden_size * (2 if rnn_layer.bidirectional else 1) \r\n",
        "        self.vocab_size = vocab_size\r\n",
        "        self.dense = nn.Linear(self.hidden_size, vocab_size)\r\n",
        "        self.state = None\r\n",
        "\r\n",
        "    def forward(self, inputs, state): # inputs: (batch, seq_len)\r\n",
        "        # 获取one-hot向量表示\r\n",
        "        X = to_onehot(inputs, self.vocab_size) # X是个list\r\n",
        "        Y, self.state = self.rnn(torch.stack(X), state)\r\n",
        "        # 全连接层会首先将Y的形状变成(num_steps * batch_size, num_hiddens)，它的输出\r\n",
        "        # 形状为(num_steps * batch_size, vocab_size)\r\n",
        "        output = self.dense(Y.view(-1, Y.shape[-1]))\r\n",
        "        return output, self.state\r\n",
        "\r\n",
        "def predict_rnn_pytorch(prefix, num_chars, model, vocab_size, device, idx_to_char,\r\n",
        "                      char_to_idx):\r\n",
        "    state = None\r\n",
        "    output = [char_to_idx[prefix[0]]] # output会记录prefix加上输出\r\n",
        "    for t in range(num_chars + len(prefix) - 1):\r\n",
        "        X = torch.tensor([output[-1]], device=device).view(1, 1)\r\n",
        "        if state is not None:\r\n",
        "            if isinstance(state, tuple): # LSTM, state:(h, c)  \r\n",
        "                state = (state[0].to(device), state[1].to(device))\r\n",
        "            else:   \r\n",
        "                state = state.to(device)\r\n",
        "            \r\n",
        "        (Y, state) = model(X, state)  # 前向计算不需要传入模型参数\r\n",
        "        if t < len(prefix) - 1:\r\n",
        "            output.append(char_to_idx[prefix[t + 1]])\r\n",
        "        else:\r\n",
        "            output.append(int(Y.argmax(dim=1).item()))\r\n",
        "    return ''.join([idx_to_char[i] for i in output])\r\n",
        "\r\n",
        "def train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,\r\n",
        "                                corpus_indices, idx_to_char, char_to_idx,\r\n",
        "                                num_epochs, num_steps, lr, clipping_theta,\r\n",
        "                                batch_size, pred_period, pred_len, prefixes):\r\n",
        "    loss = nn.CrossEntropyLoss()\r\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\r\n",
        "    model.to(device)\r\n",
        "    state = None\r\n",
        "    for epoch in range(num_epochs):\r\n",
        "        l_sum, n, start = 0.0, 0, time.time()\r\n",
        "        data_iter = data_iter_consecutive(corpus_indices, batch_size, num_steps, device) # 相邻采样\r\n",
        "        for X, Y in data_iter:\r\n",
        "            if state is not None:\r\n",
        "                # 使用detach函数从计算图分离隐藏状态, 这是为了\r\n",
        "                # 使模型参数的梯度计算只依赖一次迭代读取的小批量序列(防止梯度计算开销太大)\r\n",
        "                if isinstance (state, tuple): # LSTM, state:(h, c)  \r\n",
        "                    state = (state[0].detach(), state[1].detach())\r\n",
        "                else:   \r\n",
        "                    state = state.detach()\r\n",
        "    \r\n",
        "            (output, state) = model(X, state) # output: 形状为(num_steps * batch_size, vocab_size)\r\n",
        "            \r\n",
        "            # Y的形状是(batch_size, num_steps)，转置后再变成长度为\r\n",
        "            # batch * num_steps 的向量，这样跟输出的行一一对应\r\n",
        "            y = torch.transpose(Y, 0, 1).contiguous().view(-1)\r\n",
        "            l = loss(output, y.long())\r\n",
        "            \r\n",
        "            optimizer.zero_grad()\r\n",
        "            l.backward()\r\n",
        "            # 梯度裁剪\r\n",
        "            grad_clipping(model.parameters(), clipping_theta, device)\r\n",
        "            optimizer.step()\r\n",
        "            l_sum += l.item() * y.shape[0]\r\n",
        "            n += y.shape[0]\r\n",
        "        \r\n",
        "        try:\r\n",
        "            perplexity = math.exp(l_sum / n)\r\n",
        "        except OverflowError:\r\n",
        "            perplexity = float('inf')\r\n",
        "        if (epoch + 1) % pred_period == 0:\r\n",
        "            print('epoch %d, perplexity %f, time %.2f sec' % (\r\n",
        "                epoch + 1, perplexity, time.time() - start))\r\n",
        "            for prefix in prefixes:\r\n",
        "                print(' -', predict_rnn_pytorch(\r\n",
        "                    prefix, pred_len, model, vocab_size, device, idx_to_char,\r\n",
        "                    char_to_idx))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ######################################## 7.2 ###############################################\r\n",
        "def train_2d(trainer):  \r\n",
        "    x1, x2, s1, s2 = -5, -2, 0, 0  # s1和s2是自变量状态，本章后续几节会使用\r\n",
        "    results = [(x1, x2)]\r\n",
        "    for i in range(20):\r\n",
        "        x1, x2, s1, s2 = trainer(x1, x2, s1, s2)\r\n",
        "        results.append((x1, x2))\r\n",
        "    print('epoch %d, x1 %f, x2 %f' % (i + 1, x1, x2))\r\n",
        "    return results\r\n",
        "\r\n",
        "def show_trace_2d(f, results):  \r\n",
        "    plt.plot(*zip(*results), '-o', color='#ff7f0e')\r\n",
        "    x1, x2 = np.meshgrid(np.arange(-5.5, 1.0, 0.1), np.arange(-3.0, 1.0, 0.1))\r\n",
        "    plt.contour(x1, x2, f(x1, x2), colors='#1f77b4')\r\n",
        "    plt.xlabel('x1')\r\n",
        "    plt.ylabel('x2')\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ######################################## 7.3 ###############################################\r\n",
        "def get_data_ch7():  \r\n",
        "    data = np.genfromtxt('../../data/airfoil_self_noise.dat', delimiter='\\t')\r\n",
        "    data = (data - data.mean(axis=0)) / data.std(axis=0)\r\n",
        "    return torch.tensor(data[:1500, :-1], dtype=torch.float32), \\\r\n",
        "        torch.tensor(data[:1500, -1], dtype=torch.float32) # 前1500个样本(每个样本5个特征)\r\n",
        "\r\n",
        "def train_ch7(optimizer_fn, states, hyperparams, features, labels,\r\n",
        "              batch_size=10, num_epochs=2):\r\n",
        "    # 初始化模型\r\n",
        "    net, loss = linreg, squared_loss\r\n",
        "    \r\n",
        "    w = torch.nn.Parameter(torch.tensor(np.random.normal(0, 0.01, size=(features.shape[1], 1)), dtype=torch.float32),\r\n",
        "                           requires_grad=True)\r\n",
        "    b = torch.nn.Parameter(torch.zeros(1, dtype=torch.float32), requires_grad=True)\r\n",
        "\r\n",
        "    def eval_loss():\r\n",
        "        return loss(net(features, w, b), labels).mean().item()\r\n",
        "\r\n",
        "    ls = [eval_loss()]\r\n",
        "    data_iter = torch.utils.data.DataLoader(\r\n",
        "        torch.utils.data.TensorDataset(features, labels), batch_size, shuffle=True)\r\n",
        "    \r\n",
        "    for _ in range(num_epochs):\r\n",
        "        start = time.time()\r\n",
        "        for batch_i, (X, y) in enumerate(data_iter):\r\n",
        "            l = loss(net(X, w, b), y).mean()  # 使用平均损失\r\n",
        "            \r\n",
        "            # 梯度清零\r\n",
        "            if w.grad is not None:\r\n",
        "                w.grad.data.zero_()\r\n",
        "                b.grad.data.zero_()\r\n",
        "                \r\n",
        "            l.backward()\r\n",
        "            optimizer_fn([w, b], states, hyperparams)  # 迭代模型参数\r\n",
        "            if (batch_i + 1) * batch_size % 100 == 0:\r\n",
        "                ls.append(eval_loss())  # 每100个样本记录下当前训练误差\r\n",
        "    # 打印结果和作图\r\n",
        "    print('loss: %f, %f sec per epoch' % (ls[-1], time.time() - start))\r\n",
        "    set_figsize()\r\n",
        "    plt.plot(np.linspace(0, num_epochs, len(ls)), ls)\r\n",
        "    plt.xlabel('epoch')\r\n",
        "    plt.ylabel('loss')\r\n",
        "\r\n",
        "# 本函数与原书不同的是这里第一个参数优化器函数而不是优化器的名字\r\n",
        "# 例如: optimizer_fn=torch.optim.SGD, optimizer_hyperparams={\"lr\": 0.05}\r\n",
        "def train_pytorch_ch7(optimizer_fn, optimizer_hyperparams, features, labels,\r\n",
        "                    batch_size=10, num_epochs=2):\r\n",
        "    # 初始化模型\r\n",
        "    net = nn.Sequential(\r\n",
        "        nn.Linear(features.shape[-1], 1)\r\n",
        "    )\r\n",
        "    loss = nn.MSELoss()\r\n",
        "    optimizer = optimizer_fn(net.parameters(), **optimizer_hyperparams)\r\n",
        "\r\n",
        "    def eval_loss():\r\n",
        "        return loss(net(features).view(-1), labels).item() / 2\r\n",
        "\r\n",
        "    ls = [eval_loss()]\r\n",
        "    data_iter = torch.utils.data.DataLoader(\r\n",
        "        torch.utils.data.TensorDataset(features, labels), batch_size, shuffle=True)\r\n",
        "\r\n",
        "    for _ in range(num_epochs):\r\n",
        "        start = time.time()\r\n",
        "        for batch_i, (X, y) in enumerate(data_iter):\r\n",
        "            # 除以2是为了和train_ch7保持一致, 因为squared_loss中除了2\r\n",
        "            l = loss(net(X).view(-1), y) / 2 \r\n",
        "            \r\n",
        "            optimizer.zero_grad()\r\n",
        "            l.backward()\r\n",
        "            optimizer.step()\r\n",
        "            if (batch_i + 1) * batch_size % 100 == 0:\r\n",
        "                ls.append(eval_loss())\r\n",
        "    # 打印结果和作图\r\n",
        "    print('loss: %f, %f sec per epoch' % (ls[-1], time.time() - start))\r\n",
        "    set_figsize()\r\n",
        "    plt.plot(np.linspace(0, num_epochs, len(ls)), ls)\r\n",
        "    plt.xlabel('epoch')\r\n",
        "    plt.ylabel('loss')\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "############################## 8.3 ##################################\r\n",
        "class Benchmark():\r\n",
        "    def __init__(self, prefix=None):\r\n",
        "        self.prefix = prefix + ' ' if prefix else ''\r\n",
        "\r\n",
        "    def __enter__(self):\r\n",
        "        self.start = time.time()\r\n",
        "\r\n",
        "    def __exit__(self, *args):\r\n",
        "        print('%stime: %.4f sec' % (self.prefix, time.time() - self.start))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ########################### 9.1 ########################################\r\n",
        "def show_images(imgs, num_rows, num_cols, scale=2):\r\n",
        "    figsize = (num_cols * scale, num_rows * scale)\r\n",
        "    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)\r\n",
        "    for i in range(num_rows):\r\n",
        "        for j in range(num_cols):\r\n",
        "            axes[i][j].imshow(imgs[i * num_cols + j])\r\n",
        "            axes[i][j].axes.get_xaxis().set_visible(False)\r\n",
        "            axes[i][j].axes.get_yaxis().set_visible(False)\r\n",
        "    return axes\r\n",
        "\r\n",
        "def train(train_iter, test_iter, net, loss, optimizer, device, num_epochs):\r\n",
        "    net = net.to(device)\r\n",
        "    print(\"training on \", device)\r\n",
        "    batch_count = 0\r\n",
        "    for epoch in range(num_epochs):\r\n",
        "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\r\n",
        "        for X, y in train_iter:\r\n",
        "            X = X.to(device)\r\n",
        "            y = y.to(device)\r\n",
        "            y_hat = net(X)\r\n",
        "            l = loss(y_hat, y) \r\n",
        "            optimizer.zero_grad()\r\n",
        "            l.backward()\r\n",
        "            optimizer.step()\r\n",
        "            train_l_sum += l.cpu().item()\r\n",
        "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\r\n",
        "            n += y.shape[0]\r\n",
        "            batch_count += 1\r\n",
        "        test_acc = evaluate_accuracy(test_iter, net)\r\n",
        "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\r\n",
        "              % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "############################## 9.3 #####################\r\n",
        "def bbox_to_rect(bbox, color):\r\n",
        "    # 将边界框(左上x, 左上y, 右下x, 右下y)格式转换成matplotlib格式：\r\n",
        "    # ((左上x, 左上y), 宽, 高)\r\n",
        "    return plt.Rectangle(\r\n",
        "        xy=(bbox[0], bbox[1]), width=bbox[2]-bbox[0], height=bbox[3]-bbox[1],\r\n",
        "        fill=False, edgecolor=color, linewidth=2)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "############################ 9.4 ###########################\r\n",
        "def MultiBoxPrior(feature_map, sizes=[0.75, 0.5, 0.25], ratios=[1, 2, 0.5]):\r\n",
        "    \"\"\"\r\n",
        "    # 按照「9.4.1. 生成多个锚框」所讲的实现, anchor表示成(xmin, ymin, xmax, ymax).\r\n",
        "    https://zh.d2l.ai/chapter_computer-vision/anchor.html\r\n",
        "    Args:\r\n",
        "        feature_map: torch tensor, Shape: [N, C, H, W].\r\n",
        "        sizes: List of sizes (0~1) of generated MultiBoxPriores. \r\n",
        "        ratios: List of aspect ratios (non-negative) of generated MultiBoxPriores. \r\n",
        "    Returns:\r\n",
        "        anchors of shape (1, num_anchors, 4). 由于batch里每个都一样, 所以第一维为1\r\n",
        "    \"\"\"\r\n",
        "    pairs = [] # pair of (size, sqrt(ration))\r\n",
        "    for r in ratios:\r\n",
        "        pairs.append([sizes[0], math.sqrt(r)])\r\n",
        "    for s in sizes[1:]:\r\n",
        "        pairs.append([s, math.sqrt(ratios[0])])\r\n",
        "    \r\n",
        "    pairs = np.array(pairs)\r\n",
        "    \r\n",
        "    ss1 = pairs[:, 0] * pairs[:, 1] # size * sqrt(ration)\r\n",
        "    ss2 = pairs[:, 0] / pairs[:, 1] # size / sqrt(ration)\r\n",
        "    \r\n",
        "    base_anchors = np.stack([-ss1, -ss2, ss1, ss2], axis=1) / 2\r\n",
        "    \r\n",
        "    h, w = feature_map.shape[-2:]\r\n",
        "    shifts_x = np.arange(0, w) / w\r\n",
        "    shifts_y = np.arange(0, h) / h\r\n",
        "    shift_x, shift_y = np.meshgrid(shifts_x, shifts_y)\r\n",
        "    shift_x = shift_x.reshape(-1)\r\n",
        "    shift_y = shift_y.reshape(-1)\r\n",
        "    shifts = np.stack((shift_x, shift_y, shift_x, shift_y), axis=1)\r\n",
        "    \r\n",
        "    anchors = shifts.reshape((-1, 1, 4)) + base_anchors.reshape((1, -1, 4))\r\n",
        "    \r\n",
        "    return torch.tensor(anchors, dtype=torch.float32).view(1, -1, 4)\r\n",
        "\r\n",
        "def show_bboxes(axes, bboxes, labels=None, colors=None):\r\n",
        "    def _make_list(obj, default_values=None):\r\n",
        "        if obj is None:\r\n",
        "            obj = default_values\r\n",
        "        elif not isinstance(obj, (list, tuple)):\r\n",
        "            obj = [obj]\r\n",
        "        return obj\r\n",
        "\r\n",
        "    labels = _make_list(labels)\r\n",
        "    colors = _make_list(colors, ['b', 'g', 'r', 'm', 'c'])\r\n",
        "    for i, bbox in enumerate(bboxes):\r\n",
        "        color = colors[i % len(colors)]\r\n",
        "        rect = bbox_to_rect(bbox.detach().cpu().numpy(), color)\r\n",
        "        axes.add_patch(rect)\r\n",
        "        if labels and len(labels) > i:\r\n",
        "            text_color = 'k' if color == 'w' else 'w'\r\n",
        "            axes.text(rect.xy[0], rect.xy[1], labels[i],\r\n",
        "                      va='center', ha='center', fontsize=6, color=text_color,\r\n",
        "                      bbox=dict(facecolor=color, lw=0))\r\n",
        "\r\n",
        "def compute_intersection(set_1, set_2):\r\n",
        "    \"\"\"\r\n",
        "    计算anchor之间的交集\r\n",
        "    Args:\r\n",
        "        set_1: a tensor of dimensions (n1, 4), anchor表示成(xmin, ymin, xmax, ymax)\r\n",
        "        set_2: a tensor of dimensions (n2, 4), anchor表示成(xmin, ymin, xmax, ymax)\r\n",
        "    Returns:\r\n",
        "        intersection of each of the boxes in set 1 with respect to each of the boxes in set 2, shape: (n1, n2)\r\n",
        "    \"\"\"\r\n",
        "    # PyTorch auto-broadcasts singleton dimensions\r\n",
        "    lower_bounds = torch.max(set_1[:, :2].unsqueeze(1), set_2[:, :2].unsqueeze(0))  # (n1, n2, 2)\r\n",
        "    upper_bounds = torch.min(set_1[:, 2:].unsqueeze(1), set_2[:, 2:].unsqueeze(0))  # (n1, n2, 2)\r\n",
        "    intersection_dims = torch.clamp(upper_bounds - lower_bounds, min=0)  # (n1, n2, 2)\r\n",
        "    return intersection_dims[:, :, 0] * intersection_dims[:, :, 1]  # (n1, n2)\r\n",
        "\r\n",
        "def compute_jaccard(set_1, set_2):\r\n",
        "    \"\"\"\r\n",
        "    计算anchor之间的Jaccard系数(IoU)\r\n",
        "    Args:\r\n",
        "        set_1: a tensor of dimensions (n1, 4), anchor表示成(xmin, ymin, xmax, ymax)\r\n",
        "        set_2: a tensor of dimensions (n2, 4), anchor表示成(xmin, ymin, xmax, ymax)\r\n",
        "    Returns:\r\n",
        "        Jaccard Overlap of each of the boxes in set 1 with respect to each of the boxes in set 2, shape: (n1, n2)\r\n",
        "    \"\"\"\r\n",
        "    # Find intersections\r\n",
        "    intersection = compute_intersection(set_1, set_2)  # (n1, n2)\r\n",
        "\r\n",
        "    # Find areas of each box in both sets\r\n",
        "    areas_set_1 = (set_1[:, 2] - set_1[:, 0]) * (set_1[:, 3] - set_1[:, 1])  # (n1)\r\n",
        "    areas_set_2 = (set_2[:, 2] - set_2[:, 0]) * (set_2[:, 3] - set_2[:, 1])  # (n2)\r\n",
        "\r\n",
        "    # Find the union\r\n",
        "    # PyTorch auto-broadcasts singleton dimensions\r\n",
        "    union = areas_set_1.unsqueeze(1) + areas_set_2.unsqueeze(0) - intersection  # (n1, n2)\r\n",
        "\r\n",
        "    return intersection / union  # (n1, n2)\r\n",
        "\r\n",
        "def assign_anchor(bb, anchor, jaccard_threshold=0.5):\r\n",
        "    \"\"\"\r\n",
        "    # 按照「9.4.1. 生成多个锚框」图9.3所讲为每个anchor分配真实的bb, anchor表示成归一化(xmin, ymin, xmax, ymax).\r\n",
        "    https://zh.d2l.ai/chapter_computer-vision/anchor.html\r\n",
        "    Args:\r\n",
        "        bb: 真实边界框(bounding box), shape:（nb, 4）\r\n",
        "        anchor: 待分配的anchor, shape:（na, 4）\r\n",
        "        jaccard_threshold: 预先设定的阈值\r\n",
        "    Returns:\r\n",
        "        assigned_idx: shape: (na, ), 每个anchor分配的真实bb对应的索引, 若未分配任何bb则为-1\r\n",
        "    \"\"\"\r\n",
        "    na = anchor.shape[0]\r\n",
        "    nb = bb.shape[0]\r\n",
        "    jaccard = compute_jaccard(anchor, bb).detach().cpu().numpy() # shape: (na, nb)\r\n",
        "    assigned_idx = np.ones(na) * -1  # 初始全为-1\r\n",
        "    \r\n",
        "    # 先为每个bb分配一个anchor(不要求满足jaccard_threshold)\r\n",
        "    jaccard_cp = jaccard.copy()\r\n",
        "    for j in range(nb):\r\n",
        "        i = np.argmax(jaccard_cp[:, j])\r\n",
        "        assigned_idx[i] = j\r\n",
        "        jaccard_cp[i, :] = float(\"-inf\") # 赋值为负无穷, 相当于去掉这一行\r\n",
        "     \r\n",
        "    # 处理还未被分配的anchor, 要求满足jaccard_threshold\r\n",
        "    for i in range(na):\r\n",
        "        if assigned_idx[i] == -1:\r\n",
        "            j = np.argmax(jaccard[i, :])\r\n",
        "            if jaccard[i, j] >= jaccard_threshold:\r\n",
        "                assigned_idx[i] = j\r\n",
        "    \r\n",
        "    return torch.tensor(assigned_idx, dtype=torch.long)\r\n",
        "\r\n",
        "def xy_to_cxcy(xy):\r\n",
        "    \"\"\"\r\n",
        "    将(x_min, y_min, x_max, y_max)形式的anchor转换成(center_x, center_y, w, h)形式的.\r\n",
        "    https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/utils.py\r\n",
        "    Args:\r\n",
        "        xy: bounding boxes in boundary coordinates, a tensor of size (n_boxes, 4)\r\n",
        "    Returns: \r\n",
        "        bounding boxes in center-size coordinates, a tensor of size (n_boxes, 4)\r\n",
        "    \"\"\"\r\n",
        "    return torch.cat([(xy[:, 2:] + xy[:, :2]) / 2,  # c_x, c_y\r\n",
        "                      xy[:, 2:] - xy[:, :2]], 1)  # w, h\r\n",
        "\r\n",
        "def MultiBoxTarget(anchor, label):\r\n",
        "    \"\"\"\r\n",
        "    # 按照「9.4.1. 生成多个锚框」所讲的实现, anchor表示成归一化(xmin, ymin, xmax, ymax).\r\n",
        "    https://zh.d2l.ai/chapter_computer-vision/anchor.html\r\n",
        "    Args:\r\n",
        "        anchor: torch tensor, 输入的锚框, 一般是通过MultiBoxPrior生成, shape:（1，锚框总数，4）\r\n",
        "        label: 真实标签, shape为(bn, 每张图片最多的真实锚框数, 5)\r\n",
        "               第二维中，如果给定图片没有这么多锚框, 可以先用-1填充空白, 最后一维中的元素为[类别标签, 四个坐标值]\r\n",
        "    Returns:\r\n",
        "        列表, [bbox_offset, bbox_mask, cls_labels]\r\n",
        "        bbox_offset: 每个锚框的标注偏移量，形状为(bn，锚框总数*4)\r\n",
        "        bbox_mask: 形状同bbox_offset, 每个锚框的掩码, 一一对应上面的偏移量, 负类锚框(背景)对应的掩码均为0, 正类锚框的掩码均为1\r\n",
        "        cls_labels: 每个锚框的标注类别, 其中0表示为背景, 形状为(bn，锚框总数)\r\n",
        "    \"\"\"\r\n",
        "    assert len(anchor.shape) == 3 and len(label.shape) == 3\r\n",
        "    bn = label.shape[0]\r\n",
        "    \r\n",
        "    def MultiBoxTarget_one(anc, lab, eps=1e-6):\r\n",
        "        \"\"\"\r\n",
        "        MultiBoxTarget函数的辅助函数, 处理batch中的一个\r\n",
        "        Args:\r\n",
        "            anc: shape of (锚框总数, 4)\r\n",
        "            lab: shape of (真实锚框数, 5), 5代表[类别标签, 四个坐标值]\r\n",
        "            eps: 一个极小值, 防止log0\r\n",
        "        Returns:\r\n",
        "            offset: (锚框总数*4, )\r\n",
        "            bbox_mask: (锚框总数*4, ), 0代表背景, 1代表非背景\r\n",
        "            cls_labels: (锚框总数, 4), 0代表背景\r\n",
        "        \"\"\"\r\n",
        "        an = anc.shape[0]\r\n",
        "        assigned_idx = assign_anchor(lab[:, 1:], anc) # (锚框总数, )\r\n",
        "        bbox_mask = ((assigned_idx >= 0).float().unsqueeze(-1)).repeat(1, 4) # (锚框总数, 4)\r\n",
        "\r\n",
        "        cls_labels = torch.zeros(an, dtype=torch.long) # 0表示背景\r\n",
        "        assigned_bb = torch.zeros((an, 4), dtype=torch.float32) # 所有anchor对应的bb坐标\r\n",
        "        for i in range(an):\r\n",
        "            bb_idx = assigned_idx[i]\r\n",
        "            if bb_idx >= 0: # 即非背景\r\n",
        "                cls_labels[i] = lab[bb_idx, 0].long().item() + 1 # 注意要加一\r\n",
        "                assigned_bb[i, :] = lab[bb_idx, 1:]\r\n",
        "\r\n",
        "        center_anc = xy_to_cxcy(anc) # (center_x, center_y, w, h)\r\n",
        "        center_assigned_bb = xy_to_cxcy(assigned_bb)\r\n",
        "\r\n",
        "        offset_xy = 10.0 * (center_assigned_bb[:, :2] - center_anc[:, :2]) / center_anc[:, 2:]\r\n",
        "        offset_wh = 5.0 * torch.log(eps + center_assigned_bb[:, 2:] / center_anc[:, 2:])\r\n",
        "        offset = torch.cat([offset_xy, offset_wh], dim = 1) * bbox_mask # (锚框总数, 4)\r\n",
        "\r\n",
        "        return offset.view(-1), bbox_mask.view(-1), cls_labels\r\n",
        "    \r\n",
        "    batch_offset = []\r\n",
        "    batch_mask = []\r\n",
        "    batch_cls_labels = []\r\n",
        "    for b in range(bn):\r\n",
        "        offset, bbox_mask, cls_labels = MultiBoxTarget_one(anchor[0, :, :], label[b, :, :])\r\n",
        "        \r\n",
        "        batch_offset.append(offset)\r\n",
        "        batch_mask.append(bbox_mask)\r\n",
        "        batch_cls_labels.append(cls_labels)\r\n",
        "    \r\n",
        "    bbox_offset = torch.stack(batch_offset)\r\n",
        "    bbox_mask = torch.stack(batch_mask)\r\n",
        "    cls_labels = torch.stack(batch_cls_labels)\r\n",
        "    \r\n",
        "    return [bbox_offset, bbox_mask, cls_labels]\r\n",
        "\r\n",
        "\r\n",
        "Pred_BB_Info = namedtuple(\"Pred_BB_Info\", [\"index\", \"class_id\", \"confidence\", \"xyxy\"])\r\n",
        "def non_max_suppression(bb_info_list, nms_threshold = 0.5):\r\n",
        "    \"\"\"\r\n",
        "    非极大抑制处理预测的边界框\r\n",
        "    Args:\r\n",
        "        bb_info_list: Pred_BB_Info的列表, 包含预测类别、置信度等信息\r\n",
        "        nms_threshold: 阈值\r\n",
        "    Returns:\r\n",
        "        output: Pred_BB_Info的列表, 只保留过滤后的边界框信息\r\n",
        "    \"\"\"\r\n",
        "    output = []\r\n",
        "    # 先根据置信度从高到低排序\r\n",
        "    sorted_bb_info_list = sorted(bb_info_list, key = lambda x: x.confidence, reverse=True)\r\n",
        "\r\n",
        "    while len(sorted_bb_info_list) != 0:\r\n",
        "        best = sorted_bb_info_list.pop(0)\r\n",
        "        output.append(best)\r\n",
        "        \r\n",
        "        if len(sorted_bb_info_list) == 0:\r\n",
        "            break\r\n",
        "\r\n",
        "        bb_xyxy = []\r\n",
        "        for bb in sorted_bb_info_list:\r\n",
        "            bb_xyxy.append(bb.xyxy)\r\n",
        "        \r\n",
        "        iou = compute_jaccard(torch.tensor([best.xyxy]), \r\n",
        "                              torch.tensor(bb_xyxy))[0] # shape: (len(sorted_bb_info_list), )\r\n",
        "        \r\n",
        "        n = len(sorted_bb_info_list)\r\n",
        "        sorted_bb_info_list = [sorted_bb_info_list[i] for i in range(n) if iou[i] <= nms_threshold]\r\n",
        "    return output\r\n",
        "\r\n",
        "def MultiBoxDetection(cls_prob, loc_pred, anchor, nms_threshold = 0.5):\r\n",
        "    \"\"\"\r\n",
        "    # 按照「9.4.1. 生成多个锚框」所讲的实现, anchor表示成归一化(xmin, ymin, xmax, ymax).\r\n",
        "    https://zh.d2l.ai/chapter_computer-vision/anchor.html\r\n",
        "    Args:\r\n",
        "        cls_prob: 经过softmax后得到的各个锚框的预测概率, shape:(bn, 预测总类别数+1, 锚框个数)\r\n",
        "        loc_pred: 预测的各个锚框的偏移量, shape:(bn, 锚框个数*4)\r\n",
        "        anchor: MultiBoxPrior输出的默认锚框, shape: (1, 锚框个数, 4)\r\n",
        "        nms_threshold: 非极大抑制中的阈值\r\n",
        "    Returns:\r\n",
        "        所有锚框的信息, shape: (bn, 锚框个数, 6)\r\n",
        "        每个锚框信息由[class_id, confidence, xmin, ymin, xmax, ymax]表示\r\n",
        "        class_id=-1 表示背景或在非极大值抑制中被移除了\r\n",
        "    \"\"\"\r\n",
        "    assert len(cls_prob.shape) == 3 and len(loc_pred.shape) == 2 and len(anchor.shape) == 3\r\n",
        "    bn = cls_prob.shape[0]\r\n",
        "    \r\n",
        "    def MultiBoxDetection_one(c_p, l_p, anc, nms_threshold = 0.5):\r\n",
        "        \"\"\"\r\n",
        "        MultiBoxDetection的辅助函数, 处理batch中的一个\r\n",
        "        Args:\r\n",
        "            c_p: (预测总类别数+1, 锚框个数)\r\n",
        "            l_p: (锚框个数*4, )\r\n",
        "            anc: (锚框个数, 4)\r\n",
        "            nms_threshold: 非极大抑制中的阈值\r\n",
        "        Return:\r\n",
        "            output: (锚框个数, 6)\r\n",
        "        \"\"\"\r\n",
        "        pred_bb_num = c_p.shape[1]\r\n",
        "        anc = (anc + l_p.view(pred_bb_num, 4)).detach().cpu().numpy() # 加上偏移量\r\n",
        "        \r\n",
        "        confidence, class_id = torch.max(c_p, 0)\r\n",
        "        confidence = confidence.detach().cpu().numpy()\r\n",
        "        class_id = class_id.detach().cpu().numpy()\r\n",
        "        \r\n",
        "        pred_bb_info = [Pred_BB_Info(\r\n",
        "                            index = i,\r\n",
        "                            class_id = class_id[i] - 1, # 正类label从0开始\r\n",
        "                            confidence = confidence[i],\r\n",
        "                            xyxy=[*anc[i]]) # xyxy是个列表\r\n",
        "                        for i in range(pred_bb_num)]\r\n",
        "        \r\n",
        "        # 正类的index\r\n",
        "        obj_bb_idx = [bb.index for bb in non_max_suppression(pred_bb_info, nms_threshold)]\r\n",
        "        \r\n",
        "        output = []\r\n",
        "        for bb in pred_bb_info:\r\n",
        "            output.append([\r\n",
        "                (bb.class_id if bb.index in obj_bb_idx else -1.0),\r\n",
        "                bb.confidence,\r\n",
        "                *bb.xyxy\r\n",
        "            ])\r\n",
        "            \r\n",
        "        return torch.tensor(output) # shape: (锚框个数, 6)\r\n",
        "    \r\n",
        "    batch_output = []\r\n",
        "    for b in range(bn):\r\n",
        "        batch_output.append(MultiBoxDetection_one(cls_prob[b], loc_pred[b], anchor[0], nms_threshold))\r\n",
        "    \r\n",
        "    return torch.stack(batch_output)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ################################# 9.6 ############################\r\n",
        "class PikachuDetDataset(torch.utils.data.Dataset):\r\n",
        "    \"\"\"皮卡丘检测数据集类\"\"\"\r\n",
        "    def __init__(self, data_dir, part, image_size=(256, 256)):\r\n",
        "        assert part in [\"train\", \"val\"]\r\n",
        "        self.image_size = image_size\r\n",
        "        self.image_dir = os.path.join(data_dir, part, \"images\")\r\n",
        "        \r\n",
        "        with open(os.path.join(data_dir, part, \"label.json\")) as f:\r\n",
        "            self.label = json.load(f)\r\n",
        "            \r\n",
        "        self.transform = torchvision.transforms.Compose([\r\n",
        "            # 将 PIL 图片转换成位于[0.0, 1.0]的floatTensor, shape (C x H x W)\r\n",
        "            torchvision.transforms.ToTensor()])\r\n",
        "            \r\n",
        "    def __len__(self):\r\n",
        "        return len(self.label)\r\n",
        "    \r\n",
        "    def __getitem__(self, index):\r\n",
        "        image_path = str(index + 1) + \".png\"\r\n",
        "        \r\n",
        "        cls = self.label[image_path][\"class\"]\r\n",
        "        label = np.array([cls] + self.label[image_path][\"loc\"], \r\n",
        "                         dtype=\"float32\")[None, :]\r\n",
        "        \r\n",
        "        PIL_img = Image.open(os.path.join(self.image_dir, image_path)\r\n",
        "                            ).convert('RGB').resize(self.image_size)\r\n",
        "        img = self.transform(PIL_img)\r\n",
        "        \r\n",
        "        sample = {\r\n",
        "            \"label\": label, # shape: (1, 5) [class, xmin, ymin, xmax, ymax]\r\n",
        "            \"image\": img    # shape: (3, *image_size)\r\n",
        "        }\r\n",
        "        \r\n",
        "        return sample\r\n",
        "\r\n",
        "def load_data_pikachu(batch_size, edge_size=256, data_dir = '../../data/pikachu'):  \r\n",
        "    \"\"\"edge_size：输出图像的宽和高\"\"\"\r\n",
        "    image_size = (edge_size, edge_size)\r\n",
        "    train_dataset = PikachuDetDataset(data_dir, 'train', image_size)\r\n",
        "    val_dataset = PikachuDetDataset(data_dir, 'val', image_size)\r\n",
        "    \r\n",
        "\r\n",
        "    train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \r\n",
        "                                             shuffle=True, num_workers=4)\r\n",
        "\r\n",
        "    val_iter = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size,\r\n",
        "                                           shuffle=False, num_workers=4)\r\n",
        "    return train_iter, val_iter\r\n",
        "\r\n",
        "\r\n",
        "# ################################# 9.9 #########################\r\n",
        "def read_voc_images(root=\"../../data/VOCdevkit/VOC2012\", \r\n",
        "                    is_train=True, max_num=None):\r\n",
        "    txt_fname = '%s/ImageSets/Segmentation/%s' % (\r\n",
        "        root, 'train.txt' if is_train else 'val.txt')\r\n",
        "    with open(txt_fname, 'r') as f:\r\n",
        "        images = f.read().split()\r\n",
        "    if max_num is not None:\r\n",
        "        images = images[:min(max_num, len(images))]\r\n",
        "    features, labels = [None] * len(images), [None] * len(images)\r\n",
        "    for i, fname in tqdm(enumerate(images)):\r\n",
        "        features[i] = Image.open('%s/JPEGImages/%s.jpg' % (root, fname)).convert(\"RGB\")\r\n",
        "        labels[i] = Image.open('%s/SegmentationClass/%s.png' % (root, fname)).convert(\"RGB\")\r\n",
        "    return features, labels # PIL image\r\n",
        "\r\n",
        "# colormap2label = torch.zeros(256 ** 3, dtype=torch.uint8)\r\n",
        "# for i, colormap in enumerate(VOC_COLORMAP):\r\n",
        "#     colormap2label[(colormap[0] * 256 + colormap[1]) * 256 + colormap[2]] = i\r\n",
        "def voc_label_indices(colormap, colormap2label):\r\n",
        "    \"\"\"\r\n",
        "    convert colormap (PIL image) to colormap2label (uint8 tensor).\r\n",
        "    \"\"\"\r\n",
        "    colormap = np.array(colormap.convert(\"RGB\")).astype('int32')\r\n",
        "    idx = ((colormap[:, :, 0] * 256 + colormap[:, :, 1]) * 256\r\n",
        "           + colormap[:, :, 2])\r\n",
        "    return colormap2label[idx]\r\n",
        "\r\n",
        "def voc_rand_crop(feature, label, height, width):\r\n",
        "    \"\"\"\r\n",
        "    Random crop feature (PIL image) and label (PIL image).\r\n",
        "    \"\"\"\r\n",
        "    i, j, h, w = torchvision.transforms.RandomCrop.get_params(\r\n",
        "            feature, output_size=(height, width))\r\n",
        "    \r\n",
        "    feature = torchvision.transforms.functional.crop(feature, i, j, h, w)\r\n",
        "    label = torchvision.transforms.functional.crop(label, i, j, h, w)    \r\n",
        "\r\n",
        "    return feature, label\r\n",
        "\r\n",
        "class VOCSegDataset(torch.utils.data.Dataset):\r\n",
        "    def __init__(self, is_train, crop_size, voc_dir, colormap2label, max_num=None):\r\n",
        "        \"\"\"\r\n",
        "        crop_size: (h, w)\r\n",
        "        \"\"\"\r\n",
        "        self.rgb_mean = np.array([0.485, 0.456, 0.406])\r\n",
        "        self.rgb_std = np.array([0.229, 0.224, 0.225])\r\n",
        "        self.tsf = torchvision.transforms.Compose([\r\n",
        "            torchvision.transforms.ToTensor(),\r\n",
        "            torchvision.transforms.Normalize(mean=self.rgb_mean, \r\n",
        "                                             std=self.rgb_std)\r\n",
        "        ])\r\n",
        "        \r\n",
        "        self.crop_size = crop_size # (h, w)\r\n",
        "        features, labels = read_voc_images(root=voc_dir, \r\n",
        "                                           is_train=is_train, \r\n",
        "                                           max_num=max_num)\r\n",
        "        self.features = self.filter(features) # PIL image\r\n",
        "        self.labels = self.filter(labels)     # PIL image\r\n",
        "        self.colormap2label = colormap2label\r\n",
        "        print('read ' + str(len(self.features)) + ' valid examples')\r\n",
        "\r\n",
        "    def filter(self, imgs):\r\n",
        "        return [img for img in imgs if (\r\n",
        "            img.size[1] >= self.crop_size[0] and\r\n",
        "            img.size[0] >= self.crop_size[1])]\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        feature, label = voc_rand_crop(self.features[idx], self.labels[idx],\r\n",
        "                                       *self.crop_size)\r\n",
        "        \r\n",
        "        return (self.tsf(feature),\r\n",
        "                voc_label_indices(label, self.colormap2label))\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.features)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ############################# 10.7 ##########################\r\n",
        "def read_imdb(folder='train', data_root=\"/S1/CSCL/tangss/Datasets/aclImdb\"): \r\n",
        "    data = []\r\n",
        "    for label in ['pos', 'neg']:\r\n",
        "        folder_name = os.path.join(data_root, folder, label)\r\n",
        "        for file in tqdm(os.listdir(folder_name)):\r\n",
        "            with open(os.path.join(folder_name, file), 'rb') as f:\r\n",
        "                review = f.read().decode('utf-8').replace('\\n', '').lower()\r\n",
        "                data.append([review, 1 if label == 'pos' else 0])\r\n",
        "    random.shuffle(data)\r\n",
        "    return data\r\n",
        "\r\n",
        "def get_tokenized_imdb(data):\r\n",
        "    \"\"\"\r\n",
        "    data: list of [string, label]\r\n",
        "    \"\"\"\r\n",
        "    def tokenizer(text):\r\n",
        "        return [tok.lower() for tok in text.split(' ')]\r\n",
        "    return [tokenizer(review) for review, _ in data]\r\n",
        "\r\n",
        "def get_vocab_imdb(data):\r\n",
        "    tokenized_data = get_tokenized_imdb(data)\r\n",
        "    counter = collections.Counter([tk for st in tokenized_data for tk in st])\r\n",
        "    return torchtext.vocab.Vocab(counter, min_freq=5)\r\n",
        "\r\n",
        "def preprocess_imdb(data, vocab):\r\n",
        "    max_l = 500  # 将每条评论通过截断或者补0，使得长度变成500\r\n",
        "\r\n",
        "    def pad(x):\r\n",
        "        return x[:max_l] if len(x) > max_l else x + [0] * (max_l - len(x))\r\n",
        "\r\n",
        "    tokenized_data = get_tokenized_imdb(data)\r\n",
        "    features = torch.tensor([pad([vocab.stoi[word] for word in words]) for words in tokenized_data])\r\n",
        "    labels = torch.tensor([score for _, score in data])\r\n",
        "    return features, labels\r\n",
        "\r\n",
        "def load_pretrained_embedding(words, pretrained_vocab):\r\n",
        "    \"\"\"从预训练好的vocab中提取出words对应的词向量\"\"\"\r\n",
        "    embed = torch.zeros(len(words), pretrained_vocab.vectors[0].shape[0]) # 初始化为0\r\n",
        "    oov_count = 0 # out of vocabulary\r\n",
        "    for i, word in enumerate(words):\r\n",
        "        try:\r\n",
        "            idx = pretrained_vocab.stoi[word]\r\n",
        "            embed[i, :] = pretrained_vocab.vectors[idx]\r\n",
        "        except KeyError:\r\n",
        "            oov_count += 1\r\n",
        "    if oov_count > 0:\r\n",
        "        print(\"There are %d oov words.\" % oov_count)\r\n",
        "    return embed\r\n",
        "\r\n",
        "def predict_sentiment(net, vocab, sentence):\r\n",
        "    \"\"\"sentence是词语的列表\"\"\"\r\n",
        "    device = list(net.parameters())[0].device\r\n",
        "    sentence = torch.tensor([vocab.stoi[word] for word in sentence], device=device)\r\n",
        "    label = torch.argmax(net(sentence.view((1, -1))), dim=1)\r\n",
        "    return 'positive' if label.item() == 1 else 'negative'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PQ7WRWSlw4q"
      },
      "source": [
        "def corr2d_multi_in(X, K):\r\n",
        "    # 沿着X和K的第0维（通道维）分别计算再相加\r\n",
        "    res = corr2d(X[0, :, :], K[0, :, :])\r\n",
        "    for i in range(1, X.shape[0]):\r\n",
        "        res += corr2d(X[i, :, :], K[i, :, :])\r\n",
        "    return res"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtrMbyw_qWV3",
        "outputId": "87c70b8c-13a5-48c0-8167-6397f566eae9"
      },
      "source": [
        "# 构造一个图5.4中的输入数组X，核数组K来验证互相关运算的输出\r\n",
        "X = torch.tensor([[[0, 1, 2], [3, 4, 5], [6, 7, 8]],\r\n",
        "              [[1, 2, 3], [4, 5, 6], [7, 8, 9]]])\r\n",
        "K = torch.tensor([[[0, 1], [2, 3]], [[1, 2], [3, 4]]])\r\n",
        "\r\n",
        "corr2d_multi_in(X, K)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 56.,  72.],\n",
              "        [104., 120.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yf_bQ0ENqVz6"
      },
      "source": [
        "## 2. 多输出通道\r\n",
        "**当输入通道有多个时，因为我们对各个通道的结果做了累加，所以不论输入通道数是多少，输出通道数总是为1**。设卷积核输入通道数和输出通道数分别为$c_{i}$和$c_{0}$，高和宽分别为$k_{h}$和$k_{w}$。**如果希望得到含多个通道的输出，我们可以为每个输出通道分别创建形状为$c_{i} \\times k_{h} \\times k_w$的核数组。将它们在输出通道维上连结，卷积核的形状即$c_{0} \\times c_{i} \\times k_{h} \\times k_w$。**\r\n",
        "\r\n",
        "**在做互相关运算时，每个输出通道上的结果由卷积核在该输出通道上的核数组与整个输入数组计算而来**。\r\n",
        "\r\n",
        "下面我们实现一个互相关运算函数来计算多个通道的输出。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whXLFO85sAGo"
      },
      "source": [
        "def corr2d_multi_in_out(X, K):\r\n",
        "    # 对K的第0维遍历，每次同输入X做互相关计算。所有结果使用stack函数合并在一起\r\n",
        "    return torch.stack([corr2d_multi_in(X, k) for k in K])\r\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zx0EgITXwJQv",
        "outputId": "8f4f781d-5ed2-42cc-ee0d-ef7cb2fa93fa"
      },
      "source": [
        "# 我们将K同K+1（K中每个元素加一）和K+2连结在一起来构造一个输出通道数为3的卷积核。\r\n",
        "K = torch.stack([K,K+1,K+2])\r\n",
        "K.shape  # torch.Size([3, 2, 2, 2])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 2, 2, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiIctX1Fwgz2",
        "outputId": "0a7b5079-4cc6-468a-8f1e-73e56307c6ac"
      },
      "source": [
        "# 下面我们对输入数组X与核数组K做互相关运算。此时的输出含有3个通道。其中第一个通道的结果与之前输入数组X与多输入通道、单输出通道核的计算结果一致。\r\n",
        "corr2d_multi_in_out(X, K)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 56.,  72.],\n",
              "         [104., 120.]],\n",
              "\n",
              "        [[ 76., 100.],\n",
              "         [148., 172.]],\n",
              "\n",
              "        [[ 96., 128.],\n",
              "         [192., 224.]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLejshxtwgWq"
      },
      "source": [
        "## 3. 1X1卷积层"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppEQZAyDwImL"
      },
      "source": [
        "最后我们讨论卷积窗口形状为1×1的多通道卷积层。我们通常称之为1×1卷积层，并将其中的卷积运算称为1×1卷积。因为使用了最小窗口，1×11×1卷积失去了卷积层可以识别高和宽维度上相邻元素构成的模式的功能。实际上，1×1卷积的主要计算发生在通道维上。图5.5展示了使用输入通道数为3、输出通道数为2的1×1卷积核的互相关计算。值得注意的是，输入和输出具有相同的高和宽。输出中的每个元素来自输入中在高和宽上相同位置的元素在不同通道之间的按权重累加。**假设我们将通道维当作特征维，将高和宽维度上的元素当成数据样本，那么1×1卷积层的作用与全连接层等价。**\r\n",
        "\r\n",
        "\r\n",
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HOfuYkK0Xq5"
      },
      "source": [
        "def corr2d_multi_in_out_1x1(X,K):\r\n",
        "    c_i,h,w = X.shape\r\n",
        "    c_o = K.shape[0]\r\n",
        "    X = X.view(c_i,h*w)\r\n",
        "    K = K.view(c_o,c_i)\r\n",
        "    Y = torch.mm(K, X)  # 全连接层的矩阵乘法\r\n",
        "    return Y.view(c_o, h, w)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QhgJYTC0mXD",
        "outputId": "7a375d97-2c90-4852-e948-a11681ab9563"
      },
      "source": [
        "#经验证，做1×11×1卷积时，以上函数与之前实现的互相关运算函数corr2d_multi_in_out等价。\r\n",
        "X = torch.rand(3, 3, 3)\r\n",
        "K = torch.rand(2, 3, 1, 1)\r\n",
        "\r\n",
        "Y1 = corr2d_multi_in_out_1x1(X, K)\r\n",
        "Y2 = corr2d_multi_in_out(X, K)\r\n",
        "\r\n",
        "(Y1 - Y2).norm().item() < 1e-6"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3TToQsp02SB"
      },
      "source": [
        "在之后的模型里我们将会看到1×11×1卷积层被当作保持高和宽维度形状不变的全连接层使用。于是，我们可以通过调整网络层之间的通道数来控制模型复杂度。"
      ]
    }
  ]
}